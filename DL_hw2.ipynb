{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 24\n",
    "DATA_DIR = '../data/sign_mnist_%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset initialization ---\n",
    "\n",
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "\n",
    "# Grayscale + toTensor + Normalize\n",
    "train_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "train_set = datasets.ImageFolder(DATA_DIR % 'train', transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DATA_DIR % 'dev',   transform=test_transform)\n",
    "test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=len(dev_set), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        # Sequential 1: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=28, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=28),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # Sequential 2: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=28, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(p=0.1))\n",
    "        \n",
    "        # Sequential 3: Linear + ReLU + Linear\n",
    "        self.seq3 = nn.Sequential(\n",
    "            nn.Linear(in_features=3136, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=512, out_features=NUM_CLASSES))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # WRITE CODE HERE\n",
    "  \n",
    "        # Sequential 1\n",
    "        x = self.seq1(x)\n",
    "        \n",
    "        # Sequential 2\n",
    "        x = self.seq2(x)\n",
    "        \n",
    "        # Reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Sequential 3\n",
    "        x = self.seq3(x)\n",
    "        \n",
    "        # log_softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # Return x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--- set up ---\n",
    "\n",
    "# Print Cuda info\n",
    "print(\"Cuda is available: {} \\n\".format(torch.cuda.is_available()))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Oprimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Loss functions\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping parameters initialization\n",
    "# Make empty dev lost list and add start values 0 to list\n",
    "# When done=True, then end training\n",
    "dev_loss_list = []\n",
    "train_loss_list = []\n",
    "dev_loss_list.append(100)\n",
    "dev_loss_list.append(100)\n",
    "dev_loss_list.append(100)\n",
    "done = False\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 - Batch 0/138: Loss: 3.3063 | Train Acc: 5.500% (11/200)\n",
      "Training: Epoch 0 - Batch 1/138: Loss: 3.2552 | Train Acc: 5.750% (23/400)\n",
      "Training: Epoch 0 - Batch 2/138: Loss: 3.2378 | Train Acc: 7.000% (42/600)\n",
      "Training: Epoch 0 - Batch 3/138: Loss: 3.2063 | Train Acc: 7.000% (56/800)\n",
      "Training: Epoch 0 - Batch 4/138: Loss: 3.1847 | Train Acc: 7.300% (73/1000)\n",
      "Training: Epoch 0 - Batch 5/138: Loss: 3.1436 | Train Acc: 8.583% (103/1200)\n",
      "Training: Epoch 0 - Batch 6/138: Loss: 3.1140 | Train Acc: 9.786% (137/1400)\n",
      "Training: Epoch 0 - Batch 7/138: Loss: 3.0946 | Train Acc: 10.125% (162/1600)\n",
      "Training: Epoch 0 - Batch 8/138: Loss: 3.0619 | Train Acc: 11.278% (203/1800)\n",
      "Training: Epoch 0 - Batch 9/138: Loss: 3.0357 | Train Acc: 12.250% (245/2000)\n",
      "Training: Epoch 0 - Batch 10/138: Loss: 3.0125 | Train Acc: 12.864% (283/2200)\n",
      "Training: Epoch 0 - Batch 11/138: Loss: 2.9915 | Train Acc: 13.583% (326/2400)\n",
      "Training: Epoch 0 - Batch 12/138: Loss: 2.9685 | Train Acc: 14.385% (374/2600)\n",
      "Training: Epoch 0 - Batch 13/138: Loss: 2.9435 | Train Acc: 15.357% (430/2800)\n",
      "Training: Epoch 0 - Batch 14/138: Loss: 2.9114 | Train Acc: 16.800% (504/3000)\n",
      "Training: Epoch 0 - Batch 15/138: Loss: 2.8900 | Train Acc: 17.469% (559/3200)\n",
      "Training: Epoch 0 - Batch 16/138: Loss: 2.8682 | Train Acc: 18.353% (624/3400)\n",
      "Training: Epoch 0 - Batch 17/138: Loss: 2.8470 | Train Acc: 19.389% (698/3600)\n",
      "Training: Epoch 0 - Batch 18/138: Loss: 2.8232 | Train Acc: 20.079% (763/3800)\n",
      "Training: Epoch 0 - Batch 19/138: Loss: 2.7978 | Train Acc: 21.100% (844/4000)\n",
      "Training: Epoch 0 - Batch 20/138: Loss: 2.7814 | Train Acc: 21.762% (914/4200)\n",
      "Training: Epoch 0 - Batch 21/138: Loss: 2.7583 | Train Acc: 22.659% (997/4400)\n",
      "Training: Epoch 0 - Batch 22/138: Loss: 2.7358 | Train Acc: 23.413% (1077/4600)\n",
      "Training: Epoch 0 - Batch 23/138: Loss: 2.7185 | Train Acc: 24.125% (1158/4800)\n",
      "Training: Epoch 0 - Batch 24/138: Loss: 2.6969 | Train Acc: 25.040% (1252/5000)\n",
      "Training: Epoch 0 - Batch 25/138: Loss: 2.6767 | Train Acc: 25.654% (1334/5200)\n",
      "Training: Epoch 0 - Batch 26/138: Loss: 2.6528 | Train Acc: 26.407% (1426/5400)\n",
      "Training: Epoch 0 - Batch 27/138: Loss: 2.6319 | Train Acc: 27.143% (1520/5600)\n",
      "Training: Epoch 0 - Batch 28/138: Loss: 2.6111 | Train Acc: 28.069% (1628/5800)\n",
      "Training: Epoch 0 - Batch 29/138: Loss: 2.5915 | Train Acc: 28.650% (1719/6000)\n",
      "Training: Epoch 0 - Batch 30/138: Loss: 2.5707 | Train Acc: 29.435% (1825/6200)\n",
      "Training: Epoch 0 - Batch 31/138: Loss: 2.5495 | Train Acc: 30.094% (1926/6400)\n",
      "Training: Epoch 0 - Batch 32/138: Loss: 2.5334 | Train Acc: 30.439% (2009/6600)\n",
      "Training: Epoch 0 - Batch 33/138: Loss: 2.5134 | Train Acc: 31.132% (2117/6800)\n",
      "Training: Epoch 0 - Batch 34/138: Loss: 2.4910 | Train Acc: 31.900% (2233/7000)\n",
      "Training: Epoch 0 - Batch 35/138: Loss: 2.4690 | Train Acc: 32.639% (2350/7200)\n",
      "Training: Epoch 0 - Batch 36/138: Loss: 2.4466 | Train Acc: 33.311% (2465/7400)\n",
      "Training: Epoch 0 - Batch 37/138: Loss: 2.4249 | Train Acc: 33.921% (2578/7600)\n",
      "Training: Epoch 0 - Batch 38/138: Loss: 2.4042 | Train Acc: 34.526% (2693/7800)\n",
      "Training: Epoch 0 - Batch 39/138: Loss: 2.3857 | Train Acc: 35.100% (2808/8000)\n",
      "Training: Epoch 0 - Batch 40/138: Loss: 2.3679 | Train Acc: 35.463% (2908/8200)\n",
      "Training: Epoch 0 - Batch 41/138: Loss: 2.3495 | Train Acc: 35.964% (3021/8400)\n",
      "Training: Epoch 0 - Batch 42/138: Loss: 2.3302 | Train Acc: 36.512% (3140/8600)\n",
      "Training: Epoch 0 - Batch 43/138: Loss: 2.3142 | Train Acc: 36.818% (3240/8800)\n",
      "Training: Epoch 0 - Batch 44/138: Loss: 2.2975 | Train Acc: 37.233% (3351/9000)\n",
      "Training: Epoch 0 - Batch 45/138: Loss: 2.2784 | Train Acc: 37.772% (3475/9200)\n",
      "Training: Epoch 0 - Batch 46/138: Loss: 2.2590 | Train Acc: 38.372% (3607/9400)\n",
      "Training: Epoch 0 - Batch 47/138: Loss: 2.2413 | Train Acc: 38.865% (3731/9600)\n",
      "Training: Epoch 0 - Batch 48/138: Loss: 2.2239 | Train Acc: 39.357% (3857/9800)\n",
      "Training: Epoch 0 - Batch 49/138: Loss: 2.2076 | Train Acc: 39.830% (3983/10000)\n",
      "Training: Epoch 0 - Batch 50/138: Loss: 2.1889 | Train Acc: 40.382% (4119/10200)\n",
      "Training: Epoch 0 - Batch 51/138: Loss: 2.1725 | Train Acc: 40.894% (4253/10400)\n",
      "Training: Epoch 0 - Batch 52/138: Loss: 2.1555 | Train Acc: 41.368% (4385/10600)\n",
      "Training: Epoch 0 - Batch 53/138: Loss: 2.1404 | Train Acc: 41.731% (4507/10800)\n",
      "Training: Epoch 0 - Batch 54/138: Loss: 2.1238 | Train Acc: 42.155% (4637/11000)\n",
      "Training: Epoch 0 - Batch 55/138: Loss: 2.1094 | Train Acc: 42.491% (4759/11200)\n",
      "Training: Epoch 0 - Batch 56/138: Loss: 2.0961 | Train Acc: 42.842% (4884/11400)\n",
      "Training: Epoch 0 - Batch 57/138: Loss: 2.0806 | Train Acc: 43.276% (5020/11600)\n",
      "Training: Epoch 0 - Batch 58/138: Loss: 2.0652 | Train Acc: 43.737% (5161/11800)\n",
      "Training: Epoch 0 - Batch 59/138: Loss: 2.0521 | Train Acc: 44.108% (5293/12000)\n",
      "Training: Epoch 0 - Batch 60/138: Loss: 2.0384 | Train Acc: 44.492% (5428/12200)\n",
      "Training: Epoch 0 - Batch 61/138: Loss: 2.0216 | Train Acc: 45.000% (5580/12400)\n",
      "Training: Epoch 0 - Batch 62/138: Loss: 2.0071 | Train Acc: 45.421% (5723/12600)\n",
      "Training: Epoch 0 - Batch 63/138: Loss: 1.9933 | Train Acc: 45.758% (5857/12800)\n",
      "Training: Epoch 0 - Batch 64/138: Loss: 1.9799 | Train Acc: 46.131% (5997/13000)\n",
      "Training: Epoch 0 - Batch 65/138: Loss: 1.9660 | Train Acc: 46.538% (6143/13200)\n",
      "Training: Epoch 0 - Batch 66/138: Loss: 1.9528 | Train Acc: 46.910% (6286/13400)\n",
      "Training: Epoch 0 - Batch 67/138: Loss: 1.9388 | Train Acc: 47.309% (6434/13600)\n",
      "Training: Epoch 0 - Batch 68/138: Loss: 1.9253 | Train Acc: 47.696% (6582/13800)\n",
      "Training: Epoch 0 - Batch 69/138: Loss: 1.9128 | Train Acc: 48.057% (6728/14000)\n",
      "Training: Epoch 0 - Batch 70/138: Loss: 1.8987 | Train Acc: 48.500% (6887/14200)\n",
      "Training: Epoch 0 - Batch 71/138: Loss: 1.8857 | Train Acc: 48.875% (7038/14400)\n",
      "Training: Epoch 0 - Batch 72/138: Loss: 1.8737 | Train Acc: 49.199% (7183/14600)\n",
      "Training: Epoch 0 - Batch 73/138: Loss: 1.8606 | Train Acc: 49.595% (7340/14800)\n",
      "Training: Epoch 0 - Batch 74/138: Loss: 1.8464 | Train Acc: 49.993% (7499/15000)\n",
      "Training: Epoch 0 - Batch 75/138: Loss: 1.8352 | Train Acc: 50.336% (7651/15200)\n",
      "Training: Epoch 0 - Batch 76/138: Loss: 1.8217 | Train Acc: 50.701% (7808/15400)\n",
      "Training: Epoch 0 - Batch 77/138: Loss: 1.8098 | Train Acc: 51.019% (7959/15600)\n",
      "Training: Epoch 0 - Batch 78/138: Loss: 1.7971 | Train Acc: 51.386% (8119/15800)\n",
      "Training: Epoch 0 - Batch 79/138: Loss: 1.7860 | Train Acc: 51.706% (8273/16000)\n",
      "Training: Epoch 0 - Batch 80/138: Loss: 1.7742 | Train Acc: 52.068% (8435/16200)\n",
      "Training: Epoch 0 - Batch 81/138: Loss: 1.7637 | Train Acc: 52.366% (8588/16400)\n",
      "Training: Epoch 0 - Batch 82/138: Loss: 1.7510 | Train Acc: 52.723% (8752/16600)\n",
      "Training: Epoch 0 - Batch 83/138: Loss: 1.7408 | Train Acc: 52.940% (8894/16800)\n",
      "Training: Epoch 0 - Batch 84/138: Loss: 1.7303 | Train Acc: 53.224% (9048/17000)\n",
      "Training: Epoch 0 - Batch 85/138: Loss: 1.7205 | Train Acc: 53.465% (9196/17200)\n",
      "Training: Epoch 0 - Batch 86/138: Loss: 1.7101 | Train Acc: 53.782% (9358/17400)\n",
      "Training: Epoch 0 - Batch 87/138: Loss: 1.6994 | Train Acc: 54.102% (9522/17600)\n",
      "Training: Epoch 0 - Batch 88/138: Loss: 1.6881 | Train Acc: 54.399% (9683/17800)\n",
      "Training: Epoch 0 - Batch 89/138: Loss: 1.6774 | Train Acc: 54.700% (9846/18000)\n",
      "Training: Epoch 0 - Batch 90/138: Loss: 1.6679 | Train Acc: 54.978% (10006/18200)\n",
      "Training: Epoch 0 - Batch 91/138: Loss: 1.6576 | Train Acc: 55.288% (10173/18400)\n",
      "Training: Epoch 0 - Batch 92/138: Loss: 1.6475 | Train Acc: 55.570% (10336/18600)\n",
      "Training: Epoch 0 - Batch 93/138: Loss: 1.6382 | Train Acc: 55.814% (10493/18800)\n",
      "Training: Epoch 0 - Batch 94/138: Loss: 1.6283 | Train Acc: 56.084% (10656/19000)\n",
      "Training: Epoch 0 - Batch 95/138: Loss: 1.6192 | Train Acc: 56.323% (10814/19200)\n",
      "Training: Epoch 0 - Batch 96/138: Loss: 1.6104 | Train Acc: 56.577% (10976/19400)\n",
      "Training: Epoch 0 - Batch 97/138: Loss: 1.6016 | Train Acc: 56.796% (11132/19600)\n",
      "Training: Epoch 0 - Batch 98/138: Loss: 1.5915 | Train Acc: 57.091% (11304/19800)\n",
      "Training: Epoch 0 - Batch 99/138: Loss: 1.5828 | Train Acc: 57.330% (11466/20000)\n",
      "\n",
      "Dev loss: 1.00186 \n",
      "\n",
      "Training: Epoch 0 - Batch 100/138: Loss: 1.5740 | Train Acc: 57.569% (11629/20200)\n",
      "Training: Epoch 0 - Batch 101/138: Loss: 1.5649 | Train Acc: 57.799% (11791/20400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 - Batch 102/138: Loss: 1.5561 | Train Acc: 58.039% (11956/20600)\n",
      "Training: Epoch 0 - Batch 103/138: Loss: 1.5471 | Train Acc: 58.269% (12120/20800)\n",
      "Training: Epoch 0 - Batch 104/138: Loss: 1.5377 | Train Acc: 58.543% (12294/21000)\n",
      "Training: Epoch 0 - Batch 105/138: Loss: 1.5291 | Train Acc: 58.769% (12459/21200)\n",
      "Training: Epoch 0 - Batch 106/138: Loss: 1.5202 | Train Acc: 59.037% (12634/21400)\n",
      "Training: Epoch 0 - Batch 107/138: Loss: 1.5113 | Train Acc: 59.310% (12811/21600)\n",
      "Training: Epoch 0 - Batch 108/138: Loss: 1.5027 | Train Acc: 59.528% (12977/21800)\n",
      "Training: Epoch 0 - Batch 109/138: Loss: 1.4937 | Train Acc: 59.805% (13157/22000)\n",
      "Training: Epoch 0 - Batch 110/138: Loss: 1.4859 | Train Acc: 60.018% (13324/22200)\n",
      "Training: Epoch 0 - Batch 111/138: Loss: 1.4771 | Train Acc: 60.272% (13501/22400)\n",
      "Training: Epoch 0 - Batch 112/138: Loss: 1.4692 | Train Acc: 60.509% (13675/22600)\n",
      "Training: Epoch 0 - Batch 113/138: Loss: 1.4616 | Train Acc: 60.702% (13840/22800)\n",
      "Training: Epoch 0 - Batch 114/138: Loss: 1.4538 | Train Acc: 60.909% (14009/23000)\n",
      "Training: Epoch 0 - Batch 115/138: Loss: 1.4460 | Train Acc: 61.112% (14178/23200)\n",
      "Training: Epoch 0 - Batch 116/138: Loss: 1.4376 | Train Acc: 61.350% (14356/23400)\n",
      "Training: Epoch 0 - Batch 117/138: Loss: 1.4298 | Train Acc: 61.564% (14529/23600)\n",
      "Training: Epoch 0 - Batch 118/138: Loss: 1.4222 | Train Acc: 61.782% (14704/23800)\n",
      "Training: Epoch 0 - Batch 119/138: Loss: 1.4144 | Train Acc: 61.992% (14878/24000)\n",
      "Training: Epoch 0 - Batch 120/138: Loss: 1.4074 | Train Acc: 62.186% (15049/24200)\n",
      "Training: Epoch 0 - Batch 121/138: Loss: 1.4004 | Train Acc: 62.393% (15224/24400)\n",
      "Training: Epoch 0 - Batch 122/138: Loss: 1.3934 | Train Acc: 62.581% (15395/24600)\n",
      "Training: Epoch 0 - Batch 123/138: Loss: 1.3866 | Train Acc: 62.770% (15567/24800)\n",
      "Training: Epoch 0 - Batch 124/138: Loss: 1.3795 | Train Acc: 62.948% (15737/25000)\n",
      "Training: Epoch 0 - Batch 125/138: Loss: 1.3728 | Train Acc: 63.111% (15904/25200)\n",
      "Training: Epoch 0 - Batch 126/138: Loss: 1.3666 | Train Acc: 63.272% (16071/25400)\n",
      "Training: Epoch 0 - Batch 127/138: Loss: 1.3602 | Train Acc: 63.434% (16239/25600)\n",
      "Training: Epoch 0 - Batch 128/138: Loss: 1.3541 | Train Acc: 63.593% (16407/25800)\n",
      "Training: Epoch 0 - Batch 129/138: Loss: 1.3472 | Train Acc: 63.804% (16589/26000)\n",
      "Training: Epoch 0 - Batch 130/138: Loss: 1.3411 | Train Acc: 63.992% (16766/26200)\n",
      "Training: Epoch 0 - Batch 131/138: Loss: 1.3345 | Train Acc: 64.178% (16943/26400)\n",
      "Training: Epoch 0 - Batch 132/138: Loss: 1.3279 | Train Acc: 64.353% (17118/26600)\n",
      "Training: Epoch 0 - Batch 133/138: Loss: 1.3215 | Train Acc: 64.537% (17296/26800)\n",
      "Training: Epoch 0 - Batch 134/138: Loss: 1.3147 | Train Acc: 64.730% (17477/27000)\n",
      "Training: Epoch 0 - Batch 135/138: Loss: 1.3085 | Train Acc: 64.897% (17652/27200)\n",
      "Training: Epoch 0 - Batch 136/138: Loss: 1.3025 | Train Acc: 65.069% (17829/27400)\n",
      "Training: Epoch 0 - Batch 137/138: Loss: 1.2958 | Train Acc: 65.121% (17879/27455)\n",
      "Training: Epoch 1 - Batch 0/138: Loss: 0.3973 | Train Acc: 92.000% (184/200)\n",
      "Training: Epoch 1 - Batch 1/138: Loss: 0.4293 | Train Acc: 89.750% (359/400)\n",
      "Training: Epoch 1 - Batch 2/138: Loss: 0.4299 | Train Acc: 89.833% (539/600)\n",
      "Training: Epoch 1 - Batch 3/138: Loss: 0.4311 | Train Acc: 90.000% (720/800)\n",
      "Training: Epoch 1 - Batch 4/138: Loss: 0.4374 | Train Acc: 89.300% (893/1000)\n",
      "Training: Epoch 1 - Batch 5/138: Loss: 0.4268 | Train Acc: 89.333% (1072/1200)\n",
      "Training: Epoch 1 - Batch 6/138: Loss: 0.4294 | Train Acc: 89.357% (1251/1400)\n",
      "Training: Epoch 1 - Batch 7/138: Loss: 0.4264 | Train Acc: 89.562% (1433/1600)\n",
      "Training: Epoch 1 - Batch 8/138: Loss: 0.4233 | Train Acc: 89.667% (1614/1800)\n",
      "Training: Epoch 1 - Batch 9/138: Loss: 0.4197 | Train Acc: 89.800% (1796/2000)\n",
      "Training: Epoch 1 - Batch 10/138: Loss: 0.4159 | Train Acc: 89.909% (1978/2200)\n",
      "Training: Epoch 1 - Batch 11/138: Loss: 0.4101 | Train Acc: 90.125% (2163/2400)\n",
      "Training: Epoch 1 - Batch 12/138: Loss: 0.4089 | Train Acc: 90.231% (2346/2600)\n",
      "Training: Epoch 1 - Batch 13/138: Loss: 0.4076 | Train Acc: 90.286% (2528/2800)\n",
      "Training: Epoch 1 - Batch 14/138: Loss: 0.4027 | Train Acc: 90.467% (2714/3000)\n",
      "Training: Epoch 1 - Batch 15/138: Loss: 0.3992 | Train Acc: 90.688% (2902/3200)\n",
      "Training: Epoch 1 - Batch 16/138: Loss: 0.3962 | Train Acc: 90.824% (3088/3400)\n",
      "Training: Epoch 1 - Batch 17/138: Loss: 0.3974 | Train Acc: 90.722% (3266/3600)\n",
      "Training: Epoch 1 - Batch 18/138: Loss: 0.3954 | Train Acc: 90.711% (3447/3800)\n",
      "Training: Epoch 1 - Batch 19/138: Loss: 0.3934 | Train Acc: 90.775% (3631/4000)\n",
      "Training: Epoch 1 - Batch 20/138: Loss: 0.3956 | Train Acc: 90.500% (3801/4200)\n",
      "Training: Epoch 1 - Batch 21/138: Loss: 0.3924 | Train Acc: 90.614% (3987/4400)\n",
      "Training: Epoch 1 - Batch 22/138: Loss: 0.3911 | Train Acc: 90.717% (4173/4600)\n",
      "Training: Epoch 1 - Batch 23/138: Loss: 0.3929 | Train Acc: 90.500% (4344/4800)\n",
      "Training: Epoch 1 - Batch 24/138: Loss: 0.3922 | Train Acc: 90.520% (4526/5000)\n",
      "Training: Epoch 1 - Batch 25/138: Loss: 0.3896 | Train Acc: 90.596% (4711/5200)\n",
      "Training: Epoch 1 - Batch 26/138: Loss: 0.3870 | Train Acc: 90.685% (4897/5400)\n",
      "Training: Epoch 1 - Batch 27/138: Loss: 0.3859 | Train Acc: 90.714% (5080/5600)\n",
      "Training: Epoch 1 - Batch 28/138: Loss: 0.3857 | Train Acc: 90.638% (5257/5800)\n",
      "Training: Epoch 1 - Batch 29/138: Loss: 0.3832 | Train Acc: 90.850% (5451/6000)\n",
      "Training: Epoch 1 - Batch 30/138: Loss: 0.3819 | Train Acc: 90.887% (5635/6200)\n",
      "Training: Epoch 1 - Batch 31/138: Loss: 0.3796 | Train Acc: 90.922% (5819/6400)\n",
      "Training: Epoch 1 - Batch 32/138: Loss: 0.3788 | Train Acc: 90.970% (6004/6600)\n",
      "Training: Epoch 1 - Batch 33/138: Loss: 0.3781 | Train Acc: 91.015% (6189/6800)\n",
      "Training: Epoch 1 - Batch 34/138: Loss: 0.3762 | Train Acc: 91.043% (6373/7000)\n",
      "Training: Epoch 1 - Batch 35/138: Loss: 0.3734 | Train Acc: 91.125% (6561/7200)\n",
      "Training: Epoch 1 - Batch 36/138: Loss: 0.3714 | Train Acc: 91.189% (6748/7400)\n",
      "Training: Epoch 1 - Batch 37/138: Loss: 0.3698 | Train Acc: 91.250% (6935/7600)\n",
      "Training: Epoch 1 - Batch 38/138: Loss: 0.3683 | Train Acc: 91.269% (7119/7800)\n",
      "Training: Epoch 1 - Batch 39/138: Loss: 0.3650 | Train Acc: 91.375% (7310/8000)\n",
      "Training: Epoch 1 - Batch 40/138: Loss: 0.3645 | Train Acc: 91.354% (7491/8200)\n",
      "Training: Epoch 1 - Batch 41/138: Loss: 0.3621 | Train Acc: 91.476% (7684/8400)\n",
      "Training: Epoch 1 - Batch 42/138: Loss: 0.3606 | Train Acc: 91.523% (7871/8600)\n",
      "Training: Epoch 1 - Batch 43/138: Loss: 0.3611 | Train Acc: 91.477% (8050/8800)\n",
      "Training: Epoch 1 - Batch 44/138: Loss: 0.3590 | Train Acc: 91.544% (8239/9000)\n",
      "Training: Epoch 1 - Batch 45/138: Loss: 0.3559 | Train Acc: 91.685% (8435/9200)\n",
      "Training: Epoch 1 - Batch 46/138: Loss: 0.3548 | Train Acc: 91.702% (8620/9400)\n",
      "Training: Epoch 1 - Batch 47/138: Loss: 0.3533 | Train Acc: 91.750% (8808/9600)\n",
      "Training: Epoch 1 - Batch 48/138: Loss: 0.3521 | Train Acc: 91.776% (8994/9800)\n",
      "Training: Epoch 1 - Batch 49/138: Loss: 0.3513 | Train Acc: 91.780% (9178/10000)\n",
      "Training: Epoch 1 - Batch 50/138: Loss: 0.3496 | Train Acc: 91.814% (9365/10200)\n",
      "Training: Epoch 1 - Batch 51/138: Loss: 0.3482 | Train Acc: 91.856% (9553/10400)\n",
      "Training: Epoch 1 - Batch 52/138: Loss: 0.3459 | Train Acc: 91.972% (9749/10600)\n",
      "Training: Epoch 1 - Batch 53/138: Loss: 0.3441 | Train Acc: 92.028% (9939/10800)\n",
      "Training: Epoch 1 - Batch 54/138: Loss: 0.3441 | Train Acc: 92.000% (10120/11000)\n",
      "Training: Epoch 1 - Batch 55/138: Loss: 0.3427 | Train Acc: 92.000% (10304/11200)\n",
      "Training: Epoch 1 - Batch 56/138: Loss: 0.3414 | Train Acc: 92.070% (10496/11400)\n",
      "Training: Epoch 1 - Batch 57/138: Loss: 0.3396 | Train Acc: 92.155% (10690/11600)\n",
      "Training: Epoch 1 - Batch 58/138: Loss: 0.3382 | Train Acc: 92.229% (10883/11800)\n",
      "Training: Epoch 1 - Batch 59/138: Loss: 0.3369 | Train Acc: 92.267% (11072/12000)\n",
      "Training: Epoch 1 - Batch 60/138: Loss: 0.3356 | Train Acc: 92.303% (11261/12200)\n",
      "Training: Epoch 1 - Batch 61/138: Loss: 0.3353 | Train Acc: 92.315% (11447/12400)\n",
      "\n",
      "Dev loss: 0.60326 \n",
      "\n",
      "Training: Epoch 1 - Batch 62/138: Loss: 0.3352 | Train Acc: 92.294% (11629/12600)\n",
      "Training: Epoch 1 - Batch 63/138: Loss: 0.3332 | Train Acc: 92.344% (11820/12800)\n",
      "Training: Epoch 1 - Batch 64/138: Loss: 0.3320 | Train Acc: 92.354% (12006/13000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 1 - Batch 65/138: Loss: 0.3308 | Train Acc: 92.371% (12193/13200)\n",
      "Training: Epoch 1 - Batch 66/138: Loss: 0.3294 | Train Acc: 92.403% (12382/13400)\n",
      "Training: Epoch 1 - Batch 67/138: Loss: 0.3278 | Train Acc: 92.434% (12571/13600)\n",
      "Training: Epoch 1 - Batch 68/138: Loss: 0.3268 | Train Acc: 92.471% (12761/13800)\n",
      "Training: Epoch 1 - Batch 69/138: Loss: 0.3266 | Train Acc: 92.471% (12946/14000)\n",
      "Training: Epoch 1 - Batch 70/138: Loss: 0.3250 | Train Acc: 92.521% (13138/14200)\n",
      "Training: Epoch 1 - Batch 71/138: Loss: 0.3241 | Train Acc: 92.535% (13325/14400)\n",
      "Training: Epoch 1 - Batch 72/138: Loss: 0.3228 | Train Acc: 92.568% (13515/14600)\n",
      "Training: Epoch 1 - Batch 73/138: Loss: 0.3217 | Train Acc: 92.615% (13707/14800)\n",
      "Training: Epoch 1 - Batch 74/138: Loss: 0.3205 | Train Acc: 92.653% (13898/15000)\n",
      "Training: Epoch 1 - Batch 75/138: Loss: 0.3193 | Train Acc: 92.678% (14087/15200)\n",
      "Training: Epoch 1 - Batch 76/138: Loss: 0.3184 | Train Acc: 92.695% (14275/15400)\n",
      "Training: Epoch 1 - Batch 77/138: Loss: 0.3177 | Train Acc: 92.692% (14460/15600)\n",
      "Training: Epoch 1 - Batch 78/138: Loss: 0.3160 | Train Acc: 92.741% (14653/15800)\n",
      "Training: Epoch 1 - Batch 79/138: Loss: 0.3150 | Train Acc: 92.794% (14847/16000)\n",
      "Training: Epoch 1 - Batch 80/138: Loss: 0.3144 | Train Acc: 92.784% (15031/16200)\n",
      "Training: Epoch 1 - Batch 81/138: Loss: 0.3128 | Train Acc: 92.854% (15228/16400)\n",
      "Training: Epoch 1 - Batch 82/138: Loss: 0.3113 | Train Acc: 92.880% (15418/16600)\n",
      "Training: Epoch 1 - Batch 83/138: Loss: 0.3105 | Train Acc: 92.905% (15608/16800)\n",
      "Training: Epoch 1 - Batch 84/138: Loss: 0.3091 | Train Acc: 92.941% (15800/17000)\n",
      "Training: Epoch 1 - Batch 85/138: Loss: 0.3081 | Train Acc: 92.983% (15993/17200)\n",
      "Training: Epoch 1 - Batch 86/138: Loss: 0.3072 | Train Acc: 93.006% (16183/17400)\n",
      "Training: Epoch 1 - Batch 87/138: Loss: 0.3061 | Train Acc: 93.023% (16372/17600)\n",
      "Training: Epoch 1 - Batch 88/138: Loss: 0.3052 | Train Acc: 93.062% (16565/17800)\n",
      "Training: Epoch 1 - Batch 89/138: Loss: 0.3044 | Train Acc: 93.106% (16759/18000)\n",
      "Training: Epoch 1 - Batch 90/138: Loss: 0.3031 | Train Acc: 93.148% (16953/18200)\n",
      "Training: Epoch 1 - Batch 91/138: Loss: 0.3018 | Train Acc: 93.196% (17148/18400)\n",
      "Training: Epoch 1 - Batch 92/138: Loss: 0.3004 | Train Acc: 93.231% (17341/18600)\n",
      "Training: Epoch 1 - Batch 93/138: Loss: 0.2993 | Train Acc: 93.250% (17531/18800)\n",
      "Training: Epoch 1 - Batch 94/138: Loss: 0.2978 | Train Acc: 93.300% (17727/19000)\n",
      "Training: Epoch 1 - Batch 95/138: Loss: 0.2973 | Train Acc: 93.307% (17915/19200)\n",
      "Training: Epoch 1 - Batch 96/138: Loss: 0.2963 | Train Acc: 93.351% (18110/19400)\n",
      "Training: Epoch 1 - Batch 97/138: Loss: 0.2958 | Train Acc: 93.367% (18300/19600)\n",
      "Training: Epoch 1 - Batch 98/138: Loss: 0.2949 | Train Acc: 93.389% (18491/19800)\n",
      "Training: Epoch 1 - Batch 99/138: Loss: 0.2938 | Train Acc: 93.420% (18684/20000)\n",
      "Training: Epoch 1 - Batch 100/138: Loss: 0.2928 | Train Acc: 93.455% (18878/20200)\n",
      "Training: Epoch 1 - Batch 101/138: Loss: 0.2916 | Train Acc: 93.495% (19073/20400)\n",
      "Training: Epoch 1 - Batch 102/138: Loss: 0.2907 | Train Acc: 93.500% (19261/20600)\n",
      "Training: Epoch 1 - Batch 103/138: Loss: 0.2894 | Train Acc: 93.543% (19457/20800)\n",
      "Training: Epoch 1 - Batch 104/138: Loss: 0.2886 | Train Acc: 93.571% (19650/21000)\n",
      "Training: Epoch 1 - Batch 105/138: Loss: 0.2879 | Train Acc: 93.580% (19839/21200)\n",
      "Training: Epoch 1 - Batch 106/138: Loss: 0.2875 | Train Acc: 93.593% (20029/21400)\n",
      "Training: Epoch 1 - Batch 107/138: Loss: 0.2866 | Train Acc: 93.616% (20221/21600)\n",
      "Training: Epoch 1 - Batch 108/138: Loss: 0.2859 | Train Acc: 93.624% (20410/21800)\n",
      "Training: Epoch 1 - Batch 109/138: Loss: 0.2850 | Train Acc: 93.636% (20600/22000)\n",
      "Training: Epoch 1 - Batch 110/138: Loss: 0.2840 | Train Acc: 93.662% (20793/22200)\n",
      "Training: Epoch 1 - Batch 111/138: Loss: 0.2828 | Train Acc: 93.705% (20990/22400)\n",
      "Training: Epoch 1 - Batch 112/138: Loss: 0.2818 | Train Acc: 93.743% (21186/22600)\n",
      "Training: Epoch 1 - Batch 113/138: Loss: 0.2810 | Train Acc: 93.763% (21378/22800)\n",
      "Training: Epoch 1 - Batch 114/138: Loss: 0.2800 | Train Acc: 93.796% (21573/23000)\n",
      "Training: Epoch 1 - Batch 115/138: Loss: 0.2789 | Train Acc: 93.823% (21767/23200)\n",
      "Training: Epoch 1 - Batch 116/138: Loss: 0.2780 | Train Acc: 93.850% (21961/23400)\n",
      "Training: Epoch 1 - Batch 117/138: Loss: 0.2776 | Train Acc: 93.864% (22152/23600)\n",
      "Training: Epoch 1 - Batch 118/138: Loss: 0.2765 | Train Acc: 93.887% (22345/23800)\n",
      "Training: Epoch 1 - Batch 119/138: Loss: 0.2755 | Train Acc: 93.921% (22541/24000)\n",
      "Training: Epoch 1 - Batch 120/138: Loss: 0.2746 | Train Acc: 93.946% (22735/24200)\n",
      "Training: Epoch 1 - Batch 121/138: Loss: 0.2738 | Train Acc: 93.967% (22928/24400)\n",
      "Training: Epoch 1 - Batch 122/138: Loss: 0.2729 | Train Acc: 93.992% (23122/24600)\n",
      "Training: Epoch 1 - Batch 123/138: Loss: 0.2720 | Train Acc: 94.016% (23316/24800)\n",
      "Training: Epoch 1 - Batch 124/138: Loss: 0.2713 | Train Acc: 94.032% (23508/25000)\n",
      "Training: Epoch 1 - Batch 125/138: Loss: 0.2707 | Train Acc: 94.044% (23699/25200)\n",
      "Training: Epoch 1 - Batch 126/138: Loss: 0.2697 | Train Acc: 94.067% (23893/25400)\n",
      "Training: Epoch 1 - Batch 127/138: Loss: 0.2689 | Train Acc: 94.086% (24086/25600)\n",
      "Training: Epoch 1 - Batch 128/138: Loss: 0.2681 | Train Acc: 94.101% (24278/25800)\n",
      "Training: Epoch 1 - Batch 129/138: Loss: 0.2675 | Train Acc: 94.115% (24470/26000)\n",
      "Training: Epoch 1 - Batch 130/138: Loss: 0.2667 | Train Acc: 94.134% (24663/26200)\n",
      "Training: Epoch 1 - Batch 131/138: Loss: 0.2659 | Train Acc: 94.155% (24857/26400)\n",
      "Training: Epoch 1 - Batch 132/138: Loss: 0.2653 | Train Acc: 94.165% (25048/26600)\n",
      "Training: Epoch 1 - Batch 133/138: Loss: 0.2647 | Train Acc: 94.172% (25238/26800)\n",
      "Training: Epoch 1 - Batch 134/138: Loss: 0.2640 | Train Acc: 94.185% (25430/27000)\n",
      "Training: Epoch 1 - Batch 135/138: Loss: 0.2635 | Train Acc: 94.199% (25622/27200)\n",
      "Training: Epoch 1 - Batch 136/138: Loss: 0.2629 | Train Acc: 94.201% (25811/27400)\n",
      "Training: Epoch 1 - Batch 137/138: Loss: 0.2616 | Train Acc: 94.212% (25866/27455)\n",
      "Training: Epoch 2 - Batch 0/138: Loss: 0.1608 | Train Acc: 95.500% (191/200)\n",
      "Training: Epoch 2 - Batch 1/138: Loss: 0.1718 | Train Acc: 95.750% (383/400)\n",
      "Training: Epoch 2 - Batch 2/138: Loss: 0.1583 | Train Acc: 97.000% (582/600)\n",
      "Training: Epoch 2 - Batch 3/138: Loss: 0.1494 | Train Acc: 97.625% (781/800)\n",
      "Training: Epoch 2 - Batch 4/138: Loss: 0.1418 | Train Acc: 97.800% (978/1000)\n",
      "Training: Epoch 2 - Batch 5/138: Loss: 0.1396 | Train Acc: 98.083% (1177/1200)\n",
      "Training: Epoch 2 - Batch 6/138: Loss: 0.1434 | Train Acc: 97.714% (1368/1400)\n",
      "Training: Epoch 2 - Batch 7/138: Loss: 0.1468 | Train Acc: 97.438% (1559/1600)\n",
      "Training: Epoch 2 - Batch 8/138: Loss: 0.1473 | Train Acc: 97.278% (1751/1800)\n",
      "Training: Epoch 2 - Batch 9/138: Loss: 0.1469 | Train Acc: 97.200% (1944/2000)\n",
      "Training: Epoch 2 - Batch 10/138: Loss: 0.1462 | Train Acc: 97.227% (2139/2200)\n",
      "Training: Epoch 2 - Batch 11/138: Loss: 0.1450 | Train Acc: 97.250% (2334/2400)\n",
      "Training: Epoch 2 - Batch 12/138: Loss: 0.1434 | Train Acc: 97.231% (2528/2600)\n",
      "Training: Epoch 2 - Batch 13/138: Loss: 0.1430 | Train Acc: 97.250% (2723/2800)\n",
      "Training: Epoch 2 - Batch 14/138: Loss: 0.1424 | Train Acc: 97.267% (2918/3000)\n",
      "Training: Epoch 2 - Batch 15/138: Loss: 0.1428 | Train Acc: 97.062% (3106/3200)\n",
      "Training: Epoch 2 - Batch 16/138: Loss: 0.1420 | Train Acc: 97.147% (3303/3400)\n",
      "Training: Epoch 2 - Batch 17/138: Loss: 0.1433 | Train Acc: 97.056% (3494/3600)\n",
      "Training: Epoch 2 - Batch 18/138: Loss: 0.1424 | Train Acc: 97.158% (3692/3800)\n",
      "Training: Epoch 2 - Batch 19/138: Loss: 0.1418 | Train Acc: 97.200% (3888/4000)\n",
      "Training: Epoch 2 - Batch 20/138: Loss: 0.1408 | Train Acc: 97.238% (4084/4200)\n",
      "Training: Epoch 2 - Batch 21/138: Loss: 0.1400 | Train Acc: 97.318% (4282/4400)\n",
      "Training: Epoch 2 - Batch 22/138: Loss: 0.1393 | Train Acc: 97.304% (4476/4600)\n",
      "Training: Epoch 2 - Batch 23/138: Loss: 0.1384 | Train Acc: 97.375% (4674/4800)\n",
      "\n",
      "Dev loss: 0.45558 \n",
      "\n",
      "Training: Epoch 2 - Batch 24/138: Loss: 0.1377 | Train Acc: 97.400% (4870/5000)\n",
      "Training: Epoch 2 - Batch 25/138: Loss: 0.1389 | Train Acc: 97.385% (5064/5200)\n",
      "Training: Epoch 2 - Batch 26/138: Loss: 0.1392 | Train Acc: 97.389% (5259/5400)\n",
      "Training: Epoch 2 - Batch 27/138: Loss: 0.1388 | Train Acc: 97.411% (5455/5600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 2 - Batch 28/138: Loss: 0.1379 | Train Acc: 97.431% (5651/5800)\n",
      "Training: Epoch 2 - Batch 29/138: Loss: 0.1386 | Train Acc: 97.450% (5847/6000)\n",
      "Training: Epoch 2 - Batch 30/138: Loss: 0.1377 | Train Acc: 97.452% (6042/6200)\n",
      "Training: Epoch 2 - Batch 31/138: Loss: 0.1375 | Train Acc: 97.484% (6239/6400)\n",
      "Training: Epoch 2 - Batch 32/138: Loss: 0.1367 | Train Acc: 97.515% (6436/6600)\n",
      "Training: Epoch 2 - Batch 33/138: Loss: 0.1361 | Train Acc: 97.529% (6632/6800)\n",
      "Training: Epoch 2 - Batch 34/138: Loss: 0.1357 | Train Acc: 97.514% (6826/7000)\n",
      "Training: Epoch 2 - Batch 35/138: Loss: 0.1356 | Train Acc: 97.500% (7020/7200)\n",
      "Training: Epoch 2 - Batch 36/138: Loss: 0.1347 | Train Acc: 97.554% (7219/7400)\n",
      "Training: Epoch 2 - Batch 37/138: Loss: 0.1337 | Train Acc: 97.618% (7419/7600)\n",
      "Training: Epoch 2 - Batch 38/138: Loss: 0.1338 | Train Acc: 97.603% (7613/7800)\n",
      "Training: Epoch 2 - Batch 39/138: Loss: 0.1328 | Train Acc: 97.638% (7811/8000)\n",
      "Training: Epoch 2 - Batch 40/138: Loss: 0.1324 | Train Acc: 97.646% (8007/8200)\n",
      "Training: Epoch 2 - Batch 41/138: Loss: 0.1315 | Train Acc: 97.690% (8206/8400)\n",
      "Training: Epoch 2 - Batch 42/138: Loss: 0.1310 | Train Acc: 97.686% (8401/8600)\n",
      "Training: Epoch 2 - Batch 43/138: Loss: 0.1306 | Train Acc: 97.682% (8596/8800)\n",
      "Training: Epoch 2 - Batch 44/138: Loss: 0.1298 | Train Acc: 97.711% (8794/9000)\n",
      "Training: Epoch 2 - Batch 45/138: Loss: 0.1288 | Train Acc: 97.761% (8994/9200)\n",
      "Training: Epoch 2 - Batch 46/138: Loss: 0.1283 | Train Acc: 97.787% (9192/9400)\n",
      "Training: Epoch 2 - Batch 47/138: Loss: 0.1289 | Train Acc: 97.750% (9384/9600)\n",
      "Training: Epoch 2 - Batch 48/138: Loss: 0.1288 | Train Acc: 97.735% (9578/9800)\n",
      "Training: Epoch 2 - Batch 49/138: Loss: 0.1289 | Train Acc: 97.720% (9772/10000)\n",
      "Training: Epoch 2 - Batch 50/138: Loss: 0.1279 | Train Acc: 97.755% (9971/10200)\n",
      "Training: Epoch 2 - Batch 51/138: Loss: 0.1281 | Train Acc: 97.731% (10164/10400)\n",
      "Training: Epoch 2 - Batch 52/138: Loss: 0.1279 | Train Acc: 97.726% (10359/10600)\n",
      "Training: Epoch 2 - Batch 53/138: Loss: 0.1274 | Train Acc: 97.731% (10555/10800)\n",
      "Training: Epoch 2 - Batch 54/138: Loss: 0.1275 | Train Acc: 97.709% (10748/11000)\n",
      "Training: Epoch 2 - Batch 55/138: Loss: 0.1268 | Train Acc: 97.741% (10947/11200)\n",
      "Training: Epoch 2 - Batch 56/138: Loss: 0.1264 | Train Acc: 97.763% (11145/11400)\n",
      "Training: Epoch 2 - Batch 57/138: Loss: 0.1260 | Train Acc: 97.767% (11341/11600)\n",
      "Training: Epoch 2 - Batch 58/138: Loss: 0.1254 | Train Acc: 97.788% (11539/11800)\n",
      "Training: Epoch 2 - Batch 59/138: Loss: 0.1246 | Train Acc: 97.808% (11737/12000)\n",
      "Training: Epoch 2 - Batch 60/138: Loss: 0.1244 | Train Acc: 97.820% (11934/12200)\n",
      "Training: Epoch 2 - Batch 61/138: Loss: 0.1241 | Train Acc: 97.823% (12130/12400)\n",
      "Training: Epoch 2 - Batch 62/138: Loss: 0.1238 | Train Acc: 97.825% (12326/12600)\n",
      "Training: Epoch 2 - Batch 63/138: Loss: 0.1234 | Train Acc: 97.844% (12524/12800)\n",
      "Training: Epoch 2 - Batch 64/138: Loss: 0.1234 | Train Acc: 97.854% (12721/13000)\n",
      "Training: Epoch 2 - Batch 65/138: Loss: 0.1229 | Train Acc: 97.871% (12919/13200)\n",
      "Training: Epoch 2 - Batch 66/138: Loss: 0.1225 | Train Acc: 97.888% (13117/13400)\n",
      "Training: Epoch 2 - Batch 67/138: Loss: 0.1222 | Train Acc: 97.882% (13312/13600)\n",
      "Training: Epoch 2 - Batch 68/138: Loss: 0.1221 | Train Acc: 97.884% (13508/13800)\n",
      "Training: Epoch 2 - Batch 69/138: Loss: 0.1215 | Train Acc: 97.900% (13706/14000)\n",
      "Training: Epoch 2 - Batch 70/138: Loss: 0.1211 | Train Acc: 97.908% (13903/14200)\n",
      "Training: Epoch 2 - Batch 71/138: Loss: 0.1209 | Train Acc: 97.924% (14101/14400)\n",
      "Training: Epoch 2 - Batch 72/138: Loss: 0.1207 | Train Acc: 97.918% (14296/14600)\n",
      "Training: Epoch 2 - Batch 73/138: Loss: 0.1204 | Train Acc: 97.919% (14492/14800)\n",
      "Training: Epoch 2 - Batch 74/138: Loss: 0.1202 | Train Acc: 97.933% (14690/15000)\n",
      "Training: Epoch 2 - Batch 75/138: Loss: 0.1195 | Train Acc: 97.954% (14889/15200)\n",
      "Training: Epoch 2 - Batch 76/138: Loss: 0.1192 | Train Acc: 97.961% (15086/15400)\n",
      "Training: Epoch 2 - Batch 77/138: Loss: 0.1186 | Train Acc: 97.974% (15284/15600)\n",
      "Training: Epoch 2 - Batch 78/138: Loss: 0.1183 | Train Acc: 97.981% (15481/15800)\n",
      "Training: Epoch 2 - Batch 79/138: Loss: 0.1178 | Train Acc: 97.994% (15679/16000)\n",
      "Training: Epoch 2 - Batch 80/138: Loss: 0.1173 | Train Acc: 98.012% (15878/16200)\n",
      "Training: Epoch 2 - Batch 81/138: Loss: 0.1169 | Train Acc: 98.012% (16074/16400)\n",
      "Training: Epoch 2 - Batch 82/138: Loss: 0.1166 | Train Acc: 98.024% (16272/16600)\n",
      "Training: Epoch 2 - Batch 83/138: Loss: 0.1164 | Train Acc: 98.042% (16471/16800)\n",
      "Training: Epoch 2 - Batch 84/138: Loss: 0.1162 | Train Acc: 98.053% (16669/17000)\n",
      "Training: Epoch 2 - Batch 85/138: Loss: 0.1158 | Train Acc: 98.070% (16868/17200)\n",
      "Training: Epoch 2 - Batch 86/138: Loss: 0.1152 | Train Acc: 98.092% (17068/17400)\n",
      "Training: Epoch 2 - Batch 87/138: Loss: 0.1148 | Train Acc: 98.108% (17267/17600)\n",
      "Training: Epoch 2 - Batch 88/138: Loss: 0.1146 | Train Acc: 98.118% (17465/17800)\n",
      "Training: Epoch 2 - Batch 89/138: Loss: 0.1144 | Train Acc: 98.133% (17664/18000)\n",
      "Training: Epoch 2 - Batch 90/138: Loss: 0.1139 | Train Acc: 98.148% (17863/18200)\n",
      "Training: Epoch 2 - Batch 91/138: Loss: 0.1138 | Train Acc: 98.147% (18059/18400)\n",
      "Training: Epoch 2 - Batch 92/138: Loss: 0.1136 | Train Acc: 98.151% (18256/18600)\n",
      "Training: Epoch 2 - Batch 93/138: Loss: 0.1131 | Train Acc: 98.160% (18454/18800)\n",
      "Training: Epoch 2 - Batch 94/138: Loss: 0.1130 | Train Acc: 98.163% (18651/19000)\n",
      "Training: Epoch 2 - Batch 95/138: Loss: 0.1129 | Train Acc: 98.167% (18848/19200)\n",
      "Training: Epoch 2 - Batch 96/138: Loss: 0.1128 | Train Acc: 98.170% (19045/19400)\n",
      "Training: Epoch 2 - Batch 97/138: Loss: 0.1125 | Train Acc: 98.179% (19243/19600)\n",
      "Training: Epoch 2 - Batch 98/138: Loss: 0.1123 | Train Acc: 98.177% (19439/19800)\n",
      "Training: Epoch 2 - Batch 99/138: Loss: 0.1123 | Train Acc: 98.165% (19633/20000)\n",
      "Training: Epoch 2 - Batch 100/138: Loss: 0.1120 | Train Acc: 98.168% (19830/20200)\n",
      "Training: Epoch 2 - Batch 101/138: Loss: 0.1116 | Train Acc: 98.176% (20028/20400)\n",
      "Training: Epoch 2 - Batch 102/138: Loss: 0.1113 | Train Acc: 98.184% (20226/20600)\n",
      "Training: Epoch 2 - Batch 103/138: Loss: 0.1113 | Train Acc: 98.178% (20421/20800)\n",
      "Training: Epoch 2 - Batch 104/138: Loss: 0.1109 | Train Acc: 98.195% (20621/21000)\n",
      "Training: Epoch 2 - Batch 105/138: Loss: 0.1107 | Train Acc: 98.198% (20818/21200)\n",
      "Training: Epoch 2 - Batch 106/138: Loss: 0.1104 | Train Acc: 98.201% (21015/21400)\n",
      "Training: Epoch 2 - Batch 107/138: Loss: 0.1103 | Train Acc: 98.204% (21212/21600)\n",
      "Training: Epoch 2 - Batch 108/138: Loss: 0.1102 | Train Acc: 98.202% (21408/21800)\n",
      "Training: Epoch 2 - Batch 109/138: Loss: 0.1101 | Train Acc: 98.205% (21605/22000)\n",
      "Training: Epoch 2 - Batch 110/138: Loss: 0.1099 | Train Acc: 98.212% (21803/22200)\n",
      "Training: Epoch 2 - Batch 111/138: Loss: 0.1095 | Train Acc: 98.228% (22003/22400)\n",
      "Training: Epoch 2 - Batch 112/138: Loss: 0.1091 | Train Acc: 98.235% (22201/22600)\n",
      "Training: Epoch 2 - Batch 113/138: Loss: 0.1092 | Train Acc: 98.228% (22396/22800)\n",
      "Training: Epoch 2 - Batch 114/138: Loss: 0.1089 | Train Acc: 98.226% (22592/23000)\n",
      "Training: Epoch 2 - Batch 115/138: Loss: 0.1086 | Train Acc: 98.237% (22791/23200)\n",
      "Training: Epoch 2 - Batch 116/138: Loss: 0.1086 | Train Acc: 98.235% (22987/23400)\n",
      "Training: Epoch 2 - Batch 117/138: Loss: 0.1083 | Train Acc: 98.242% (23185/23600)\n",
      "Training: Epoch 2 - Batch 118/138: Loss: 0.1080 | Train Acc: 98.256% (23385/23800)\n",
      "Training: Epoch 2 - Batch 119/138: Loss: 0.1076 | Train Acc: 98.263% (23583/24000)\n",
      "Training: Epoch 2 - Batch 120/138: Loss: 0.1073 | Train Acc: 98.269% (23781/24200)\n",
      "Training: Epoch 2 - Batch 121/138: Loss: 0.1073 | Train Acc: 98.266% (23977/24400)\n",
      "Training: Epoch 2 - Batch 122/138: Loss: 0.1069 | Train Acc: 98.276% (24176/24600)\n",
      "Training: Epoch 2 - Batch 123/138: Loss: 0.1067 | Train Acc: 98.290% (24376/24800)\n",
      "\n",
      "Dev loss: 0.41244 \n",
      "\n",
      "Training: Epoch 2 - Batch 124/138: Loss: 0.1064 | Train Acc: 98.300% (24575/25000)\n",
      "Training: Epoch 2 - Batch 125/138: Loss: 0.1063 | Train Acc: 98.294% (24770/25200)\n",
      "Training: Epoch 2 - Batch 126/138: Loss: 0.1061 | Train Acc: 98.303% (24969/25400)\n",
      "Training: Epoch 2 - Batch 127/138: Loss: 0.1060 | Train Acc: 98.305% (25166/25600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 2 - Batch 128/138: Loss: 0.1058 | Train Acc: 98.302% (25362/25800)\n",
      "Training: Epoch 2 - Batch 129/138: Loss: 0.1058 | Train Acc: 98.300% (25558/26000)\n",
      "Training: Epoch 2 - Batch 130/138: Loss: 0.1056 | Train Acc: 98.309% (25757/26200)\n",
      "Training: Epoch 2 - Batch 131/138: Loss: 0.1054 | Train Acc: 98.314% (25955/26400)\n",
      "Training: Epoch 2 - Batch 132/138: Loss: 0.1050 | Train Acc: 98.316% (26152/26600)\n",
      "Training: Epoch 2 - Batch 133/138: Loss: 0.1047 | Train Acc: 98.325% (26351/26800)\n",
      "Training: Epoch 2 - Batch 134/138: Loss: 0.1044 | Train Acc: 98.337% (26551/27000)\n",
      "Training: Epoch 2 - Batch 135/138: Loss: 0.1042 | Train Acc: 98.342% (26749/27200)\n",
      "Training: Epoch 2 - Batch 136/138: Loss: 0.1039 | Train Acc: 98.350% (26948/27400)\n",
      "Training: Epoch 2 - Batch 137/138: Loss: 0.1037 | Train Acc: 98.346% (27001/27455)\n",
      "Training: Epoch 3 - Batch 0/138: Loss: 0.0656 | Train Acc: 99.000% (198/200)\n",
      "Training: Epoch 3 - Batch 1/138: Loss: 0.0668 | Train Acc: 99.250% (397/400)\n",
      "Training: Epoch 3 - Batch 2/138: Loss: 0.0767 | Train Acc: 98.833% (593/600)\n",
      "Training: Epoch 3 - Batch 3/138: Loss: 0.0746 | Train Acc: 99.000% (792/800)\n",
      "Training: Epoch 3 - Batch 4/138: Loss: 0.0758 | Train Acc: 99.000% (990/1000)\n",
      "Training: Epoch 3 - Batch 5/138: Loss: 0.0743 | Train Acc: 99.000% (1188/1200)\n",
      "Training: Epoch 3 - Batch 6/138: Loss: 0.0710 | Train Acc: 99.071% (1387/1400)\n",
      "Training: Epoch 3 - Batch 7/138: Loss: 0.0703 | Train Acc: 99.000% (1584/1600)\n",
      "Training: Epoch 3 - Batch 8/138: Loss: 0.0720 | Train Acc: 99.000% (1782/1800)\n",
      "Training: Epoch 3 - Batch 9/138: Loss: 0.0702 | Train Acc: 99.100% (1982/2000)\n",
      "Training: Epoch 3 - Batch 10/138: Loss: 0.0706 | Train Acc: 99.091% (2180/2200)\n",
      "Training: Epoch 3 - Batch 11/138: Loss: 0.0708 | Train Acc: 99.125% (2379/2400)\n",
      "Training: Epoch 3 - Batch 12/138: Loss: 0.0717 | Train Acc: 99.000% (2574/2600)\n",
      "Training: Epoch 3 - Batch 13/138: Loss: 0.0712 | Train Acc: 99.000% (2772/2800)\n",
      "Training: Epoch 3 - Batch 14/138: Loss: 0.0708 | Train Acc: 99.033% (2971/3000)\n",
      "Training: Epoch 3 - Batch 15/138: Loss: 0.0697 | Train Acc: 99.094% (3171/3200)\n",
      "Training: Epoch 3 - Batch 16/138: Loss: 0.0708 | Train Acc: 98.941% (3364/3400)\n",
      "Training: Epoch 3 - Batch 17/138: Loss: 0.0701 | Train Acc: 98.944% (3562/3600)\n",
      "Training: Epoch 3 - Batch 18/138: Loss: 0.0687 | Train Acc: 99.000% (3762/3800)\n",
      "Training: Epoch 3 - Batch 19/138: Loss: 0.0682 | Train Acc: 99.050% (3962/4000)\n",
      "Training: Epoch 3 - Batch 20/138: Loss: 0.0680 | Train Acc: 99.095% (4162/4200)\n",
      "Training: Epoch 3 - Batch 21/138: Loss: 0.0687 | Train Acc: 99.045% (4358/4400)\n",
      "Training: Epoch 3 - Batch 22/138: Loss: 0.0681 | Train Acc: 99.043% (4556/4600)\n",
      "Training: Epoch 3 - Batch 23/138: Loss: 0.0682 | Train Acc: 99.042% (4754/4800)\n",
      "Training: Epoch 3 - Batch 24/138: Loss: 0.0684 | Train Acc: 99.020% (4951/5000)\n",
      "Training: Epoch 3 - Batch 25/138: Loss: 0.0675 | Train Acc: 99.058% (5151/5200)\n",
      "Training: Epoch 3 - Batch 26/138: Loss: 0.0677 | Train Acc: 99.037% (5348/5400)\n",
      "Training: Epoch 3 - Batch 27/138: Loss: 0.0674 | Train Acc: 99.036% (5546/5600)\n",
      "Training: Epoch 3 - Batch 28/138: Loss: 0.0675 | Train Acc: 99.017% (5743/5800)\n",
      "Training: Epoch 3 - Batch 29/138: Loss: 0.0669 | Train Acc: 99.050% (5943/6000)\n",
      "Training: Epoch 3 - Batch 30/138: Loss: 0.0670 | Train Acc: 99.048% (6141/6200)\n",
      "Training: Epoch 3 - Batch 31/138: Loss: 0.0665 | Train Acc: 99.062% (6340/6400)\n",
      "Training: Epoch 3 - Batch 32/138: Loss: 0.0662 | Train Acc: 99.076% (6539/6600)\n",
      "Training: Epoch 3 - Batch 33/138: Loss: 0.0655 | Train Acc: 99.103% (6739/6800)\n",
      "Training: Epoch 3 - Batch 34/138: Loss: 0.0650 | Train Acc: 99.114% (6938/7000)\n",
      "Training: Epoch 3 - Batch 35/138: Loss: 0.0653 | Train Acc: 99.083% (7134/7200)\n",
      "Training: Epoch 3 - Batch 36/138: Loss: 0.0655 | Train Acc: 99.081% (7332/7400)\n",
      "Training: Epoch 3 - Batch 37/138: Loss: 0.0658 | Train Acc: 99.053% (7528/7600)\n",
      "Training: Epoch 3 - Batch 38/138: Loss: 0.0657 | Train Acc: 99.064% (7727/7800)\n",
      "Training: Epoch 3 - Batch 39/138: Loss: 0.0653 | Train Acc: 99.088% (7927/8000)\n",
      "Training: Epoch 3 - Batch 40/138: Loss: 0.0653 | Train Acc: 99.110% (8127/8200)\n",
      "Training: Epoch 3 - Batch 41/138: Loss: 0.0653 | Train Acc: 99.095% (8324/8400)\n",
      "Training: Epoch 3 - Batch 42/138: Loss: 0.0652 | Train Acc: 99.116% (8524/8600)\n",
      "Training: Epoch 3 - Batch 43/138: Loss: 0.0648 | Train Acc: 99.125% (8723/8800)\n",
      "Training: Epoch 3 - Batch 44/138: Loss: 0.0647 | Train Acc: 99.122% (8921/9000)\n",
      "Training: Epoch 3 - Batch 45/138: Loss: 0.0645 | Train Acc: 99.120% (9119/9200)\n",
      "Training: Epoch 3 - Batch 46/138: Loss: 0.0646 | Train Acc: 99.117% (9317/9400)\n",
      "Training: Epoch 3 - Batch 47/138: Loss: 0.0648 | Train Acc: 99.104% (9514/9600)\n",
      "Training: Epoch 3 - Batch 48/138: Loss: 0.0647 | Train Acc: 99.122% (9714/9800)\n",
      "Training: Epoch 3 - Batch 49/138: Loss: 0.0643 | Train Acc: 99.140% (9914/10000)\n",
      "Training: Epoch 3 - Batch 50/138: Loss: 0.0642 | Train Acc: 99.137% (10112/10200)\n",
      "Training: Epoch 3 - Batch 51/138: Loss: 0.0639 | Train Acc: 99.144% (10311/10400)\n",
      "Training: Epoch 3 - Batch 52/138: Loss: 0.0638 | Train Acc: 99.151% (10510/10600)\n",
      "Training: Epoch 3 - Batch 53/138: Loss: 0.0636 | Train Acc: 99.148% (10708/10800)\n",
      "Training: Epoch 3 - Batch 54/138: Loss: 0.0634 | Train Acc: 99.145% (10906/11000)\n",
      "Training: Epoch 3 - Batch 55/138: Loss: 0.0633 | Train Acc: 99.161% (11106/11200)\n",
      "Training: Epoch 3 - Batch 56/138: Loss: 0.0632 | Train Acc: 99.158% (11304/11400)\n",
      "Training: Epoch 3 - Batch 57/138: Loss: 0.0630 | Train Acc: 99.155% (11502/11600)\n",
      "Training: Epoch 3 - Batch 58/138: Loss: 0.0630 | Train Acc: 99.169% (11702/11800)\n",
      "Training: Epoch 3 - Batch 59/138: Loss: 0.0629 | Train Acc: 99.167% (11900/12000)\n",
      "Training: Epoch 3 - Batch 60/138: Loss: 0.0625 | Train Acc: 99.172% (12099/12200)\n",
      "Training: Epoch 3 - Batch 61/138: Loss: 0.0620 | Train Acc: 99.185% (12299/12400)\n",
      "Training: Epoch 3 - Batch 62/138: Loss: 0.0622 | Train Acc: 99.175% (12496/12600)\n",
      "Training: Epoch 3 - Batch 63/138: Loss: 0.0622 | Train Acc: 99.180% (12695/12800)\n",
      "Training: Epoch 3 - Batch 64/138: Loss: 0.0618 | Train Acc: 99.192% (12895/13000)\n",
      "Training: Epoch 3 - Batch 65/138: Loss: 0.0616 | Train Acc: 99.189% (13093/13200)\n",
      "Training: Epoch 3 - Batch 66/138: Loss: 0.0615 | Train Acc: 99.194% (13292/13400)\n",
      "Training: Epoch 3 - Batch 67/138: Loss: 0.0613 | Train Acc: 99.206% (13492/13600)\n",
      "Training: Epoch 3 - Batch 68/138: Loss: 0.0612 | Train Acc: 99.210% (13691/13800)\n",
      "Training: Epoch 3 - Batch 69/138: Loss: 0.0613 | Train Acc: 99.200% (13888/14000)\n",
      "Training: Epoch 3 - Batch 70/138: Loss: 0.0613 | Train Acc: 99.197% (14086/14200)\n",
      "Training: Epoch 3 - Batch 71/138: Loss: 0.0612 | Train Acc: 99.194% (14284/14400)\n",
      "Training: Epoch 3 - Batch 72/138: Loss: 0.0613 | Train Acc: 99.171% (14479/14600)\n",
      "Training: Epoch 3 - Batch 73/138: Loss: 0.0611 | Train Acc: 99.182% (14679/14800)\n",
      "Training: Epoch 3 - Batch 74/138: Loss: 0.0611 | Train Acc: 99.180% (14877/15000)\n",
      "Training: Epoch 3 - Batch 75/138: Loss: 0.0610 | Train Acc: 99.178% (15075/15200)\n",
      "Training: Epoch 3 - Batch 76/138: Loss: 0.0614 | Train Acc: 99.156% (15270/15400)\n",
      "Training: Epoch 3 - Batch 77/138: Loss: 0.0611 | Train Acc: 99.167% (15470/15600)\n",
      "Training: Epoch 3 - Batch 78/138: Loss: 0.0611 | Train Acc: 99.165% (15668/15800)\n",
      "Training: Epoch 3 - Batch 79/138: Loss: 0.0609 | Train Acc: 99.175% (15868/16000)\n",
      "Training: Epoch 3 - Batch 80/138: Loss: 0.0607 | Train Acc: 99.185% (16068/16200)\n",
      "Training: Epoch 3 - Batch 81/138: Loss: 0.0607 | Train Acc: 99.183% (16266/16400)\n",
      "Training: Epoch 3 - Batch 82/138: Loss: 0.0606 | Train Acc: 99.193% (16466/16600)\n",
      "Training: Epoch 3 - Batch 83/138: Loss: 0.0606 | Train Acc: 99.185% (16663/16800)\n",
      "Training: Epoch 3 - Batch 84/138: Loss: 0.0604 | Train Acc: 99.188% (16862/17000)\n",
      "Training: Epoch 3 - Batch 85/138: Loss: 0.0602 | Train Acc: 99.198% (17062/17200)\n",
      "\n",
      "Dev loss: 0.39764 \n",
      "\n",
      "Training: Epoch 3 - Batch 86/138: Loss: 0.0601 | Train Acc: 99.207% (17262/17400)\n",
      "Training: Epoch 3 - Batch 87/138: Loss: 0.0599 | Train Acc: 99.216% (17462/17600)\n",
      "Training: Epoch 3 - Batch 88/138: Loss: 0.0597 | Train Acc: 99.219% (17661/17800)\n",
      "Training: Epoch 3 - Batch 89/138: Loss: 0.0595 | Train Acc: 99.228% (17861/18000)\n",
      "Training: Epoch 3 - Batch 90/138: Loss: 0.0594 | Train Acc: 99.231% (18060/18200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 3 - Batch 91/138: Loss: 0.0592 | Train Acc: 99.239% (18260/18400)\n",
      "Training: Epoch 3 - Batch 92/138: Loss: 0.0591 | Train Acc: 99.237% (18458/18600)\n",
      "Training: Epoch 3 - Batch 93/138: Loss: 0.0590 | Train Acc: 99.239% (18657/18800)\n",
      "Training: Epoch 3 - Batch 94/138: Loss: 0.0589 | Train Acc: 99.242% (18856/19000)\n",
      "Training: Epoch 3 - Batch 95/138: Loss: 0.0587 | Train Acc: 99.250% (19056/19200)\n",
      "Training: Epoch 3 - Batch 96/138: Loss: 0.0587 | Train Acc: 99.247% (19254/19400)\n",
      "Training: Epoch 3 - Batch 97/138: Loss: 0.0586 | Train Acc: 99.255% (19454/19600)\n",
      "Training: Epoch 3 - Batch 98/138: Loss: 0.0584 | Train Acc: 99.258% (19653/19800)\n",
      "Training: Epoch 3 - Batch 99/138: Loss: 0.0583 | Train Acc: 99.265% (19853/20000)\n",
      "Training: Epoch 3 - Batch 100/138: Loss: 0.0580 | Train Acc: 99.267% (20052/20200)\n",
      "Training: Epoch 3 - Batch 101/138: Loss: 0.0579 | Train Acc: 99.270% (20251/20400)\n",
      "Training: Epoch 3 - Batch 102/138: Loss: 0.0577 | Train Acc: 99.277% (20451/20600)\n",
      "Training: Epoch 3 - Batch 103/138: Loss: 0.0576 | Train Acc: 99.284% (20651/20800)\n",
      "Training: Epoch 3 - Batch 104/138: Loss: 0.0576 | Train Acc: 99.286% (20850/21000)\n",
      "Training: Epoch 3 - Batch 105/138: Loss: 0.0574 | Train Acc: 99.292% (21050/21200)\n",
      "Training: Epoch 3 - Batch 106/138: Loss: 0.0573 | Train Acc: 99.290% (21248/21400)\n",
      "Training: Epoch 3 - Batch 107/138: Loss: 0.0572 | Train Acc: 99.292% (21447/21600)\n",
      "Training: Epoch 3 - Batch 108/138: Loss: 0.0571 | Train Acc: 99.294% (21646/21800)\n",
      "Training: Epoch 3 - Batch 109/138: Loss: 0.0570 | Train Acc: 99.295% (21845/22000)\n",
      "Training: Epoch 3 - Batch 110/138: Loss: 0.0568 | Train Acc: 99.302% (22045/22200)\n",
      "Training: Epoch 3 - Batch 111/138: Loss: 0.0567 | Train Acc: 99.308% (22245/22400)\n",
      "Training: Epoch 3 - Batch 112/138: Loss: 0.0565 | Train Acc: 99.305% (22443/22600)\n",
      "Training: Epoch 3 - Batch 113/138: Loss: 0.0563 | Train Acc: 99.307% (22642/22800)\n",
      "Training: Epoch 3 - Batch 114/138: Loss: 0.0562 | Train Acc: 99.313% (22842/23000)\n",
      "Training: Epoch 3 - Batch 115/138: Loss: 0.0561 | Train Acc: 99.310% (23040/23200)\n",
      "Training: Epoch 3 - Batch 116/138: Loss: 0.0560 | Train Acc: 99.312% (23239/23400)\n",
      "Training: Epoch 3 - Batch 117/138: Loss: 0.0560 | Train Acc: 99.305% (23436/23600)\n",
      "Training: Epoch 3 - Batch 118/138: Loss: 0.0559 | Train Acc: 99.311% (23636/23800)\n",
      "Training: Epoch 3 - Batch 119/138: Loss: 0.0559 | Train Acc: 99.308% (23834/24000)\n",
      "Training: Epoch 3 - Batch 120/138: Loss: 0.0559 | Train Acc: 99.306% (24032/24200)\n",
      "Training: Epoch 3 - Batch 121/138: Loss: 0.0558 | Train Acc: 99.311% (24232/24400)\n",
      "Training: Epoch 3 - Batch 122/138: Loss: 0.0556 | Train Acc: 99.317% (24432/24600)\n",
      "Training: Epoch 3 - Batch 123/138: Loss: 0.0555 | Train Acc: 99.319% (24631/24800)\n",
      "Training: Epoch 3 - Batch 124/138: Loss: 0.0556 | Train Acc: 99.316% (24829/25000)\n",
      "Training: Epoch 3 - Batch 125/138: Loss: 0.0555 | Train Acc: 99.317% (25028/25200)\n",
      "Training: Epoch 3 - Batch 126/138: Loss: 0.0554 | Train Acc: 99.319% (25227/25400)\n",
      "Training: Epoch 3 - Batch 127/138: Loss: 0.0553 | Train Acc: 99.320% (25426/25600)\n",
      "Training: Epoch 3 - Batch 128/138: Loss: 0.0552 | Train Acc: 99.322% (25625/25800)\n",
      "Training: Epoch 3 - Batch 129/138: Loss: 0.0550 | Train Acc: 99.323% (25824/26000)\n",
      "Training: Epoch 3 - Batch 130/138: Loss: 0.0550 | Train Acc: 99.321% (26022/26200)\n",
      "Training: Epoch 3 - Batch 131/138: Loss: 0.0549 | Train Acc: 99.322% (26221/26400)\n",
      "Training: Epoch 3 - Batch 132/138: Loss: 0.0547 | Train Acc: 99.327% (26421/26600)\n",
      "Training: Epoch 3 - Batch 133/138: Loss: 0.0546 | Train Acc: 99.325% (26619/26800)\n",
      "Training: Epoch 3 - Batch 134/138: Loss: 0.0547 | Train Acc: 99.322% (26817/27000)\n",
      "Training: Epoch 3 - Batch 135/138: Loss: 0.0546 | Train Acc: 99.324% (27016/27200)\n",
      "Training: Epoch 3 - Batch 136/138: Loss: 0.0545 | Train Acc: 99.328% (27216/27400)\n",
      "Training: Epoch 3 - Batch 137/138: Loss: 0.0544 | Train Acc: 99.326% (27270/27455)\n",
      "Training: Epoch 4 - Batch 0/138: Loss: 0.0447 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 4 - Batch 1/138: Loss: 0.0429 | Train Acc: 100.000% (400/400)\n",
      "Training: Epoch 4 - Batch 2/138: Loss: 0.0417 | Train Acc: 100.000% (600/600)\n",
      "Training: Epoch 4 - Batch 3/138: Loss: 0.0474 | Train Acc: 99.250% (794/800)\n",
      "Training: Epoch 4 - Batch 4/138: Loss: 0.0484 | Train Acc: 99.300% (993/1000)\n",
      "Training: Epoch 4 - Batch 5/138: Loss: 0.0483 | Train Acc: 99.250% (1191/1200)\n",
      "Training: Epoch 4 - Batch 6/138: Loss: 0.0455 | Train Acc: 99.286% (1390/1400)\n",
      "Training: Epoch 4 - Batch 7/138: Loss: 0.0446 | Train Acc: 99.312% (1589/1600)\n",
      "Training: Epoch 4 - Batch 8/138: Loss: 0.0427 | Train Acc: 99.389% (1789/1800)\n",
      "Training: Epoch 4 - Batch 9/138: Loss: 0.0422 | Train Acc: 99.400% (1988/2000)\n",
      "Training: Epoch 4 - Batch 10/138: Loss: 0.0416 | Train Acc: 99.409% (2187/2200)\n",
      "Training: Epoch 4 - Batch 11/138: Loss: 0.0411 | Train Acc: 99.458% (2387/2400)\n",
      "Training: Epoch 4 - Batch 12/138: Loss: 0.0412 | Train Acc: 99.462% (2586/2600)\n",
      "Training: Epoch 4 - Batch 13/138: Loss: 0.0410 | Train Acc: 99.500% (2786/2800)\n",
      "Training: Epoch 4 - Batch 14/138: Loss: 0.0415 | Train Acc: 99.467% (2984/3000)\n",
      "Training: Epoch 4 - Batch 15/138: Loss: 0.0413 | Train Acc: 99.500% (3184/3200)\n",
      "Training: Epoch 4 - Batch 16/138: Loss: 0.0412 | Train Acc: 99.529% (3384/3400)\n",
      "Training: Epoch 4 - Batch 17/138: Loss: 0.0408 | Train Acc: 99.528% (3583/3600)\n",
      "Training: Epoch 4 - Batch 18/138: Loss: 0.0408 | Train Acc: 99.526% (3782/3800)\n",
      "Training: Epoch 4 - Batch 19/138: Loss: 0.0415 | Train Acc: 99.525% (3981/4000)\n",
      "Training: Epoch 4 - Batch 20/138: Loss: 0.0419 | Train Acc: 99.500% (4179/4200)\n",
      "Training: Epoch 4 - Batch 21/138: Loss: 0.0418 | Train Acc: 99.523% (4379/4400)\n",
      "Training: Epoch 4 - Batch 22/138: Loss: 0.0417 | Train Acc: 99.522% (4578/4600)\n",
      "Training: Epoch 4 - Batch 23/138: Loss: 0.0413 | Train Acc: 99.542% (4778/4800)\n",
      "Training: Epoch 4 - Batch 24/138: Loss: 0.0412 | Train Acc: 99.560% (4978/5000)\n",
      "Training: Epoch 4 - Batch 25/138: Loss: 0.0411 | Train Acc: 99.558% (5177/5200)\n",
      "Training: Epoch 4 - Batch 26/138: Loss: 0.0413 | Train Acc: 99.537% (5375/5400)\n",
      "Training: Epoch 4 - Batch 27/138: Loss: 0.0411 | Train Acc: 99.554% (5575/5600)\n",
      "Training: Epoch 4 - Batch 28/138: Loss: 0.0408 | Train Acc: 99.569% (5775/5800)\n",
      "Training: Epoch 4 - Batch 29/138: Loss: 0.0406 | Train Acc: 99.583% (5975/6000)\n",
      "Training: Epoch 4 - Batch 30/138: Loss: 0.0403 | Train Acc: 99.597% (6175/6200)\n",
      "Training: Epoch 4 - Batch 31/138: Loss: 0.0402 | Train Acc: 99.609% (6375/6400)\n",
      "Training: Epoch 4 - Batch 32/138: Loss: 0.0398 | Train Acc: 99.621% (6575/6600)\n",
      "Training: Epoch 4 - Batch 33/138: Loss: 0.0397 | Train Acc: 99.618% (6774/6800)\n",
      "Training: Epoch 4 - Batch 34/138: Loss: 0.0395 | Train Acc: 99.629% (6974/7000)\n",
      "Training: Epoch 4 - Batch 35/138: Loss: 0.0391 | Train Acc: 99.639% (7174/7200)\n",
      "Training: Epoch 4 - Batch 36/138: Loss: 0.0388 | Train Acc: 99.635% (7373/7400)\n",
      "Training: Epoch 4 - Batch 37/138: Loss: 0.0388 | Train Acc: 99.618% (7571/7600)\n",
      "Training: Epoch 4 - Batch 38/138: Loss: 0.0389 | Train Acc: 99.615% (7770/7800)\n",
      "Training: Epoch 4 - Batch 39/138: Loss: 0.0388 | Train Acc: 99.612% (7969/8000)\n",
      "Training: Epoch 4 - Batch 40/138: Loss: 0.0388 | Train Acc: 99.598% (8167/8200)\n",
      "Training: Epoch 4 - Batch 41/138: Loss: 0.0388 | Train Acc: 99.607% (8367/8400)\n",
      "Training: Epoch 4 - Batch 42/138: Loss: 0.0386 | Train Acc: 99.605% (8566/8600)\n",
      "Training: Epoch 4 - Batch 43/138: Loss: 0.0385 | Train Acc: 99.602% (8765/8800)\n",
      "Training: Epoch 4 - Batch 44/138: Loss: 0.0385 | Train Acc: 99.589% (8963/9000)\n",
      "Training: Epoch 4 - Batch 45/138: Loss: 0.0386 | Train Acc: 99.565% (9160/9200)\n",
      "Training: Epoch 4 - Batch 46/138: Loss: 0.0386 | Train Acc: 99.564% (9359/9400)\n",
      "Training: Epoch 4 - Batch 47/138: Loss: 0.0386 | Train Acc: 99.562% (9558/9600)\n",
      "\n",
      "Dev loss: 0.37222 \n",
      "\n",
      "Training: Epoch 4 - Batch 48/138: Loss: 0.0385 | Train Acc: 99.561% (9757/9800)\n",
      "Training: Epoch 4 - Batch 49/138: Loss: 0.0382 | Train Acc: 99.570% (9957/10000)\n",
      "Training: Epoch 4 - Batch 50/138: Loss: 0.0382 | Train Acc: 99.578% (10157/10200)\n",
      "Training: Epoch 4 - Batch 51/138: Loss: 0.0384 | Train Acc: 99.567% (10355/10400)\n",
      "Training: Epoch 4 - Batch 52/138: Loss: 0.0382 | Train Acc: 99.566% (10554/10600)\n",
      "Training: Epoch 4 - Batch 53/138: Loss: 0.0379 | Train Acc: 99.574% (10754/10800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 4 - Batch 54/138: Loss: 0.0378 | Train Acc: 99.573% (10953/11000)\n",
      "Training: Epoch 4 - Batch 55/138: Loss: 0.0378 | Train Acc: 99.562% (11151/11200)\n",
      "Training: Epoch 4 - Batch 56/138: Loss: 0.0378 | Train Acc: 99.561% (11350/11400)\n",
      "Training: Epoch 4 - Batch 57/138: Loss: 0.0376 | Train Acc: 99.569% (11550/11600)\n",
      "Training: Epoch 4 - Batch 58/138: Loss: 0.0374 | Train Acc: 99.576% (11750/11800)\n",
      "Training: Epoch 4 - Batch 59/138: Loss: 0.0373 | Train Acc: 99.583% (11950/12000)\n",
      "Training: Epoch 4 - Batch 60/138: Loss: 0.0372 | Train Acc: 99.574% (12148/12200)\n",
      "Training: Epoch 4 - Batch 61/138: Loss: 0.0372 | Train Acc: 99.581% (12348/12400)\n",
      "Training: Epoch 4 - Batch 62/138: Loss: 0.0372 | Train Acc: 99.579% (12547/12600)\n",
      "Training: Epoch 4 - Batch 63/138: Loss: 0.0371 | Train Acc: 99.586% (12747/12800)\n",
      "Training: Epoch 4 - Batch 64/138: Loss: 0.0371 | Train Acc: 99.585% (12946/13000)\n",
      "Training: Epoch 4 - Batch 65/138: Loss: 0.0370 | Train Acc: 99.591% (13146/13200)\n",
      "Training: Epoch 4 - Batch 66/138: Loss: 0.0370 | Train Acc: 99.590% (13345/13400)\n",
      "Training: Epoch 4 - Batch 67/138: Loss: 0.0371 | Train Acc: 99.588% (13544/13600)\n",
      "Training: Epoch 4 - Batch 68/138: Loss: 0.0370 | Train Acc: 99.594% (13744/13800)\n",
      "Training: Epoch 4 - Batch 69/138: Loss: 0.0370 | Train Acc: 99.600% (13944/14000)\n",
      "Training: Epoch 4 - Batch 70/138: Loss: 0.0369 | Train Acc: 99.606% (14144/14200)\n",
      "Training: Epoch 4 - Batch 71/138: Loss: 0.0367 | Train Acc: 99.611% (14344/14400)\n",
      "Training: Epoch 4 - Batch 72/138: Loss: 0.0366 | Train Acc: 99.616% (14544/14600)\n",
      "Training: Epoch 4 - Batch 73/138: Loss: 0.0367 | Train Acc: 99.615% (14743/14800)\n",
      "Training: Epoch 4 - Batch 74/138: Loss: 0.0366 | Train Acc: 99.607% (14941/15000)\n",
      "Training: Epoch 4 - Batch 75/138: Loss: 0.0368 | Train Acc: 99.586% (15137/15200)\n",
      "Training: Epoch 4 - Batch 76/138: Loss: 0.0368 | Train Acc: 99.584% (15336/15400)\n",
      "Training: Epoch 4 - Batch 77/138: Loss: 0.0367 | Train Acc: 99.590% (15536/15600)\n",
      "Training: Epoch 4 - Batch 78/138: Loss: 0.0368 | Train Acc: 99.589% (15735/15800)\n",
      "Training: Epoch 4 - Batch 79/138: Loss: 0.0366 | Train Acc: 99.594% (15935/16000)\n",
      "Training: Epoch 4 - Batch 80/138: Loss: 0.0366 | Train Acc: 99.586% (16133/16200)\n",
      "Training: Epoch 4 - Batch 81/138: Loss: 0.0367 | Train Acc: 99.579% (16331/16400)\n",
      "Training: Epoch 4 - Batch 82/138: Loss: 0.0369 | Train Acc: 99.578% (16530/16600)\n",
      "Training: Epoch 4 - Batch 83/138: Loss: 0.0369 | Train Acc: 99.571% (16728/16800)\n",
      "Training: Epoch 4 - Batch 84/138: Loss: 0.0368 | Train Acc: 99.576% (16928/17000)\n",
      "Training: Epoch 4 - Batch 85/138: Loss: 0.0368 | Train Acc: 99.576% (17127/17200)\n",
      "Training: Epoch 4 - Batch 86/138: Loss: 0.0368 | Train Acc: 99.580% (17327/17400)\n",
      "Training: Epoch 4 - Batch 87/138: Loss: 0.0367 | Train Acc: 99.574% (17525/17600)\n",
      "Training: Epoch 4 - Batch 88/138: Loss: 0.0366 | Train Acc: 99.573% (17724/17800)\n",
      "Training: Epoch 4 - Batch 89/138: Loss: 0.0366 | Train Acc: 99.572% (17923/18000)\n",
      "Training: Epoch 4 - Batch 90/138: Loss: 0.0366 | Train Acc: 99.577% (18123/18200)\n",
      "Training: Epoch 4 - Batch 91/138: Loss: 0.0365 | Train Acc: 99.582% (18323/18400)\n",
      "Training: Epoch 4 - Batch 92/138: Loss: 0.0364 | Train Acc: 99.586% (18523/18600)\n",
      "Training: Epoch 4 - Batch 93/138: Loss: 0.0363 | Train Acc: 99.585% (18722/18800)\n",
      "Training: Epoch 4 - Batch 94/138: Loss: 0.0362 | Train Acc: 99.589% (18922/19000)\n",
      "Training: Epoch 4 - Batch 95/138: Loss: 0.0362 | Train Acc: 99.589% (19121/19200)\n",
      "Training: Epoch 4 - Batch 96/138: Loss: 0.0360 | Train Acc: 99.593% (19321/19400)\n",
      "Training: Epoch 4 - Batch 97/138: Loss: 0.0359 | Train Acc: 99.592% (19520/19600)\n",
      "Training: Epoch 4 - Batch 98/138: Loss: 0.0358 | Train Acc: 99.596% (19720/19800)\n",
      "Training: Epoch 4 - Batch 99/138: Loss: 0.0357 | Train Acc: 99.600% (19920/20000)\n",
      "Training: Epoch 4 - Batch 100/138: Loss: 0.0355 | Train Acc: 99.604% (20120/20200)\n",
      "Training: Epoch 4 - Batch 101/138: Loss: 0.0354 | Train Acc: 99.603% (20319/20400)\n",
      "Training: Epoch 4 - Batch 102/138: Loss: 0.0356 | Train Acc: 99.592% (20516/20600)\n",
      "Training: Epoch 4 - Batch 103/138: Loss: 0.0355 | Train Acc: 99.596% (20716/20800)\n",
      "Training: Epoch 4 - Batch 104/138: Loss: 0.0354 | Train Acc: 99.600% (20916/21000)\n",
      "Training: Epoch 4 - Batch 105/138: Loss: 0.0353 | Train Acc: 99.599% (21115/21200)\n",
      "Training: Epoch 4 - Batch 106/138: Loss: 0.0353 | Train Acc: 99.598% (21314/21400)\n",
      "Training: Epoch 4 - Batch 107/138: Loss: 0.0354 | Train Acc: 99.597% (21513/21600)\n",
      "Training: Epoch 4 - Batch 108/138: Loss: 0.0353 | Train Acc: 99.592% (21711/21800)\n",
      "Training: Epoch 4 - Batch 109/138: Loss: 0.0353 | Train Acc: 99.591% (21910/22000)\n",
      "Training: Epoch 4 - Batch 110/138: Loss: 0.0353 | Train Acc: 99.590% (22109/22200)\n",
      "Training: Epoch 4 - Batch 111/138: Loss: 0.0352 | Train Acc: 99.594% (22309/22400)\n",
      "Training: Epoch 4 - Batch 112/138: Loss: 0.0352 | Train Acc: 99.597% (22509/22600)\n",
      "Training: Epoch 4 - Batch 113/138: Loss: 0.0352 | Train Acc: 99.592% (22707/22800)\n",
      "Training: Epoch 4 - Batch 114/138: Loss: 0.0350 | Train Acc: 99.596% (22907/23000)\n",
      "Training: Epoch 4 - Batch 115/138: Loss: 0.0349 | Train Acc: 99.595% (23106/23200)\n",
      "Training: Epoch 4 - Batch 116/138: Loss: 0.0349 | Train Acc: 99.594% (23305/23400)\n",
      "Training: Epoch 4 - Batch 117/138: Loss: 0.0349 | Train Acc: 99.593% (23504/23600)\n",
      "Training: Epoch 4 - Batch 118/138: Loss: 0.0348 | Train Acc: 99.597% (23704/23800)\n",
      "Training: Epoch 4 - Batch 119/138: Loss: 0.0347 | Train Acc: 99.600% (23904/24000)\n",
      "Training: Epoch 4 - Batch 120/138: Loss: 0.0347 | Train Acc: 99.599% (24103/24200)\n",
      "Training: Epoch 4 - Batch 121/138: Loss: 0.0346 | Train Acc: 99.602% (24303/24400)\n",
      "Training: Epoch 4 - Batch 122/138: Loss: 0.0345 | Train Acc: 99.602% (24502/24600)\n",
      "Training: Epoch 4 - Batch 123/138: Loss: 0.0344 | Train Acc: 99.605% (24702/24800)\n",
      "Training: Epoch 4 - Batch 124/138: Loss: 0.0343 | Train Acc: 99.608% (24902/25000)\n",
      "Training: Epoch 4 - Batch 125/138: Loss: 0.0344 | Train Acc: 99.599% (25099/25200)\n",
      "Training: Epoch 4 - Batch 126/138: Loss: 0.0343 | Train Acc: 99.602% (25299/25400)\n",
      "Training: Epoch 4 - Batch 127/138: Loss: 0.0343 | Train Acc: 99.602% (25498/25600)\n",
      "Training: Epoch 4 - Batch 128/138: Loss: 0.0342 | Train Acc: 99.605% (25698/25800)\n",
      "Training: Epoch 4 - Batch 129/138: Loss: 0.0341 | Train Acc: 99.608% (25898/26000)\n",
      "Training: Epoch 4 - Batch 130/138: Loss: 0.0341 | Train Acc: 99.611% (26098/26200)\n",
      "Training: Epoch 4 - Batch 131/138: Loss: 0.0340 | Train Acc: 99.614% (26298/26400)\n",
      "Training: Epoch 4 - Batch 132/138: Loss: 0.0339 | Train Acc: 99.613% (26497/26600)\n",
      "Training: Epoch 4 - Batch 133/138: Loss: 0.0338 | Train Acc: 99.616% (26697/26800)\n",
      "Training: Epoch 4 - Batch 134/138: Loss: 0.0337 | Train Acc: 99.619% (26897/27000)\n",
      "Training: Epoch 4 - Batch 135/138: Loss: 0.0337 | Train Acc: 99.618% (27096/27200)\n",
      "Training: Epoch 4 - Batch 136/138: Loss: 0.0337 | Train Acc: 99.617% (27295/27400)\n",
      "Training: Epoch 4 - Batch 137/138: Loss: 0.0336 | Train Acc: 99.618% (27350/27455)\n",
      "Training: Epoch 5 - Batch 0/138: Loss: 0.0300 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 5 - Batch 1/138: Loss: 0.0267 | Train Acc: 100.000% (400/400)\n",
      "Training: Epoch 5 - Batch 2/138: Loss: 0.0242 | Train Acc: 100.000% (600/600)\n",
      "Training: Epoch 5 - Batch 3/138: Loss: 0.0246 | Train Acc: 100.000% (800/800)\n",
      "Training: Epoch 5 - Batch 4/138: Loss: 0.0238 | Train Acc: 100.000% (1000/1000)\n",
      "Training: Epoch 5 - Batch 5/138: Loss: 0.0233 | Train Acc: 99.917% (1199/1200)\n",
      "Training: Epoch 5 - Batch 6/138: Loss: 0.0241 | Train Acc: 99.857% (1398/1400)\n",
      "Training: Epoch 5 - Batch 7/138: Loss: 0.0239 | Train Acc: 99.875% (1598/1600)\n",
      "Training: Epoch 5 - Batch 8/138: Loss: 0.0234 | Train Acc: 99.833% (1797/1800)\n",
      "Training: Epoch 5 - Batch 9/138: Loss: 0.0231 | Train Acc: 99.850% (1997/2000)\n",
      "\n",
      "Dev loss: 0.36418 \n",
      "\n",
      "Training: Epoch 5 - Batch 10/138: Loss: 0.0229 | Train Acc: 99.864% (2197/2200)\n",
      "Training: Epoch 5 - Batch 11/138: Loss: 0.0234 | Train Acc: 99.875% (2397/2400)\n",
      "Training: Epoch 5 - Batch 12/138: Loss: 0.0235 | Train Acc: 99.846% (2596/2600)\n",
      "Training: Epoch 5 - Batch 13/138: Loss: 0.0231 | Train Acc: 99.857% (2796/2800)\n",
      "Training: Epoch 5 - Batch 14/138: Loss: 0.0235 | Train Acc: 99.833% (2995/3000)\n",
      "Training: Epoch 5 - Batch 15/138: Loss: 0.0241 | Train Acc: 99.781% (3193/3200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 5 - Batch 16/138: Loss: 0.0241 | Train Acc: 99.794% (3393/3400)\n",
      "Training: Epoch 5 - Batch 17/138: Loss: 0.0237 | Train Acc: 99.806% (3593/3600)\n",
      "Training: Epoch 5 - Batch 18/138: Loss: 0.0245 | Train Acc: 99.763% (3791/3800)\n",
      "Training: Epoch 5 - Batch 19/138: Loss: 0.0240 | Train Acc: 99.775% (3991/4000)\n",
      "Training: Epoch 5 - Batch 20/138: Loss: 0.0240 | Train Acc: 99.786% (4191/4200)\n",
      "Training: Epoch 5 - Batch 21/138: Loss: 0.0242 | Train Acc: 99.795% (4391/4400)\n",
      "Training: Epoch 5 - Batch 22/138: Loss: 0.0240 | Train Acc: 99.804% (4591/4600)\n",
      "Training: Epoch 5 - Batch 23/138: Loss: 0.0239 | Train Acc: 99.812% (4791/4800)\n",
      "Training: Epoch 5 - Batch 24/138: Loss: 0.0238 | Train Acc: 99.820% (4991/5000)\n",
      "Training: Epoch 5 - Batch 25/138: Loss: 0.0237 | Train Acc: 99.827% (5191/5200)\n",
      "Training: Epoch 5 - Batch 26/138: Loss: 0.0240 | Train Acc: 99.815% (5390/5400)\n",
      "Training: Epoch 5 - Batch 27/138: Loss: 0.0239 | Train Acc: 99.821% (5590/5600)\n",
      "Training: Epoch 5 - Batch 28/138: Loss: 0.0238 | Train Acc: 99.810% (5789/5800)\n",
      "Training: Epoch 5 - Batch 29/138: Loss: 0.0243 | Train Acc: 99.800% (5988/6000)\n",
      "Training: Epoch 5 - Batch 30/138: Loss: 0.0242 | Train Acc: 99.790% (6187/6200)\n",
      "Training: Epoch 5 - Batch 31/138: Loss: 0.0240 | Train Acc: 99.797% (6387/6400)\n",
      "Training: Epoch 5 - Batch 32/138: Loss: 0.0239 | Train Acc: 99.803% (6587/6600)\n",
      "Training: Epoch 5 - Batch 33/138: Loss: 0.0237 | Train Acc: 99.809% (6787/6800)\n",
      "Training: Epoch 5 - Batch 34/138: Loss: 0.0241 | Train Acc: 99.771% (6984/7000)\n",
      "Training: Epoch 5 - Batch 35/138: Loss: 0.0243 | Train Acc: 99.778% (7184/7200)\n",
      "Training: Epoch 5 - Batch 36/138: Loss: 0.0242 | Train Acc: 99.784% (7384/7400)\n",
      "Training: Epoch 5 - Batch 37/138: Loss: 0.0241 | Train Acc: 99.789% (7584/7600)\n",
      "Training: Epoch 5 - Batch 38/138: Loss: 0.0242 | Train Acc: 99.795% (7784/7800)\n",
      "Training: Epoch 5 - Batch 39/138: Loss: 0.0239 | Train Acc: 99.800% (7984/8000)\n",
      "Training: Epoch 5 - Batch 40/138: Loss: 0.0241 | Train Acc: 99.793% (8183/8200)\n",
      "Training: Epoch 5 - Batch 41/138: Loss: 0.0240 | Train Acc: 99.798% (8383/8400)\n",
      "Training: Epoch 5 - Batch 42/138: Loss: 0.0239 | Train Acc: 99.802% (8583/8600)\n",
      "Training: Epoch 5 - Batch 43/138: Loss: 0.0238 | Train Acc: 99.807% (8783/8800)\n",
      "Training: Epoch 5 - Batch 44/138: Loss: 0.0238 | Train Acc: 99.811% (8983/9000)\n",
      "Training: Epoch 5 - Batch 45/138: Loss: 0.0237 | Train Acc: 99.804% (9182/9200)\n",
      "Training: Epoch 5 - Batch 46/138: Loss: 0.0237 | Train Acc: 99.809% (9382/9400)\n",
      "Training: Epoch 5 - Batch 47/138: Loss: 0.0236 | Train Acc: 99.812% (9582/9600)\n",
      "Training: Epoch 5 - Batch 48/138: Loss: 0.0235 | Train Acc: 99.816% (9782/9800)\n",
      "Training: Epoch 5 - Batch 49/138: Loss: 0.0235 | Train Acc: 99.820% (9982/10000)\n",
      "Training: Epoch 5 - Batch 50/138: Loss: 0.0233 | Train Acc: 99.824% (10182/10200)\n",
      "Training: Epoch 5 - Batch 51/138: Loss: 0.0231 | Train Acc: 99.827% (10382/10400)\n",
      "Training: Epoch 5 - Batch 52/138: Loss: 0.0231 | Train Acc: 99.830% (10582/10600)\n",
      "Training: Epoch 5 - Batch 53/138: Loss: 0.0230 | Train Acc: 99.833% (10782/10800)\n",
      "Training: Epoch 5 - Batch 54/138: Loss: 0.0230 | Train Acc: 99.836% (10982/11000)\n",
      "Training: Epoch 5 - Batch 55/138: Loss: 0.0232 | Train Acc: 99.821% (11180/11200)\n",
      "Training: Epoch 5 - Batch 56/138: Loss: 0.0233 | Train Acc: 99.825% (11380/11400)\n",
      "Training: Epoch 5 - Batch 57/138: Loss: 0.0232 | Train Acc: 99.819% (11579/11600)\n",
      "Training: Epoch 5 - Batch 58/138: Loss: 0.0231 | Train Acc: 99.822% (11779/11800)\n",
      "Training: Epoch 5 - Batch 59/138: Loss: 0.0231 | Train Acc: 99.825% (11979/12000)\n",
      "Training: Epoch 5 - Batch 60/138: Loss: 0.0233 | Train Acc: 99.811% (12177/12200)\n",
      "Training: Epoch 5 - Batch 61/138: Loss: 0.0232 | Train Acc: 99.815% (12377/12400)\n",
      "Training: Epoch 5 - Batch 62/138: Loss: 0.0232 | Train Acc: 99.817% (12577/12600)\n",
      "Training: Epoch 5 - Batch 63/138: Loss: 0.0232 | Train Acc: 99.820% (12777/12800)\n",
      "Training: Epoch 5 - Batch 64/138: Loss: 0.0233 | Train Acc: 99.815% (12976/13000)\n",
      "Training: Epoch 5 - Batch 65/138: Loss: 0.0233 | Train Acc: 99.818% (13176/13200)\n",
      "Training: Epoch 5 - Batch 66/138: Loss: 0.0232 | Train Acc: 99.821% (13376/13400)\n",
      "Training: Epoch 5 - Batch 67/138: Loss: 0.0231 | Train Acc: 99.824% (13576/13600)\n",
      "Training: Epoch 5 - Batch 68/138: Loss: 0.0230 | Train Acc: 99.826% (13776/13800)\n",
      "Training: Epoch 5 - Batch 69/138: Loss: 0.0229 | Train Acc: 99.829% (13976/14000)\n",
      "Training: Epoch 5 - Batch 70/138: Loss: 0.0228 | Train Acc: 99.831% (14176/14200)\n",
      "Training: Epoch 5 - Batch 71/138: Loss: 0.0228 | Train Acc: 99.833% (14376/14400)\n",
      "Training: Epoch 5 - Batch 72/138: Loss: 0.0228 | Train Acc: 99.829% (14575/14600)\n",
      "Training: Epoch 5 - Batch 73/138: Loss: 0.0231 | Train Acc: 99.818% (14773/14800)\n",
      "Training: Epoch 5 - Batch 74/138: Loss: 0.0233 | Train Acc: 99.807% (14971/15000)\n",
      "Training: Epoch 5 - Batch 75/138: Loss: 0.0232 | Train Acc: 99.809% (15171/15200)\n",
      "Training: Epoch 5 - Batch 76/138: Loss: 0.0232 | Train Acc: 99.812% (15371/15400)\n",
      "Training: Epoch 5 - Batch 77/138: Loss: 0.0231 | Train Acc: 99.814% (15571/15600)\n",
      "Training: Epoch 5 - Batch 78/138: Loss: 0.0232 | Train Acc: 99.810% (15770/15800)\n",
      "Training: Epoch 5 - Batch 79/138: Loss: 0.0231 | Train Acc: 99.812% (15970/16000)\n",
      "Training: Epoch 5 - Batch 80/138: Loss: 0.0231 | Train Acc: 99.815% (16170/16200)\n",
      "Training: Epoch 5 - Batch 81/138: Loss: 0.0232 | Train Acc: 99.805% (16368/16400)\n",
      "Training: Epoch 5 - Batch 82/138: Loss: 0.0231 | Train Acc: 99.801% (16567/16600)\n",
      "Training: Epoch 5 - Batch 83/138: Loss: 0.0232 | Train Acc: 99.804% (16767/16800)\n",
      "Training: Epoch 5 - Batch 84/138: Loss: 0.0232 | Train Acc: 99.800% (16966/17000)\n",
      "Training: Epoch 5 - Batch 85/138: Loss: 0.0232 | Train Acc: 99.791% (17164/17200)\n",
      "Training: Epoch 5 - Batch 86/138: Loss: 0.0232 | Train Acc: 99.793% (17364/17400)\n",
      "Training: Epoch 5 - Batch 87/138: Loss: 0.0230 | Train Acc: 99.795% (17564/17600)\n",
      "Training: Epoch 5 - Batch 88/138: Loss: 0.0230 | Train Acc: 99.798% (17764/17800)\n",
      "Training: Epoch 5 - Batch 89/138: Loss: 0.0230 | Train Acc: 99.800% (17964/18000)\n",
      "Training: Epoch 5 - Batch 90/138: Loss: 0.0230 | Train Acc: 99.802% (18164/18200)\n",
      "Training: Epoch 5 - Batch 91/138: Loss: 0.0229 | Train Acc: 99.804% (18364/18400)\n",
      "Training: Epoch 5 - Batch 92/138: Loss: 0.0229 | Train Acc: 99.806% (18564/18600)\n",
      "Training: Epoch 5 - Batch 93/138: Loss: 0.0229 | Train Acc: 99.803% (18763/18800)\n",
      "Training: Epoch 5 - Batch 94/138: Loss: 0.0229 | Train Acc: 99.805% (18963/19000)\n",
      "Training: Epoch 5 - Batch 95/138: Loss: 0.0227 | Train Acc: 99.807% (19163/19200)\n",
      "Training: Epoch 5 - Batch 96/138: Loss: 0.0227 | Train Acc: 99.809% (19363/19400)\n",
      "Training: Epoch 5 - Batch 97/138: Loss: 0.0227 | Train Acc: 99.811% (19563/19600)\n",
      "Training: Epoch 5 - Batch 98/138: Loss: 0.0227 | Train Acc: 99.813% (19763/19800)\n",
      "Training: Epoch 5 - Batch 99/138: Loss: 0.0227 | Train Acc: 99.810% (19962/20000)\n",
      "Training: Epoch 5 - Batch 100/138: Loss: 0.0226 | Train Acc: 99.812% (20162/20200)\n",
      "Training: Epoch 5 - Batch 101/138: Loss: 0.0227 | Train Acc: 99.809% (20361/20400)\n",
      "Training: Epoch 5 - Batch 102/138: Loss: 0.0227 | Train Acc: 99.806% (20560/20600)\n",
      "Training: Epoch 5 - Batch 103/138: Loss: 0.0228 | Train Acc: 99.803% (20759/20800)\n",
      "Training: Epoch 5 - Batch 104/138: Loss: 0.0228 | Train Acc: 99.800% (20958/21000)\n",
      "Training: Epoch 5 - Batch 105/138: Loss: 0.0229 | Train Acc: 99.802% (21158/21200)\n",
      "Training: Epoch 5 - Batch 106/138: Loss: 0.0229 | Train Acc: 99.804% (21358/21400)\n",
      "Training: Epoch 5 - Batch 107/138: Loss: 0.0228 | Train Acc: 99.806% (21558/21600)\n",
      "Training: Epoch 5 - Batch 108/138: Loss: 0.0228 | Train Acc: 99.803% (21757/21800)\n",
      "Training: Epoch 5 - Batch 109/138: Loss: 0.0228 | Train Acc: 99.800% (21956/22000)\n",
      "\n",
      "Dev loss: 0.36759 \n",
      "\n",
      "Training: Epoch 5 - Batch 110/138: Loss: 0.0228 | Train Acc: 99.802% (22156/22200)\n",
      "Training: Epoch 5 - Batch 111/138: Loss: 0.0228 | Train Acc: 99.799% (22355/22400)\n",
      "Training: Epoch 5 - Batch 112/138: Loss: 0.0228 | Train Acc: 99.801% (22555/22600)\n",
      "Training: Epoch 5 - Batch 113/138: Loss: 0.0228 | Train Acc: 99.803% (22755/22800)\n",
      "Training: Epoch 5 - Batch 114/138: Loss: 0.0227 | Train Acc: 99.804% (22955/23000)\n",
      "Training: Epoch 5 - Batch 115/138: Loss: 0.0228 | Train Acc: 99.797% (23153/23200)\n",
      "Training: Epoch 5 - Batch 116/138: Loss: 0.0228 | Train Acc: 99.799% (23353/23400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 5 - Batch 117/138: Loss: 0.0227 | Train Acc: 99.801% (23553/23600)\n",
      "Training: Epoch 5 - Batch 118/138: Loss: 0.0227 | Train Acc: 99.803% (23753/23800)\n",
      "Training: Epoch 5 - Batch 119/138: Loss: 0.0227 | Train Acc: 99.804% (23953/24000)\n",
      "Training: Epoch 5 - Batch 120/138: Loss: 0.0227 | Train Acc: 99.806% (24153/24200)\n",
      "Training: Epoch 5 - Batch 121/138: Loss: 0.0229 | Train Acc: 99.791% (24349/24400)\n",
      "Training: Epoch 5 - Batch 122/138: Loss: 0.0228 | Train Acc: 99.793% (24549/24600)\n",
      "Training: Epoch 5 - Batch 123/138: Loss: 0.0227 | Train Acc: 99.794% (24749/24800)\n",
      "Training: Epoch 5 - Batch 124/138: Loss: 0.0226 | Train Acc: 99.796% (24949/25000)\n",
      "Training: Epoch 5 - Batch 125/138: Loss: 0.0226 | Train Acc: 99.794% (25148/25200)\n",
      "Training: Epoch 5 - Batch 126/138: Loss: 0.0226 | Train Acc: 99.795% (25348/25400)\n",
      "Training: Epoch 5 - Batch 127/138: Loss: 0.0226 | Train Acc: 99.797% (25548/25600)\n",
      "Training: Epoch 5 - Batch 128/138: Loss: 0.0226 | Train Acc: 99.798% (25748/25800)\n",
      "Training: Epoch 5 - Batch 129/138: Loss: 0.0226 | Train Acc: 99.800% (25948/26000)\n",
      "Training: Epoch 5 - Batch 130/138: Loss: 0.0225 | Train Acc: 99.802% (26148/26200)\n",
      "Training: Epoch 5 - Batch 131/138: Loss: 0.0225 | Train Acc: 99.803% (26348/26400)\n",
      "Training: Epoch 5 - Batch 132/138: Loss: 0.0224 | Train Acc: 99.801% (26547/26600)\n",
      "Training: Epoch 5 - Batch 133/138: Loss: 0.0224 | Train Acc: 99.802% (26747/26800)\n",
      "Training: Epoch 5 - Batch 134/138: Loss: 0.0224 | Train Acc: 99.804% (26947/27000)\n",
      "Training: Epoch 5 - Batch 135/138: Loss: 0.0223 | Train Acc: 99.805% (27147/27200)\n",
      "Training: Epoch 5 - Batch 136/138: Loss: 0.0222 | Train Acc: 99.807% (27347/27400)\n",
      "Training: Epoch 5 - Batch 137/138: Loss: 0.0222 | Train Acc: 99.807% (27402/27455)\n",
      "Training: Epoch 6 - Batch 0/138: Loss: 0.0146 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 6 - Batch 1/138: Loss: 0.0182 | Train Acc: 99.750% (399/400)\n",
      "Training: Epoch 6 - Batch 2/138: Loss: 0.0206 | Train Acc: 99.667% (598/600)\n",
      "Training: Epoch 6 - Batch 3/138: Loss: 0.0225 | Train Acc: 99.625% (797/800)\n",
      "Training: Epoch 6 - Batch 4/138: Loss: 0.0213 | Train Acc: 99.700% (997/1000)\n",
      "Training: Epoch 6 - Batch 5/138: Loss: 0.0203 | Train Acc: 99.750% (1197/1200)\n",
      "Training: Epoch 6 - Batch 6/138: Loss: 0.0196 | Train Acc: 99.786% (1397/1400)\n",
      "Training: Epoch 6 - Batch 7/138: Loss: 0.0188 | Train Acc: 99.812% (1597/1600)\n",
      "Training: Epoch 6 - Batch 8/138: Loss: 0.0187 | Train Acc: 99.833% (1797/1800)\n",
      "Training: Epoch 6 - Batch 9/138: Loss: 0.0181 | Train Acc: 99.850% (1997/2000)\n",
      "Training: Epoch 6 - Batch 10/138: Loss: 0.0185 | Train Acc: 99.864% (2197/2200)\n",
      "Training: Epoch 6 - Batch 11/138: Loss: 0.0187 | Train Acc: 99.875% (2397/2400)\n",
      "Training: Epoch 6 - Batch 12/138: Loss: 0.0184 | Train Acc: 99.885% (2597/2600)\n",
      "Training: Epoch 6 - Batch 13/138: Loss: 0.0186 | Train Acc: 99.893% (2797/2800)\n",
      "Training: Epoch 6 - Batch 14/138: Loss: 0.0185 | Train Acc: 99.900% (2997/3000)\n",
      "Training: Epoch 6 - Batch 15/138: Loss: 0.0182 | Train Acc: 99.906% (3197/3200)\n",
      "Training: Epoch 6 - Batch 16/138: Loss: 0.0182 | Train Acc: 99.912% (3397/3400)\n",
      "Training: Epoch 6 - Batch 17/138: Loss: 0.0177 | Train Acc: 99.917% (3597/3600)\n",
      "Training: Epoch 6 - Batch 18/138: Loss: 0.0178 | Train Acc: 99.895% (3796/3800)\n",
      "Training: Epoch 6 - Batch 19/138: Loss: 0.0180 | Train Acc: 99.900% (3996/4000)\n",
      "Training: Epoch 6 - Batch 20/138: Loss: 0.0179 | Train Acc: 99.905% (4196/4200)\n",
      "Training: Epoch 6 - Batch 21/138: Loss: 0.0177 | Train Acc: 99.909% (4396/4400)\n",
      "Training: Epoch 6 - Batch 22/138: Loss: 0.0181 | Train Acc: 99.891% (4595/4600)\n",
      "Training: Epoch 6 - Batch 23/138: Loss: 0.0182 | Train Acc: 99.896% (4795/4800)\n",
      "Training: Epoch 6 - Batch 24/138: Loss: 0.0183 | Train Acc: 99.900% (4995/5000)\n",
      "Training: Epoch 6 - Batch 25/138: Loss: 0.0183 | Train Acc: 99.904% (5195/5200)\n",
      "Training: Epoch 6 - Batch 26/138: Loss: 0.0181 | Train Acc: 99.907% (5395/5400)\n",
      "Training: Epoch 6 - Batch 27/138: Loss: 0.0182 | Train Acc: 99.911% (5595/5600)\n",
      "Training: Epoch 6 - Batch 28/138: Loss: 0.0181 | Train Acc: 99.914% (5795/5800)\n",
      "Training: Epoch 6 - Batch 29/138: Loss: 0.0182 | Train Acc: 99.900% (5994/6000)\n",
      "Training: Epoch 6 - Batch 30/138: Loss: 0.0183 | Train Acc: 99.903% (6194/6200)\n",
      "Training: Epoch 6 - Batch 31/138: Loss: 0.0181 | Train Acc: 99.906% (6394/6400)\n",
      "Training: Epoch 6 - Batch 32/138: Loss: 0.0179 | Train Acc: 99.909% (6594/6600)\n",
      "Training: Epoch 6 - Batch 33/138: Loss: 0.0179 | Train Acc: 99.912% (6794/6800)\n",
      "Training: Epoch 6 - Batch 34/138: Loss: 0.0177 | Train Acc: 99.914% (6994/7000)\n",
      "Training: Epoch 6 - Batch 35/138: Loss: 0.0179 | Train Acc: 99.903% (7193/7200)\n",
      "Training: Epoch 6 - Batch 36/138: Loss: 0.0180 | Train Acc: 99.892% (7392/7400)\n",
      "Training: Epoch 6 - Batch 37/138: Loss: 0.0180 | Train Acc: 99.895% (7592/7600)\n",
      "Training: Epoch 6 - Batch 38/138: Loss: 0.0179 | Train Acc: 99.897% (7792/7800)\n",
      "Training: Epoch 6 - Batch 39/138: Loss: 0.0177 | Train Acc: 99.900% (7992/8000)\n",
      "Training: Epoch 6 - Batch 40/138: Loss: 0.0178 | Train Acc: 99.890% (8191/8200)\n",
      "Training: Epoch 6 - Batch 41/138: Loss: 0.0178 | Train Acc: 99.881% (8390/8400)\n",
      "Training: Epoch 6 - Batch 42/138: Loss: 0.0177 | Train Acc: 99.884% (8590/8600)\n",
      "Training: Epoch 6 - Batch 43/138: Loss: 0.0176 | Train Acc: 99.886% (8790/8800)\n",
      "Training: Epoch 6 - Batch 44/138: Loss: 0.0176 | Train Acc: 99.889% (8990/9000)\n",
      "Training: Epoch 6 - Batch 45/138: Loss: 0.0177 | Train Acc: 99.880% (9189/9200)\n",
      "Training: Epoch 6 - Batch 46/138: Loss: 0.0176 | Train Acc: 99.883% (9389/9400)\n",
      "Training: Epoch 6 - Batch 47/138: Loss: 0.0175 | Train Acc: 99.885% (9589/9600)\n",
      "Training: Epoch 6 - Batch 48/138: Loss: 0.0174 | Train Acc: 99.888% (9789/9800)\n",
      "Training: Epoch 6 - Batch 49/138: Loss: 0.0173 | Train Acc: 99.890% (9989/10000)\n",
      "Training: Epoch 6 - Batch 50/138: Loss: 0.0172 | Train Acc: 99.892% (10189/10200)\n",
      "Training: Epoch 6 - Batch 51/138: Loss: 0.0172 | Train Acc: 99.894% (10389/10400)\n",
      "Training: Epoch 6 - Batch 52/138: Loss: 0.0171 | Train Acc: 99.896% (10589/10600)\n",
      "Training: Epoch 6 - Batch 53/138: Loss: 0.0171 | Train Acc: 99.898% (10789/10800)\n",
      "Training: Epoch 6 - Batch 54/138: Loss: 0.0172 | Train Acc: 99.891% (10988/11000)\n",
      "Training: Epoch 6 - Batch 55/138: Loss: 0.0172 | Train Acc: 99.893% (11188/11200)\n",
      "Training: Epoch 6 - Batch 56/138: Loss: 0.0172 | Train Acc: 99.886% (11387/11400)\n",
      "Training: Epoch 6 - Batch 57/138: Loss: 0.0171 | Train Acc: 99.888% (11587/11600)\n",
      "Training: Epoch 6 - Batch 58/138: Loss: 0.0170 | Train Acc: 99.890% (11787/11800)\n",
      "Training: Epoch 6 - Batch 59/138: Loss: 0.0174 | Train Acc: 99.875% (11985/12000)\n",
      "Training: Epoch 6 - Batch 60/138: Loss: 0.0174 | Train Acc: 99.877% (12185/12200)\n",
      "Training: Epoch 6 - Batch 61/138: Loss: 0.0173 | Train Acc: 99.879% (12385/12400)\n",
      "Training: Epoch 6 - Batch 62/138: Loss: 0.0172 | Train Acc: 99.881% (12585/12600)\n",
      "Training: Epoch 6 - Batch 63/138: Loss: 0.0171 | Train Acc: 99.883% (12785/12800)\n",
      "Training: Epoch 6 - Batch 64/138: Loss: 0.0171 | Train Acc: 99.885% (12985/13000)\n",
      "Training: Epoch 6 - Batch 65/138: Loss: 0.0171 | Train Acc: 99.886% (13185/13200)\n",
      "Training: Epoch 6 - Batch 66/138: Loss: 0.0171 | Train Acc: 99.888% (13385/13400)\n",
      "Training: Epoch 6 - Batch 67/138: Loss: 0.0171 | Train Acc: 99.890% (13585/13600)\n",
      "Training: Epoch 6 - Batch 68/138: Loss: 0.0170 | Train Acc: 99.891% (13785/13800)\n",
      "Training: Epoch 6 - Batch 69/138: Loss: 0.0171 | Train Acc: 99.886% (13984/14000)\n",
      "Training: Epoch 6 - Batch 70/138: Loss: 0.0171 | Train Acc: 99.880% (14183/14200)\n",
      "Training: Epoch 6 - Batch 71/138: Loss: 0.0170 | Train Acc: 99.882% (14383/14400)\n",
      "\n",
      "Dev loss: 0.35171 \n",
      "\n",
      "Training: Epoch 6 - Batch 72/138: Loss: 0.0170 | Train Acc: 99.884% (14583/14600)\n",
      "Training: Epoch 6 - Batch 73/138: Loss: 0.0169 | Train Acc: 99.885% (14783/14800)\n",
      "Training: Epoch 6 - Batch 74/138: Loss: 0.0168 | Train Acc: 99.887% (14983/15000)\n",
      "Training: Epoch 6 - Batch 75/138: Loss: 0.0169 | Train Acc: 99.882% (15182/15200)\n",
      "Training: Epoch 6 - Batch 76/138: Loss: 0.0169 | Train Acc: 99.883% (15382/15400)\n",
      "Training: Epoch 6 - Batch 77/138: Loss: 0.0168 | Train Acc: 99.885% (15582/15600)\n",
      "Training: Epoch 6 - Batch 78/138: Loss: 0.0168 | Train Acc: 99.886% (15782/15800)\n",
      "Training: Epoch 6 - Batch 79/138: Loss: 0.0168 | Train Acc: 99.888% (15982/16000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 6 - Batch 80/138: Loss: 0.0168 | Train Acc: 99.889% (16182/16200)\n",
      "Training: Epoch 6 - Batch 81/138: Loss: 0.0168 | Train Acc: 99.884% (16381/16400)\n",
      "Training: Epoch 6 - Batch 82/138: Loss: 0.0167 | Train Acc: 99.886% (16581/16600)\n",
      "Training: Epoch 6 - Batch 83/138: Loss: 0.0166 | Train Acc: 99.887% (16781/16800)\n",
      "Training: Epoch 6 - Batch 84/138: Loss: 0.0166 | Train Acc: 99.882% (16980/17000)\n",
      "Training: Epoch 6 - Batch 85/138: Loss: 0.0166 | Train Acc: 99.884% (17180/17200)\n",
      "Training: Epoch 6 - Batch 86/138: Loss: 0.0165 | Train Acc: 99.885% (17380/17400)\n",
      "Training: Epoch 6 - Batch 87/138: Loss: 0.0165 | Train Acc: 99.886% (17580/17600)\n",
      "Training: Epoch 6 - Batch 88/138: Loss: 0.0164 | Train Acc: 99.888% (17780/17800)\n",
      "Training: Epoch 6 - Batch 89/138: Loss: 0.0163 | Train Acc: 99.889% (17980/18000)\n",
      "Training: Epoch 6 - Batch 90/138: Loss: 0.0163 | Train Acc: 99.890% (18180/18200)\n",
      "Training: Epoch 6 - Batch 91/138: Loss: 0.0164 | Train Acc: 99.886% (18379/18400)\n",
      "Training: Epoch 6 - Batch 92/138: Loss: 0.0164 | Train Acc: 99.882% (18578/18600)\n",
      "Training: Epoch 6 - Batch 93/138: Loss: 0.0165 | Train Acc: 99.872% (18776/18800)\n",
      "Training: Epoch 6 - Batch 94/138: Loss: 0.0165 | Train Acc: 99.874% (18976/19000)\n",
      "Training: Epoch 6 - Batch 95/138: Loss: 0.0165 | Train Acc: 99.875% (19176/19200)\n",
      "Training: Epoch 6 - Batch 96/138: Loss: 0.0165 | Train Acc: 99.871% (19375/19400)\n",
      "Training: Epoch 6 - Batch 97/138: Loss: 0.0164 | Train Acc: 99.872% (19575/19600)\n",
      "Training: Epoch 6 - Batch 98/138: Loss: 0.0164 | Train Acc: 99.874% (19775/19800)\n",
      "Training: Epoch 6 - Batch 99/138: Loss: 0.0164 | Train Acc: 99.875% (19975/20000)\n",
      "Training: Epoch 6 - Batch 100/138: Loss: 0.0164 | Train Acc: 99.866% (20173/20200)\n",
      "Training: Epoch 6 - Batch 101/138: Loss: 0.0164 | Train Acc: 99.868% (20373/20400)\n",
      "Training: Epoch 6 - Batch 102/138: Loss: 0.0164 | Train Acc: 99.869% (20573/20600)\n",
      "Training: Epoch 6 - Batch 103/138: Loss: 0.0164 | Train Acc: 99.870% (20773/20800)\n",
      "Training: Epoch 6 - Batch 104/138: Loss: 0.0164 | Train Acc: 99.871% (20973/21000)\n",
      "Training: Epoch 6 - Batch 105/138: Loss: 0.0164 | Train Acc: 99.873% (21173/21200)\n",
      "Training: Epoch 6 - Batch 106/138: Loss: 0.0164 | Train Acc: 99.874% (21373/21400)\n",
      "Training: Epoch 6 - Batch 107/138: Loss: 0.0163 | Train Acc: 99.875% (21573/21600)\n",
      "Training: Epoch 6 - Batch 108/138: Loss: 0.0163 | Train Acc: 99.876% (21773/21800)\n",
      "Training: Epoch 6 - Batch 109/138: Loss: 0.0163 | Train Acc: 99.877% (21973/22000)\n",
      "Training: Epoch 6 - Batch 110/138: Loss: 0.0163 | Train Acc: 99.878% (22173/22200)\n",
      "Training: Epoch 6 - Batch 111/138: Loss: 0.0162 | Train Acc: 99.879% (22373/22400)\n",
      "Training: Epoch 6 - Batch 112/138: Loss: 0.0162 | Train Acc: 99.881% (22573/22600)\n",
      "Training: Epoch 6 - Batch 113/138: Loss: 0.0162 | Train Acc: 99.882% (22773/22800)\n",
      "Training: Epoch 6 - Batch 114/138: Loss: 0.0161 | Train Acc: 99.883% (22973/23000)\n",
      "Training: Epoch 6 - Batch 115/138: Loss: 0.0162 | Train Acc: 99.884% (23173/23200)\n",
      "Training: Epoch 6 - Batch 116/138: Loss: 0.0161 | Train Acc: 99.885% (23373/23400)\n",
      "Training: Epoch 6 - Batch 117/138: Loss: 0.0162 | Train Acc: 99.886% (23573/23600)\n",
      "Training: Epoch 6 - Batch 118/138: Loss: 0.0162 | Train Acc: 99.882% (23772/23800)\n",
      "Training: Epoch 6 - Batch 119/138: Loss: 0.0162 | Train Acc: 99.883% (23972/24000)\n",
      "Training: Epoch 6 - Batch 120/138: Loss: 0.0161 | Train Acc: 99.884% (24172/24200)\n",
      "Training: Epoch 6 - Batch 121/138: Loss: 0.0161 | Train Acc: 99.885% (24372/24400)\n",
      "Training: Epoch 6 - Batch 122/138: Loss: 0.0160 | Train Acc: 99.886% (24572/24600)\n",
      "Training: Epoch 6 - Batch 123/138: Loss: 0.0160 | Train Acc: 99.887% (24772/24800)\n",
      "Training: Epoch 6 - Batch 124/138: Loss: 0.0159 | Train Acc: 99.888% (24972/25000)\n",
      "Training: Epoch 6 - Batch 125/138: Loss: 0.0159 | Train Acc: 99.889% (25172/25200)\n",
      "Training: Epoch 6 - Batch 126/138: Loss: 0.0159 | Train Acc: 99.890% (25372/25400)\n",
      "Training: Epoch 6 - Batch 127/138: Loss: 0.0159 | Train Acc: 99.887% (25571/25600)\n",
      "Training: Epoch 6 - Batch 128/138: Loss: 0.0158 | Train Acc: 99.888% (25771/25800)\n",
      "Training: Epoch 6 - Batch 129/138: Loss: 0.0158 | Train Acc: 99.888% (25971/26000)\n",
      "Training: Epoch 6 - Batch 130/138: Loss: 0.0158 | Train Acc: 99.889% (26171/26200)\n",
      "Training: Epoch 6 - Batch 131/138: Loss: 0.0158 | Train Acc: 99.890% (26371/26400)\n",
      "Training: Epoch 6 - Batch 132/138: Loss: 0.0158 | Train Acc: 99.891% (26571/26600)\n",
      "Training: Epoch 6 - Batch 133/138: Loss: 0.0158 | Train Acc: 99.892% (26771/26800)\n",
      "Training: Epoch 6 - Batch 134/138: Loss: 0.0158 | Train Acc: 99.893% (26971/27000)\n",
      "Training: Epoch 6 - Batch 135/138: Loss: 0.0158 | Train Acc: 99.893% (27171/27200)\n",
      "Training: Epoch 6 - Batch 136/138: Loss: 0.0157 | Train Acc: 99.894% (27371/27400)\n",
      "Training: Epoch 6 - Batch 137/138: Loss: 0.0157 | Train Acc: 99.894% (27426/27455)\n",
      "Training: Epoch 7 - Batch 0/138: Loss: 0.0120 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 7 - Batch 1/138: Loss: 0.0132 | Train Acc: 100.000% (400/400)\n",
      "Training: Epoch 7 - Batch 2/138: Loss: 0.0158 | Train Acc: 99.833% (599/600)\n",
      "Training: Epoch 7 - Batch 3/138: Loss: 0.0155 | Train Acc: 99.875% (799/800)\n",
      "Training: Epoch 7 - Batch 4/138: Loss: 0.0153 | Train Acc: 99.900% (999/1000)\n",
      "Training: Epoch 7 - Batch 5/138: Loss: 0.0155 | Train Acc: 99.917% (1199/1200)\n",
      "Training: Epoch 7 - Batch 6/138: Loss: 0.0158 | Train Acc: 99.929% (1399/1400)\n",
      "Training: Epoch 7 - Batch 7/138: Loss: 0.0148 | Train Acc: 99.938% (1599/1600)\n",
      "Training: Epoch 7 - Batch 8/138: Loss: 0.0150 | Train Acc: 99.944% (1799/1800)\n",
      "Training: Epoch 7 - Batch 9/138: Loss: 0.0151 | Train Acc: 99.950% (1999/2000)\n",
      "Training: Epoch 7 - Batch 10/138: Loss: 0.0147 | Train Acc: 99.955% (2199/2200)\n",
      "Training: Epoch 7 - Batch 11/138: Loss: 0.0143 | Train Acc: 99.958% (2399/2400)\n",
      "Training: Epoch 7 - Batch 12/138: Loss: 0.0140 | Train Acc: 99.962% (2599/2600)\n",
      "Training: Epoch 7 - Batch 13/138: Loss: 0.0138 | Train Acc: 99.929% (2798/2800)\n",
      "Training: Epoch 7 - Batch 14/138: Loss: 0.0143 | Train Acc: 99.900% (2997/3000)\n",
      "Training: Epoch 7 - Batch 15/138: Loss: 0.0143 | Train Acc: 99.906% (3197/3200)\n",
      "Training: Epoch 7 - Batch 16/138: Loss: 0.0142 | Train Acc: 99.912% (3397/3400)\n",
      "Training: Epoch 7 - Batch 17/138: Loss: 0.0142 | Train Acc: 99.889% (3596/3600)\n",
      "Training: Epoch 7 - Batch 18/138: Loss: 0.0142 | Train Acc: 99.868% (3795/3800)\n",
      "Training: Epoch 7 - Batch 19/138: Loss: 0.0140 | Train Acc: 99.875% (3995/4000)\n",
      "Training: Epoch 7 - Batch 20/138: Loss: 0.0141 | Train Acc: 99.857% (4194/4200)\n",
      "Training: Epoch 7 - Batch 21/138: Loss: 0.0139 | Train Acc: 99.864% (4394/4400)\n",
      "Training: Epoch 7 - Batch 22/138: Loss: 0.0148 | Train Acc: 99.848% (4593/4600)\n",
      "Training: Epoch 7 - Batch 23/138: Loss: 0.0147 | Train Acc: 99.854% (4793/4800)\n",
      "Training: Epoch 7 - Batch 24/138: Loss: 0.0146 | Train Acc: 99.860% (4993/5000)\n",
      "Training: Epoch 7 - Batch 25/138: Loss: 0.0145 | Train Acc: 99.865% (5193/5200)\n",
      "Training: Epoch 7 - Batch 26/138: Loss: 0.0144 | Train Acc: 99.870% (5393/5400)\n",
      "Training: Epoch 7 - Batch 27/138: Loss: 0.0143 | Train Acc: 99.857% (5592/5600)\n",
      "Training: Epoch 7 - Batch 28/138: Loss: 0.0141 | Train Acc: 99.862% (5792/5800)\n",
      "Training: Epoch 7 - Batch 29/138: Loss: 0.0141 | Train Acc: 99.867% (5992/6000)\n",
      "Training: Epoch 7 - Batch 30/138: Loss: 0.0142 | Train Acc: 99.855% (6191/6200)\n",
      "Training: Epoch 7 - Batch 31/138: Loss: 0.0141 | Train Acc: 99.859% (6391/6400)\n",
      "Training: Epoch 7 - Batch 32/138: Loss: 0.0139 | Train Acc: 99.864% (6591/6600)\n",
      "Training: Epoch 7 - Batch 33/138: Loss: 0.0138 | Train Acc: 99.868% (6791/6800)\n",
      "\n",
      "Dev loss: 0.35569 \n",
      "\n",
      "Training: Epoch 7 - Batch 34/138: Loss: 0.0136 | Train Acc: 99.871% (6991/7000)\n",
      "Training: Epoch 7 - Batch 35/138: Loss: 0.0135 | Train Acc: 99.875% (7191/7200)\n",
      "Training: Epoch 7 - Batch 36/138: Loss: 0.0134 | Train Acc: 99.878% (7391/7400)\n",
      "Training: Epoch 7 - Batch 37/138: Loss: 0.0134 | Train Acc: 99.882% (7591/7600)\n",
      "Training: Epoch 7 - Batch 38/138: Loss: 0.0133 | Train Acc: 99.885% (7791/7800)\n",
      "Training: Epoch 7 - Batch 39/138: Loss: 0.0133 | Train Acc: 99.875% (7990/8000)\n",
      "Training: Epoch 7 - Batch 40/138: Loss: 0.0133 | Train Acc: 99.878% (8190/8200)\n",
      "Training: Epoch 7 - Batch 41/138: Loss: 0.0133 | Train Acc: 99.881% (8390/8400)\n",
      "Training: Epoch 7 - Batch 42/138: Loss: 0.0133 | Train Acc: 99.884% (8590/8600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 7 - Batch 43/138: Loss: 0.0132 | Train Acc: 99.886% (8790/8800)\n",
      "Training: Epoch 7 - Batch 44/138: Loss: 0.0132 | Train Acc: 99.889% (8990/9000)\n",
      "Training: Epoch 7 - Batch 45/138: Loss: 0.0131 | Train Acc: 99.891% (9190/9200)\n",
      "Training: Epoch 7 - Batch 46/138: Loss: 0.0131 | Train Acc: 99.894% (9390/9400)\n",
      "Training: Epoch 7 - Batch 47/138: Loss: 0.0131 | Train Acc: 99.885% (9589/9600)\n",
      "Training: Epoch 7 - Batch 48/138: Loss: 0.0131 | Train Acc: 99.888% (9789/9800)\n",
      "Training: Epoch 7 - Batch 49/138: Loss: 0.0132 | Train Acc: 99.880% (9988/10000)\n",
      "Training: Epoch 7 - Batch 50/138: Loss: 0.0131 | Train Acc: 99.882% (10188/10200)\n",
      "Training: Epoch 7 - Batch 51/138: Loss: 0.0130 | Train Acc: 99.885% (10388/10400)\n",
      "Training: Epoch 7 - Batch 52/138: Loss: 0.0131 | Train Acc: 99.887% (10588/10600)\n",
      "Training: Epoch 7 - Batch 53/138: Loss: 0.0131 | Train Acc: 99.889% (10788/10800)\n",
      "Training: Epoch 7 - Batch 54/138: Loss: 0.0130 | Train Acc: 99.891% (10988/11000)\n",
      "Training: Epoch 7 - Batch 55/138: Loss: 0.0130 | Train Acc: 99.884% (11187/11200)\n",
      "Training: Epoch 7 - Batch 56/138: Loss: 0.0130 | Train Acc: 99.886% (11387/11400)\n",
      "Training: Epoch 7 - Batch 57/138: Loss: 0.0130 | Train Acc: 99.888% (11587/11600)\n",
      "Training: Epoch 7 - Batch 58/138: Loss: 0.0130 | Train Acc: 99.890% (11787/11800)\n",
      "Training: Epoch 7 - Batch 59/138: Loss: 0.0130 | Train Acc: 99.892% (11987/12000)\n",
      "Training: Epoch 7 - Batch 60/138: Loss: 0.0130 | Train Acc: 99.893% (12187/12200)\n",
      "Training: Epoch 7 - Batch 61/138: Loss: 0.0130 | Train Acc: 99.895% (12387/12400)\n",
      "Training: Epoch 7 - Batch 62/138: Loss: 0.0129 | Train Acc: 99.897% (12587/12600)\n",
      "Training: Epoch 7 - Batch 63/138: Loss: 0.0130 | Train Acc: 99.883% (12785/12800)\n",
      "Training: Epoch 7 - Batch 64/138: Loss: 0.0129 | Train Acc: 99.885% (12985/13000)\n",
      "Training: Epoch 7 - Batch 65/138: Loss: 0.0129 | Train Acc: 99.886% (13185/13200)\n",
      "Training: Epoch 7 - Batch 66/138: Loss: 0.0129 | Train Acc: 99.873% (13383/13400)\n",
      "Training: Epoch 7 - Batch 67/138: Loss: 0.0128 | Train Acc: 99.875% (13583/13600)\n",
      "Training: Epoch 7 - Batch 68/138: Loss: 0.0128 | Train Acc: 99.877% (13783/13800)\n",
      "Training: Epoch 7 - Batch 69/138: Loss: 0.0128 | Train Acc: 99.879% (13983/14000)\n",
      "Training: Epoch 7 - Batch 70/138: Loss: 0.0127 | Train Acc: 99.880% (14183/14200)\n",
      "Training: Epoch 7 - Batch 71/138: Loss: 0.0126 | Train Acc: 99.882% (14383/14400)\n",
      "Training: Epoch 7 - Batch 72/138: Loss: 0.0126 | Train Acc: 99.884% (14583/14600)\n",
      "Training: Epoch 7 - Batch 73/138: Loss: 0.0127 | Train Acc: 99.878% (14782/14800)\n",
      "Training: Epoch 7 - Batch 74/138: Loss: 0.0127 | Train Acc: 99.873% (14981/15000)\n",
      "Training: Epoch 7 - Batch 75/138: Loss: 0.0128 | Train Acc: 99.868% (15180/15200)\n",
      "Training: Epoch 7 - Batch 76/138: Loss: 0.0127 | Train Acc: 99.870% (15380/15400)\n",
      "Training: Epoch 7 - Batch 77/138: Loss: 0.0127 | Train Acc: 99.872% (15580/15600)\n",
      "Training: Epoch 7 - Batch 78/138: Loss: 0.0127 | Train Acc: 99.873% (15780/15800)\n",
      "Training: Epoch 7 - Batch 79/138: Loss: 0.0126 | Train Acc: 99.875% (15980/16000)\n",
      "Training: Epoch 7 - Batch 80/138: Loss: 0.0126 | Train Acc: 99.877% (16180/16200)\n",
      "Training: Epoch 7 - Batch 81/138: Loss: 0.0126 | Train Acc: 99.878% (16380/16400)\n",
      "Training: Epoch 7 - Batch 82/138: Loss: 0.0125 | Train Acc: 99.880% (16580/16600)\n",
      "Training: Epoch 7 - Batch 83/138: Loss: 0.0125 | Train Acc: 99.881% (16780/16800)\n",
      "Training: Epoch 7 - Batch 84/138: Loss: 0.0125 | Train Acc: 99.882% (16980/17000)\n",
      "Training: Epoch 7 - Batch 85/138: Loss: 0.0125 | Train Acc: 99.884% (17180/17200)\n",
      "Training: Epoch 7 - Batch 86/138: Loss: 0.0125 | Train Acc: 99.885% (17380/17400)\n",
      "Training: Epoch 7 - Batch 87/138: Loss: 0.0125 | Train Acc: 99.886% (17580/17600)\n",
      "Training: Epoch 7 - Batch 88/138: Loss: 0.0124 | Train Acc: 99.888% (17780/17800)\n",
      "Training: Epoch 7 - Batch 89/138: Loss: 0.0125 | Train Acc: 99.889% (17980/18000)\n",
      "Training: Epoch 7 - Batch 90/138: Loss: 0.0124 | Train Acc: 99.890% (18180/18200)\n",
      "Training: Epoch 7 - Batch 91/138: Loss: 0.0124 | Train Acc: 99.891% (18380/18400)\n",
      "Training: Epoch 7 - Batch 92/138: Loss: 0.0124 | Train Acc: 99.892% (18580/18600)\n",
      "Training: Epoch 7 - Batch 93/138: Loss: 0.0125 | Train Acc: 99.888% (18779/18800)\n",
      "Training: Epoch 7 - Batch 94/138: Loss: 0.0125 | Train Acc: 99.889% (18979/19000)\n",
      "Training: Epoch 7 - Batch 95/138: Loss: 0.0125 | Train Acc: 99.891% (19179/19200)\n",
      "Training: Epoch 7 - Batch 96/138: Loss: 0.0124 | Train Acc: 99.892% (19379/19400)\n",
      "Training: Epoch 7 - Batch 97/138: Loss: 0.0124 | Train Acc: 99.893% (19579/19600)\n",
      "Training: Epoch 7 - Batch 98/138: Loss: 0.0124 | Train Acc: 99.884% (19777/19800)\n",
      "Training: Epoch 7 - Batch 99/138: Loss: 0.0124 | Train Acc: 99.885% (19977/20000)\n",
      "Training: Epoch 7 - Batch 100/138: Loss: 0.0124 | Train Acc: 99.886% (20177/20200)\n",
      "Training: Epoch 7 - Batch 101/138: Loss: 0.0124 | Train Acc: 99.887% (20377/20400)\n",
      "Training: Epoch 7 - Batch 102/138: Loss: 0.0125 | Train Acc: 99.879% (20575/20600)\n",
      "Training: Epoch 7 - Batch 103/138: Loss: 0.0125 | Train Acc: 99.880% (20775/20800)\n",
      "Training: Epoch 7 - Batch 104/138: Loss: 0.0125 | Train Acc: 99.881% (20975/21000)\n",
      "Training: Epoch 7 - Batch 105/138: Loss: 0.0125 | Train Acc: 99.882% (21175/21200)\n",
      "Training: Epoch 7 - Batch 106/138: Loss: 0.0125 | Train Acc: 99.883% (21375/21400)\n",
      "Training: Epoch 7 - Batch 107/138: Loss: 0.0125 | Train Acc: 99.884% (21575/21600)\n",
      "Training: Epoch 7 - Batch 108/138: Loss: 0.0125 | Train Acc: 99.885% (21775/21800)\n",
      "Training: Epoch 7 - Batch 109/138: Loss: 0.0125 | Train Acc: 99.886% (21975/22000)\n",
      "Training: Epoch 7 - Batch 110/138: Loss: 0.0125 | Train Acc: 99.887% (22175/22200)\n",
      "Training: Epoch 7 - Batch 111/138: Loss: 0.0125 | Train Acc: 99.888% (22375/22400)\n",
      "Training: Epoch 7 - Batch 112/138: Loss: 0.0125 | Train Acc: 99.889% (22575/22600)\n",
      "Training: Epoch 7 - Batch 113/138: Loss: 0.0125 | Train Acc: 99.890% (22775/22800)\n",
      "Training: Epoch 7 - Batch 114/138: Loss: 0.0124 | Train Acc: 99.891% (22975/23000)\n",
      "Training: Epoch 7 - Batch 115/138: Loss: 0.0124 | Train Acc: 99.888% (23174/23200)\n",
      "Training: Epoch 7 - Batch 116/138: Loss: 0.0124 | Train Acc: 99.889% (23374/23400)\n",
      "Training: Epoch 7 - Batch 117/138: Loss: 0.0124 | Train Acc: 99.890% (23574/23600)\n",
      "Training: Epoch 7 - Batch 118/138: Loss: 0.0123 | Train Acc: 99.891% (23774/23800)\n",
      "Training: Epoch 7 - Batch 119/138: Loss: 0.0123 | Train Acc: 99.892% (23974/24000)\n",
      "Training: Epoch 7 - Batch 120/138: Loss: 0.0124 | Train Acc: 99.884% (24172/24200)\n",
      "Training: Epoch 7 - Batch 121/138: Loss: 0.0124 | Train Acc: 99.885% (24372/24400)\n",
      "Training: Epoch 7 - Batch 122/138: Loss: 0.0124 | Train Acc: 99.886% (24572/24600)\n",
      "Training: Epoch 7 - Batch 123/138: Loss: 0.0123 | Train Acc: 99.887% (24772/24800)\n",
      "Training: Epoch 7 - Batch 124/138: Loss: 0.0123 | Train Acc: 99.888% (24972/25000)\n",
      "Training: Epoch 7 - Batch 125/138: Loss: 0.0123 | Train Acc: 99.889% (25172/25200)\n",
      "Training: Epoch 7 - Batch 126/138: Loss: 0.0123 | Train Acc: 99.890% (25372/25400)\n",
      "Training: Epoch 7 - Batch 127/138: Loss: 0.0122 | Train Acc: 99.891% (25572/25600)\n",
      "Training: Epoch 7 - Batch 128/138: Loss: 0.0122 | Train Acc: 99.891% (25772/25800)\n",
      "Training: Epoch 7 - Batch 129/138: Loss: 0.0122 | Train Acc: 99.892% (25972/26000)\n",
      "Training: Epoch 7 - Batch 130/138: Loss: 0.0122 | Train Acc: 99.889% (26171/26200)\n",
      "Training: Epoch 7 - Batch 131/138: Loss: 0.0122 | Train Acc: 99.890% (26371/26400)\n",
      "Training: Epoch 7 - Batch 132/138: Loss: 0.0121 | Train Acc: 99.891% (26571/26600)\n",
      "Training: Epoch 7 - Batch 133/138: Loss: 0.0121 | Train Acc: 99.892% (26771/26800)\n",
      "\n",
      "Dev loss: 0.33401 \n",
      "\n",
      "Training: Epoch 7 - Batch 134/138: Loss: 0.0121 | Train Acc: 99.893% (26971/27000)\n",
      "Training: Epoch 7 - Batch 135/138: Loss: 0.0121 | Train Acc: 99.893% (27171/27200)\n",
      "Training: Epoch 7 - Batch 136/138: Loss: 0.0121 | Train Acc: 99.894% (27371/27400)\n",
      "Training: Epoch 7 - Batch 137/138: Loss: 0.0121 | Train Acc: 99.894% (27426/27455)\n",
      "Training: Epoch 8 - Batch 0/138: Loss: 0.0062 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 8 - Batch 1/138: Loss: 0.0080 | Train Acc: 100.000% (400/400)\n",
      "Training: Epoch 8 - Batch 2/138: Loss: 0.0079 | Train Acc: 100.000% (600/600)\n",
      "Training: Epoch 8 - Batch 3/138: Loss: 0.0081 | Train Acc: 100.000% (800/800)\n",
      "Training: Epoch 8 - Batch 4/138: Loss: 0.0083 | Train Acc: 100.000% (1000/1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 8 - Batch 5/138: Loss: 0.0095 | Train Acc: 100.000% (1200/1200)\n",
      "Training: Epoch 8 - Batch 6/138: Loss: 0.0097 | Train Acc: 100.000% (1400/1400)\n",
      "Training: Epoch 8 - Batch 7/138: Loss: 0.0097 | Train Acc: 100.000% (1600/1600)\n",
      "Training: Epoch 8 - Batch 8/138: Loss: 0.0103 | Train Acc: 100.000% (1800/1800)\n",
      "Training: Epoch 8 - Batch 9/138: Loss: 0.0104 | Train Acc: 99.950% (1999/2000)\n",
      "Training: Epoch 8 - Batch 10/138: Loss: 0.0107 | Train Acc: 99.955% (2199/2200)\n",
      "Training: Epoch 8 - Batch 11/138: Loss: 0.0103 | Train Acc: 99.958% (2399/2400)\n",
      "Training: Epoch 8 - Batch 12/138: Loss: 0.0103 | Train Acc: 99.962% (2599/2600)\n",
      "Training: Epoch 8 - Batch 13/138: Loss: 0.0100 | Train Acc: 99.964% (2799/2800)\n",
      "Training: Epoch 8 - Batch 14/138: Loss: 0.0101 | Train Acc: 99.967% (2999/3000)\n",
      "Training: Epoch 8 - Batch 15/138: Loss: 0.0103 | Train Acc: 99.969% (3199/3200)\n",
      "Training: Epoch 8 - Batch 16/138: Loss: 0.0100 | Train Acc: 99.971% (3399/3400)\n",
      "Training: Epoch 8 - Batch 17/138: Loss: 0.0098 | Train Acc: 99.972% (3599/3600)\n",
      "Training: Epoch 8 - Batch 18/138: Loss: 0.0098 | Train Acc: 99.974% (3799/3800)\n",
      "Training: Epoch 8 - Batch 19/138: Loss: 0.0098 | Train Acc: 99.975% (3999/4000)\n",
      "Training: Epoch 8 - Batch 20/138: Loss: 0.0102 | Train Acc: 99.952% (4198/4200)\n",
      "Training: Epoch 8 - Batch 21/138: Loss: 0.0101 | Train Acc: 99.955% (4398/4400)\n",
      "Training: Epoch 8 - Batch 22/138: Loss: 0.0100 | Train Acc: 99.957% (4598/4600)\n",
      "Training: Epoch 8 - Batch 23/138: Loss: 0.0103 | Train Acc: 99.958% (4798/4800)\n",
      "Training: Epoch 8 - Batch 24/138: Loss: 0.0102 | Train Acc: 99.960% (4998/5000)\n",
      "Training: Epoch 8 - Batch 25/138: Loss: 0.0101 | Train Acc: 99.962% (5198/5200)\n",
      "Training: Epoch 8 - Batch 26/138: Loss: 0.0102 | Train Acc: 99.963% (5398/5400)\n",
      "Training: Epoch 8 - Batch 27/138: Loss: 0.0101 | Train Acc: 99.964% (5598/5600)\n",
      "Training: Epoch 8 - Batch 28/138: Loss: 0.0101 | Train Acc: 99.966% (5798/5800)\n",
      "Training: Epoch 8 - Batch 29/138: Loss: 0.0100 | Train Acc: 99.967% (5998/6000)\n",
      "Training: Epoch 8 - Batch 30/138: Loss: 0.0103 | Train Acc: 99.952% (6197/6200)\n",
      "Training: Epoch 8 - Batch 31/138: Loss: 0.0102 | Train Acc: 99.953% (6397/6400)\n",
      "Training: Epoch 8 - Batch 32/138: Loss: 0.0101 | Train Acc: 99.955% (6597/6600)\n",
      "Training: Epoch 8 - Batch 33/138: Loss: 0.0102 | Train Acc: 99.956% (6797/6800)\n",
      "Training: Epoch 8 - Batch 34/138: Loss: 0.0103 | Train Acc: 99.957% (6997/7000)\n",
      "Training: Epoch 8 - Batch 35/138: Loss: 0.0102 | Train Acc: 99.958% (7197/7200)\n",
      "Training: Epoch 8 - Batch 36/138: Loss: 0.0102 | Train Acc: 99.959% (7397/7400)\n",
      "Training: Epoch 8 - Batch 37/138: Loss: 0.0103 | Train Acc: 99.961% (7597/7600)\n",
      "Training: Epoch 8 - Batch 38/138: Loss: 0.0102 | Train Acc: 99.962% (7797/7800)\n",
      "Training: Epoch 8 - Batch 39/138: Loss: 0.0102 | Train Acc: 99.963% (7997/8000)\n",
      "Training: Epoch 8 - Batch 40/138: Loss: 0.0102 | Train Acc: 99.963% (8197/8200)\n",
      "Training: Epoch 8 - Batch 41/138: Loss: 0.0103 | Train Acc: 99.964% (8397/8400)\n",
      "Training: Epoch 8 - Batch 42/138: Loss: 0.0102 | Train Acc: 99.965% (8597/8600)\n",
      "Training: Epoch 8 - Batch 43/138: Loss: 0.0102 | Train Acc: 99.966% (8797/8800)\n",
      "Training: Epoch 8 - Batch 44/138: Loss: 0.0101 | Train Acc: 99.967% (8997/9000)\n",
      "Training: Epoch 8 - Batch 45/138: Loss: 0.0101 | Train Acc: 99.967% (9197/9200)\n",
      "Training: Epoch 8 - Batch 46/138: Loss: 0.0100 | Train Acc: 99.968% (9397/9400)\n",
      "Training: Epoch 8 - Batch 47/138: Loss: 0.0100 | Train Acc: 99.969% (9597/9600)\n",
      "Training: Epoch 8 - Batch 48/138: Loss: 0.0099 | Train Acc: 99.969% (9797/9800)\n",
      "Training: Epoch 8 - Batch 49/138: Loss: 0.0100 | Train Acc: 99.970% (9997/10000)\n",
      "Training: Epoch 8 - Batch 50/138: Loss: 0.0100 | Train Acc: 99.971% (10197/10200)\n",
      "Training: Epoch 8 - Batch 51/138: Loss: 0.0100 | Train Acc: 99.971% (10397/10400)\n",
      "Training: Epoch 8 - Batch 52/138: Loss: 0.0101 | Train Acc: 99.972% (10597/10600)\n",
      "Training: Epoch 8 - Batch 53/138: Loss: 0.0102 | Train Acc: 99.972% (10797/10800)\n",
      "Training: Epoch 8 - Batch 54/138: Loss: 0.0102 | Train Acc: 99.973% (10997/11000)\n",
      "Training: Epoch 8 - Batch 55/138: Loss: 0.0102 | Train Acc: 99.973% (11197/11200)\n",
      "Training: Epoch 8 - Batch 56/138: Loss: 0.0102 | Train Acc: 99.974% (11397/11400)\n",
      "Training: Epoch 8 - Batch 57/138: Loss: 0.0101 | Train Acc: 99.974% (11597/11600)\n",
      "Training: Epoch 8 - Batch 58/138: Loss: 0.0102 | Train Acc: 99.966% (11796/11800)\n",
      "Training: Epoch 8 - Batch 59/138: Loss: 0.0101 | Train Acc: 99.967% (11996/12000)\n",
      "Training: Epoch 8 - Batch 60/138: Loss: 0.0101 | Train Acc: 99.967% (12196/12200)\n",
      "Training: Epoch 8 - Batch 61/138: Loss: 0.0101 | Train Acc: 99.960% (12395/12400)\n",
      "Training: Epoch 8 - Batch 62/138: Loss: 0.0101 | Train Acc: 99.960% (12595/12600)\n",
      "Training: Epoch 8 - Batch 63/138: Loss: 0.0100 | Train Acc: 99.961% (12795/12800)\n",
      "Training: Epoch 8 - Batch 64/138: Loss: 0.0100 | Train Acc: 99.962% (12995/13000)\n",
      "Training: Epoch 8 - Batch 65/138: Loss: 0.0100 | Train Acc: 99.962% (13195/13200)\n",
      "Training: Epoch 8 - Batch 66/138: Loss: 0.0100 | Train Acc: 99.963% (13395/13400)\n",
      "Training: Epoch 8 - Batch 67/138: Loss: 0.0100 | Train Acc: 99.963% (13595/13600)\n",
      "Training: Epoch 8 - Batch 68/138: Loss: 0.0099 | Train Acc: 99.964% (13795/13800)\n",
      "Training: Epoch 8 - Batch 69/138: Loss: 0.0099 | Train Acc: 99.964% (13995/14000)\n",
      "Training: Epoch 8 - Batch 70/138: Loss: 0.0100 | Train Acc: 99.965% (14195/14200)\n",
      "Training: Epoch 8 - Batch 71/138: Loss: 0.0099 | Train Acc: 99.965% (14395/14400)\n",
      "Training: Epoch 8 - Batch 72/138: Loss: 0.0099 | Train Acc: 99.966% (14595/14600)\n",
      "Training: Epoch 8 - Batch 73/138: Loss: 0.0099 | Train Acc: 99.966% (14795/14800)\n",
      "Training: Epoch 8 - Batch 74/138: Loss: 0.0099 | Train Acc: 99.967% (14995/15000)\n",
      "Training: Epoch 8 - Batch 75/138: Loss: 0.0099 | Train Acc: 99.967% (15195/15200)\n",
      "Training: Epoch 8 - Batch 76/138: Loss: 0.0099 | Train Acc: 99.961% (15394/15400)\n",
      "Training: Epoch 8 - Batch 77/138: Loss: 0.0099 | Train Acc: 99.962% (15594/15600)\n",
      "Training: Epoch 8 - Batch 78/138: Loss: 0.0098 | Train Acc: 99.962% (15794/15800)\n",
      "Training: Epoch 8 - Batch 79/138: Loss: 0.0099 | Train Acc: 99.956% (15993/16000)\n",
      "Training: Epoch 8 - Batch 80/138: Loss: 0.0099 | Train Acc: 99.957% (16193/16200)\n",
      "Training: Epoch 8 - Batch 81/138: Loss: 0.0098 | Train Acc: 99.957% (16393/16400)\n",
      "Training: Epoch 8 - Batch 82/138: Loss: 0.0098 | Train Acc: 99.958% (16593/16600)\n",
      "Training: Epoch 8 - Batch 83/138: Loss: 0.0098 | Train Acc: 99.958% (16793/16800)\n",
      "Training: Epoch 8 - Batch 84/138: Loss: 0.0098 | Train Acc: 99.959% (16993/17000)\n",
      "Training: Epoch 8 - Batch 85/138: Loss: 0.0098 | Train Acc: 99.959% (17193/17200)\n",
      "Training: Epoch 8 - Batch 86/138: Loss: 0.0098 | Train Acc: 99.960% (17393/17400)\n",
      "Training: Epoch 8 - Batch 87/138: Loss: 0.0098 | Train Acc: 99.960% (17593/17600)\n",
      "Training: Epoch 8 - Batch 88/138: Loss: 0.0098 | Train Acc: 99.961% (17793/17800)\n",
      "Training: Epoch 8 - Batch 89/138: Loss: 0.0098 | Train Acc: 99.961% (17993/18000)\n",
      "Training: Epoch 8 - Batch 90/138: Loss: 0.0098 | Train Acc: 99.962% (18193/18200)\n",
      "Training: Epoch 8 - Batch 91/138: Loss: 0.0097 | Train Acc: 99.962% (18393/18400)\n",
      "Training: Epoch 8 - Batch 92/138: Loss: 0.0098 | Train Acc: 99.962% (18593/18600)\n",
      "Training: Epoch 8 - Batch 93/138: Loss: 0.0098 | Train Acc: 99.963% (18793/18800)\n",
      "Training: Epoch 8 - Batch 94/138: Loss: 0.0098 | Train Acc: 99.958% (18992/19000)\n",
      "Training: Epoch 8 - Batch 95/138: Loss: 0.0097 | Train Acc: 99.958% (19192/19200)\n",
      "\n",
      "Dev loss: 0.34321 \n",
      "\n",
      "Training: Epoch 8 - Batch 96/138: Loss: 0.0098 | Train Acc: 99.959% (19392/19400)\n",
      "Training: Epoch 8 - Batch 97/138: Loss: 0.0098 | Train Acc: 99.959% (19592/19600)\n",
      "Training: Epoch 8 - Batch 98/138: Loss: 0.0097 | Train Acc: 99.960% (19792/19800)\n",
      "Training: Epoch 8 - Batch 99/138: Loss: 0.0098 | Train Acc: 99.955% (19991/20000)\n",
      "Training: Epoch 8 - Batch 100/138: Loss: 0.0098 | Train Acc: 99.955% (20191/20200)\n",
      "Training: Epoch 8 - Batch 101/138: Loss: 0.0098 | Train Acc: 99.956% (20391/20400)\n",
      "Training: Epoch 8 - Batch 102/138: Loss: 0.0097 | Train Acc: 99.956% (20591/20600)\n",
      "Training: Epoch 8 - Batch 103/138: Loss: 0.0097 | Train Acc: 99.957% (20791/20800)\n",
      "Training: Epoch 8 - Batch 104/138: Loss: 0.0097 | Train Acc: 99.957% (20991/21000)\n",
      "Training: Epoch 8 - Batch 105/138: Loss: 0.0097 | Train Acc: 99.958% (21191/21200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 8 - Batch 106/138: Loss: 0.0098 | Train Acc: 99.953% (21390/21400)\n",
      "Training: Epoch 8 - Batch 107/138: Loss: 0.0098 | Train Acc: 99.954% (21590/21600)\n",
      "Training: Epoch 8 - Batch 108/138: Loss: 0.0098 | Train Acc: 99.954% (21790/21800)\n",
      "Training: Epoch 8 - Batch 109/138: Loss: 0.0098 | Train Acc: 99.955% (21990/22000)\n",
      "Training: Epoch 8 - Batch 110/138: Loss: 0.0098 | Train Acc: 99.955% (22190/22200)\n",
      "Training: Epoch 8 - Batch 111/138: Loss: 0.0098 | Train Acc: 99.955% (22390/22400)\n",
      "Training: Epoch 8 - Batch 112/138: Loss: 0.0098 | Train Acc: 99.956% (22590/22600)\n",
      "Training: Epoch 8 - Batch 113/138: Loss: 0.0098 | Train Acc: 99.952% (22789/22800)\n",
      "Training: Epoch 8 - Batch 114/138: Loss: 0.0098 | Train Acc: 99.952% (22989/23000)\n",
      "Training: Epoch 8 - Batch 115/138: Loss: 0.0098 | Train Acc: 99.953% (23189/23200)\n",
      "Training: Epoch 8 - Batch 116/138: Loss: 0.0098 | Train Acc: 99.953% (23389/23400)\n",
      "Training: Epoch 8 - Batch 117/138: Loss: 0.0098 | Train Acc: 99.953% (23589/23600)\n",
      "Training: Epoch 8 - Batch 118/138: Loss: 0.0098 | Train Acc: 99.954% (23789/23800)\n",
      "Training: Epoch 8 - Batch 119/138: Loss: 0.0098 | Train Acc: 99.954% (23989/24000)\n",
      "Training: Epoch 8 - Batch 120/138: Loss: 0.0098 | Train Acc: 99.955% (24189/24200)\n",
      "Training: Epoch 8 - Batch 121/138: Loss: 0.0098 | Train Acc: 99.955% (24389/24400)\n",
      "Training: Epoch 8 - Batch 122/138: Loss: 0.0098 | Train Acc: 99.955% (24589/24600)\n",
      "Training: Epoch 8 - Batch 123/138: Loss: 0.0098 | Train Acc: 99.956% (24789/24800)\n",
      "Training: Epoch 8 - Batch 124/138: Loss: 0.0098 | Train Acc: 99.956% (24989/25000)\n",
      "Training: Epoch 8 - Batch 125/138: Loss: 0.0098 | Train Acc: 99.956% (25189/25200)\n",
      "Training: Epoch 8 - Batch 126/138: Loss: 0.0098 | Train Acc: 99.957% (25389/25400)\n",
      "Training: Epoch 8 - Batch 127/138: Loss: 0.0098 | Train Acc: 99.957% (25589/25600)\n",
      "Training: Epoch 8 - Batch 128/138: Loss: 0.0098 | Train Acc: 99.957% (25789/25800)\n",
      "Training: Epoch 8 - Batch 129/138: Loss: 0.0098 | Train Acc: 99.954% (25988/26000)\n",
      "Training: Epoch 8 - Batch 130/138: Loss: 0.0098 | Train Acc: 99.954% (26188/26200)\n",
      "Training: Epoch 8 - Batch 131/138: Loss: 0.0098 | Train Acc: 99.955% (26388/26400)\n",
      "Training: Epoch 8 - Batch 132/138: Loss: 0.0098 | Train Acc: 99.951% (26587/26600)\n",
      "Training: Epoch 8 - Batch 133/138: Loss: 0.0098 | Train Acc: 99.951% (26787/26800)\n",
      "Training: Epoch 8 - Batch 134/138: Loss: 0.0098 | Train Acc: 99.952% (26987/27000)\n",
      "Training: Epoch 8 - Batch 135/138: Loss: 0.0098 | Train Acc: 99.952% (27187/27200)\n",
      "Training: Epoch 8 - Batch 136/138: Loss: 0.0098 | Train Acc: 99.949% (27386/27400)\n",
      "Training: Epoch 8 - Batch 137/138: Loss: 0.0098 | Train Acc: 99.949% (27441/27455)\n",
      "Training: Epoch 9 - Batch 0/138: Loss: 0.0104 | Train Acc: 100.000% (200/200)\n",
      "Training: Epoch 9 - Batch 1/138: Loss: 0.0093 | Train Acc: 100.000% (400/400)\n",
      "Training: Epoch 9 - Batch 2/138: Loss: 0.0104 | Train Acc: 100.000% (600/600)\n",
      "Training: Epoch 9 - Batch 3/138: Loss: 0.0100 | Train Acc: 100.000% (800/800)\n",
      "Training: Epoch 9 - Batch 4/138: Loss: 0.0094 | Train Acc: 100.000% (1000/1000)\n",
      "Training: Epoch 9 - Batch 5/138: Loss: 0.0095 | Train Acc: 100.000% (1200/1200)\n",
      "Training: Epoch 9 - Batch 6/138: Loss: 0.0105 | Train Acc: 99.929% (1399/1400)\n",
      "Training: Epoch 9 - Batch 7/138: Loss: 0.0105 | Train Acc: 99.938% (1599/1600)\n",
      "Training: Epoch 9 - Batch 8/138: Loss: 0.0106 | Train Acc: 99.944% (1799/1800)\n",
      "Training: Epoch 9 - Batch 9/138: Loss: 0.0102 | Train Acc: 99.950% (1999/2000)\n",
      "Training: Epoch 9 - Batch 10/138: Loss: 0.0098 | Train Acc: 99.955% (2199/2200)\n",
      "Training: Epoch 9 - Batch 11/138: Loss: 0.0095 | Train Acc: 99.958% (2399/2400)\n",
      "Training: Epoch 9 - Batch 12/138: Loss: 0.0099 | Train Acc: 99.962% (2599/2600)\n",
      "Training: Epoch 9 - Batch 13/138: Loss: 0.0099 | Train Acc: 99.964% (2799/2800)\n",
      "Training: Epoch 9 - Batch 14/138: Loss: 0.0095 | Train Acc: 99.967% (2999/3000)\n",
      "Training: Epoch 9 - Batch 15/138: Loss: 0.0093 | Train Acc: 99.969% (3199/3200)\n",
      "Training: Epoch 9 - Batch 16/138: Loss: 0.0091 | Train Acc: 99.971% (3399/3400)\n",
      "Training: Epoch 9 - Batch 17/138: Loss: 0.0089 | Train Acc: 99.972% (3599/3600)\n",
      "Training: Epoch 9 - Batch 18/138: Loss: 0.0088 | Train Acc: 99.974% (3799/3800)\n",
      "Training: Epoch 9 - Batch 19/138: Loss: 0.0090 | Train Acc: 99.975% (3999/4000)\n",
      "Training: Epoch 9 - Batch 20/138: Loss: 0.0091 | Train Acc: 99.952% (4198/4200)\n",
      "Training: Epoch 9 - Batch 21/138: Loss: 0.0090 | Train Acc: 99.955% (4398/4400)\n",
      "Training: Epoch 9 - Batch 22/138: Loss: 0.0090 | Train Acc: 99.957% (4598/4600)\n",
      "Training: Epoch 9 - Batch 23/138: Loss: 0.0089 | Train Acc: 99.958% (4798/4800)\n",
      "Training: Epoch 9 - Batch 24/138: Loss: 0.0091 | Train Acc: 99.940% (4997/5000)\n",
      "Training: Epoch 9 - Batch 25/138: Loss: 0.0090 | Train Acc: 99.942% (5197/5200)\n",
      "Training: Epoch 9 - Batch 26/138: Loss: 0.0088 | Train Acc: 99.944% (5397/5400)\n",
      "Training: Epoch 9 - Batch 27/138: Loss: 0.0089 | Train Acc: 99.946% (5597/5600)\n",
      "Training: Epoch 9 - Batch 28/138: Loss: 0.0088 | Train Acc: 99.948% (5797/5800)\n",
      "Training: Epoch 9 - Batch 29/138: Loss: 0.0087 | Train Acc: 99.950% (5997/6000)\n",
      "Training: Epoch 9 - Batch 30/138: Loss: 0.0087 | Train Acc: 99.952% (6197/6200)\n",
      "Training: Epoch 9 - Batch 31/138: Loss: 0.0087 | Train Acc: 99.953% (6397/6400)\n",
      "Training: Epoch 9 - Batch 32/138: Loss: 0.0088 | Train Acc: 99.955% (6597/6600)\n",
      "Training: Epoch 9 - Batch 33/138: Loss: 0.0088 | Train Acc: 99.956% (6797/6800)\n",
      "Training: Epoch 9 - Batch 34/138: Loss: 0.0087 | Train Acc: 99.957% (6997/7000)\n",
      "Training: Epoch 9 - Batch 35/138: Loss: 0.0088 | Train Acc: 99.944% (7196/7200)\n",
      "Training: Epoch 9 - Batch 36/138: Loss: 0.0089 | Train Acc: 99.946% (7396/7400)\n",
      "Training: Epoch 9 - Batch 37/138: Loss: 0.0089 | Train Acc: 99.947% (7596/7600)\n",
      "Training: Epoch 9 - Batch 38/138: Loss: 0.0089 | Train Acc: 99.949% (7796/7800)\n",
      "Training: Epoch 9 - Batch 39/138: Loss: 0.0089 | Train Acc: 99.950% (7996/8000)\n",
      "Training: Epoch 9 - Batch 40/138: Loss: 0.0089 | Train Acc: 99.951% (8196/8200)\n",
      "Training: Epoch 9 - Batch 41/138: Loss: 0.0088 | Train Acc: 99.952% (8396/8400)\n",
      "Training: Epoch 9 - Batch 42/138: Loss: 0.0090 | Train Acc: 99.942% (8595/8600)\n",
      "Training: Epoch 9 - Batch 43/138: Loss: 0.0090 | Train Acc: 99.943% (8795/8800)\n",
      "Training: Epoch 9 - Batch 44/138: Loss: 0.0090 | Train Acc: 99.933% (8994/9000)\n",
      "Training: Epoch 9 - Batch 45/138: Loss: 0.0090 | Train Acc: 99.935% (9194/9200)\n",
      "Training: Epoch 9 - Batch 46/138: Loss: 0.0089 | Train Acc: 99.936% (9394/9400)\n",
      "Training: Epoch 9 - Batch 47/138: Loss: 0.0090 | Train Acc: 99.938% (9594/9600)\n",
      "Training: Epoch 9 - Batch 48/138: Loss: 0.0090 | Train Acc: 99.939% (9794/9800)\n",
      "Training: Epoch 9 - Batch 49/138: Loss: 0.0089 | Train Acc: 99.940% (9994/10000)\n",
      "Training: Epoch 9 - Batch 50/138: Loss: 0.0089 | Train Acc: 99.941% (10194/10200)\n",
      "Training: Epoch 9 - Batch 51/138: Loss: 0.0088 | Train Acc: 99.942% (10394/10400)\n",
      "Training: Epoch 9 - Batch 52/138: Loss: 0.0088 | Train Acc: 99.943% (10594/10600)\n",
      "Training: Epoch 9 - Batch 53/138: Loss: 0.0089 | Train Acc: 99.944% (10794/10800)\n",
      "Training: Epoch 9 - Batch 54/138: Loss: 0.0088 | Train Acc: 99.945% (10994/11000)\n",
      "Training: Epoch 9 - Batch 55/138: Loss: 0.0089 | Train Acc: 99.946% (11194/11200)\n",
      "Training: Epoch 9 - Batch 56/138: Loss: 0.0090 | Train Acc: 99.939% (11393/11400)\n",
      "Training: Epoch 9 - Batch 57/138: Loss: 0.0090 | Train Acc: 99.940% (11593/11600)\n",
      "\n",
      "Dev loss: 0.31288 \n",
      "\n",
      "Training: Epoch 9 - Batch 58/138: Loss: 0.0089 | Train Acc: 99.941% (11793/11800)\n",
      "Training: Epoch 9 - Batch 59/138: Loss: 0.0089 | Train Acc: 99.942% (11993/12000)\n",
      "Training: Epoch 9 - Batch 60/138: Loss: 0.0088 | Train Acc: 99.943% (12193/12200)\n",
      "Training: Epoch 9 - Batch 61/138: Loss: 0.0088 | Train Acc: 99.944% (12393/12400)\n",
      "Training: Epoch 9 - Batch 62/138: Loss: 0.0088 | Train Acc: 99.944% (12593/12600)\n",
      "Training: Epoch 9 - Batch 63/138: Loss: 0.0088 | Train Acc: 99.938% (12792/12800)\n",
      "Training: Epoch 9 - Batch 64/138: Loss: 0.0088 | Train Acc: 99.938% (12992/13000)\n",
      "Training: Epoch 9 - Batch 65/138: Loss: 0.0089 | Train Acc: 99.939% (13192/13200)\n",
      "Training: Epoch 9 - Batch 66/138: Loss: 0.0089 | Train Acc: 99.940% (13392/13400)\n",
      "Training: Epoch 9 - Batch 67/138: Loss: 0.0088 | Train Acc: 99.941% (13592/13600)\n",
      "Training: Epoch 9 - Batch 68/138: Loss: 0.0088 | Train Acc: 99.942% (13792/13800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 9 - Batch 69/138: Loss: 0.0088 | Train Acc: 99.943% (13992/14000)\n",
      "Training: Epoch 9 - Batch 70/138: Loss: 0.0088 | Train Acc: 99.944% (14192/14200)\n",
      "Training: Epoch 9 - Batch 71/138: Loss: 0.0088 | Train Acc: 99.944% (14392/14400)\n",
      "Training: Epoch 9 - Batch 72/138: Loss: 0.0087 | Train Acc: 99.945% (14592/14600)\n",
      "Training: Epoch 9 - Batch 73/138: Loss: 0.0088 | Train Acc: 99.939% (14791/14800)\n",
      "Training: Epoch 9 - Batch 74/138: Loss: 0.0088 | Train Acc: 99.940% (14991/15000)\n",
      "Training: Epoch 9 - Batch 75/138: Loss: 0.0088 | Train Acc: 99.941% (15191/15200)\n",
      "Training: Epoch 9 - Batch 76/138: Loss: 0.0088 | Train Acc: 99.942% (15391/15400)\n",
      "Training: Epoch 9 - Batch 77/138: Loss: 0.0088 | Train Acc: 99.942% (15591/15600)\n",
      "Training: Epoch 9 - Batch 78/138: Loss: 0.0087 | Train Acc: 99.943% (15791/15800)\n",
      "Training: Epoch 9 - Batch 79/138: Loss: 0.0087 | Train Acc: 99.944% (15991/16000)\n",
      "Training: Epoch 9 - Batch 80/138: Loss: 0.0087 | Train Acc: 99.944% (16191/16200)\n",
      "Training: Epoch 9 - Batch 81/138: Loss: 0.0087 | Train Acc: 99.945% (16391/16400)\n",
      "Training: Epoch 9 - Batch 82/138: Loss: 0.0087 | Train Acc: 99.940% (16590/16600)\n",
      "Training: Epoch 9 - Batch 83/138: Loss: 0.0088 | Train Acc: 99.935% (16789/16800)\n",
      "Training: Epoch 9 - Batch 84/138: Loss: 0.0088 | Train Acc: 99.935% (16989/17000)\n",
      "Training: Epoch 9 - Batch 85/138: Loss: 0.0087 | Train Acc: 99.936% (17189/17200)\n",
      "Training: Epoch 9 - Batch 86/138: Loss: 0.0088 | Train Acc: 99.931% (17388/17400)\n",
      "Training: Epoch 9 - Batch 87/138: Loss: 0.0087 | Train Acc: 99.932% (17588/17600)\n",
      "Training: Epoch 9 - Batch 88/138: Loss: 0.0087 | Train Acc: 99.933% (17788/17800)\n",
      "Training: Epoch 9 - Batch 89/138: Loss: 0.0087 | Train Acc: 99.933% (17988/18000)\n",
      "Training: Epoch 9 - Batch 90/138: Loss: 0.0088 | Train Acc: 99.929% (18187/18200)\n",
      "Training: Epoch 9 - Batch 91/138: Loss: 0.0087 | Train Acc: 99.929% (18387/18400)\n",
      "Training: Epoch 9 - Batch 92/138: Loss: 0.0087 | Train Acc: 99.930% (18587/18600)\n",
      "Training: Epoch 9 - Batch 93/138: Loss: 0.0087 | Train Acc: 99.931% (18787/18800)\n",
      "Training: Epoch 9 - Batch 94/138: Loss: 0.0087 | Train Acc: 99.932% (18987/19000)\n",
      "Training: Epoch 9 - Batch 95/138: Loss: 0.0087 | Train Acc: 99.932% (19187/19200)\n",
      "Training: Epoch 9 - Batch 96/138: Loss: 0.0086 | Train Acc: 99.933% (19387/19400)\n",
      "Training: Epoch 9 - Batch 97/138: Loss: 0.0086 | Train Acc: 99.934% (19587/19600)\n",
      "Training: Epoch 9 - Batch 98/138: Loss: 0.0086 | Train Acc: 99.934% (19787/19800)\n",
      "Training: Epoch 9 - Batch 99/138: Loss: 0.0085 | Train Acc: 99.935% (19987/20000)\n",
      "Training: Epoch 9 - Batch 100/138: Loss: 0.0085 | Train Acc: 99.936% (20187/20200)\n",
      "Training: Epoch 9 - Batch 101/138: Loss: 0.0085 | Train Acc: 99.936% (20387/20400)\n",
      "Training: Epoch 9 - Batch 102/138: Loss: 0.0085 | Train Acc: 99.937% (20587/20600)\n",
      "Training: Epoch 9 - Batch 103/138: Loss: 0.0085 | Train Acc: 99.938% (20787/20800)\n",
      "Training: Epoch 9 - Batch 104/138: Loss: 0.0085 | Train Acc: 99.938% (20987/21000)\n",
      "Training: Epoch 9 - Batch 105/138: Loss: 0.0085 | Train Acc: 99.939% (21187/21200)\n",
      "Training: Epoch 9 - Batch 106/138: Loss: 0.0085 | Train Acc: 99.939% (21387/21400)\n",
      "Training: Epoch 9 - Batch 107/138: Loss: 0.0085 | Train Acc: 99.940% (21587/21600)\n",
      "Training: Epoch 9 - Batch 108/138: Loss: 0.0085 | Train Acc: 99.940% (21787/21800)\n",
      "Training: Epoch 9 - Batch 109/138: Loss: 0.0085 | Train Acc: 99.941% (21987/22000)\n",
      "Training: Epoch 9 - Batch 110/138: Loss: 0.0086 | Train Acc: 99.932% (22185/22200)\n",
      "Training: Epoch 9 - Batch 111/138: Loss: 0.0086 | Train Acc: 99.933% (22385/22400)\n",
      "Training: Epoch 9 - Batch 112/138: Loss: 0.0086 | Train Acc: 99.934% (22585/22600)\n",
      "Training: Epoch 9 - Batch 113/138: Loss: 0.0086 | Train Acc: 99.930% (22784/22800)\n",
      "Training: Epoch 9 - Batch 114/138: Loss: 0.0087 | Train Acc: 99.930% (22984/23000)\n",
      "Training: Epoch 9 - Batch 115/138: Loss: 0.0087 | Train Acc: 99.931% (23184/23200)\n",
      "Training: Epoch 9 - Batch 116/138: Loss: 0.0087 | Train Acc: 99.932% (23384/23400)\n",
      "Training: Epoch 9 - Batch 117/138: Loss: 0.0086 | Train Acc: 99.932% (23584/23600)\n",
      "Training: Epoch 9 - Batch 118/138: Loss: 0.0086 | Train Acc: 99.933% (23784/23800)\n",
      "Training: Epoch 9 - Batch 119/138: Loss: 0.0086 | Train Acc: 99.933% (23984/24000)\n",
      "Training: Epoch 9 - Batch 120/138: Loss: 0.0086 | Train Acc: 99.934% (24184/24200)\n",
      "Training: Epoch 9 - Batch 121/138: Loss: 0.0086 | Train Acc: 99.934% (24384/24400)\n",
      "Training: Epoch 9 - Batch 122/138: Loss: 0.0086 | Train Acc: 99.935% (24584/24600)\n",
      "Training: Epoch 9 - Batch 123/138: Loss: 0.0086 | Train Acc: 99.935% (24784/24800)\n",
      "Training: Epoch 9 - Batch 124/138: Loss: 0.0086 | Train Acc: 99.936% (24984/25000)\n",
      "Training: Epoch 9 - Batch 125/138: Loss: 0.0086 | Train Acc: 99.937% (25184/25200)\n",
      "Training: Epoch 9 - Batch 126/138: Loss: 0.0085 | Train Acc: 99.937% (25384/25400)\n",
      "Training: Epoch 9 - Batch 127/138: Loss: 0.0085 | Train Acc: 99.938% (25584/25600)\n",
      "Training: Epoch 9 - Batch 128/138: Loss: 0.0085 | Train Acc: 99.938% (25784/25800)\n",
      "Training: Epoch 9 - Batch 129/138: Loss: 0.0085 | Train Acc: 99.938% (25984/26000)\n",
      "Training: Epoch 9 - Batch 130/138: Loss: 0.0085 | Train Acc: 99.935% (26183/26200)\n",
      "Training: Epoch 9 - Batch 131/138: Loss: 0.0085 | Train Acc: 99.936% (26383/26400)\n",
      "Training: Epoch 9 - Batch 132/138: Loss: 0.0085 | Train Acc: 99.936% (26583/26600)\n",
      "Training: Epoch 9 - Batch 133/138: Loss: 0.0085 | Train Acc: 99.937% (26783/26800)\n",
      "Training: Epoch 9 - Batch 134/138: Loss: 0.0085 | Train Acc: 99.937% (26983/27000)\n",
      "Training: Epoch 9 - Batch 135/138: Loss: 0.0085 | Train Acc: 99.938% (27183/27200)\n",
      "Training: Epoch 9 - Batch 136/138: Loss: 0.0085 | Train Acc: 99.938% (27383/27400)\n",
      "Training: Epoch 9 - Batch 137/138: Loss: 0.0085 | Train Acc: 99.938% (27438/27455)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--- training ---\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # If done=True, stop training\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    # Initialize values\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # If done=True, stop training\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        # Get data and target\n",
    "        data, target = data.to(device), target.to(device)\n",
    "            \n",
    "        # Sum of sizes\n",
    "        total += target.size(0)\n",
    "        \n",
    "        # Get prediction values\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Get prediction labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Sum of number of correct predictions\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss=loss_function(outputs, target)\n",
    "        \n",
    "        # Backward + optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss+=loss\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "        \n",
    "# -- Check development set accuracy and stop training if it start to decline\n",
    "\n",
    "    # WRITE CODE HERE\n",
    "    # Please implement early stopping here.\n",
    "    # You can try different versions, simplest way is to calculate the dev error and\n",
    "    # compare this with the previous dev error, stopping if the error has grown.\n",
    "\n",
    "    \n",
    "    # In the early stopping we compare latest dev loss to previous last 3 dev losses\n",
    "    # To make training more efficiont, we will compute dev loss once in every 100 iterations\n",
    "    # If latest dev loss is larger than all previous 3, then end training\n",
    "        if (counter%100 == 0):\n",
    "            train_loss_list.append(float(train_loss / (batch_num + 1)))\n",
    "            for dev_batch_num, (dev_data, dev_target) in enumerate(dev_loader):\n",
    "                dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                dev_outputs = model(dev_data)\n",
    "                _, dev_predicted = torch.max(dev_outputs, 1)   \n",
    "                dev_loss=loss_function(dev_outputs, dev_target)\n",
    "                dev_loss = float(dev_loss)      \n",
    "                print(\"\\nDev loss: {} \\n\".format(round(dev_loss, 5)))\n",
    "                \n",
    "                # Save dev loss to list\n",
    "                dev_loss_list.append(dev_loss)\n",
    "        \n",
    "                # If the latest dev correct accuracy is smaller than the last 3 values, then end training\n",
    "                dev_last_three = dev_loss_list[-4:-1]\n",
    "                if dev_loss > max(dev_last_three):\n",
    "                    print(\"Dev accuracy declining. Activate early stopping. \\n\")\n",
    "                    done = True\n",
    "                    break\n",
    "    \n",
    "        # Add 1 to counter\n",
    "        counter = counter+1\n",
    "\n",
    "# Remove initialized first three values from dev_correct_list\n",
    "dev_loss_list = dev_loss_list[3:]\n",
    "\n",
    "# Plot dev correct\n",
    "plt.plot(dev_loss_list, label = \"Dev loss\")\n",
    "plt.plot(train_loss_list, label = \"Train loss\")\n",
    "plt.ylim((0, max(dev_loss_list+train_loss_list)+0.01))\n",
    "plt.title(\"Train and dev loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 0/19: Loss: 0.0434 | Test Acc: 98.500% (197/200)\n",
      "Evaluating: Batch 1/19: Loss: 0.0621 | Test Acc: 98.250% (393/400)\n",
      "Evaluating: Batch 2/19: Loss: 0.1231 | Test Acc: 96.667% (580/600)\n",
      "Evaluating: Batch 3/19: Loss: 0.1191 | Test Acc: 96.750% (774/800)\n",
      "Evaluating: Batch 4/19: Loss: 0.1110 | Test Acc: 96.900% (969/1000)\n",
      "Evaluating: Batch 5/19: Loss: 0.1208 | Test Acc: 96.167% (1154/1200)\n",
      "Evaluating: Batch 6/19: Loss: 0.1483 | Test Acc: 95.357% (1335/1400)\n",
      "Evaluating: Batch 7/19: Loss: 0.1656 | Test Acc: 94.688% (1515/1600)\n",
      "Evaluating: Batch 8/19: Loss: 0.1957 | Test Acc: 94.111% (1694/1800)\n",
      "Evaluating: Batch 9/19: Loss: 0.1957 | Test Acc: 94.100% (1882/2000)\n",
      "Evaluating: Batch 10/19: Loss: 0.2368 | Test Acc: 92.682% (2039/2200)\n",
      "Evaluating: Batch 11/19: Loss: 0.2634 | Test Acc: 91.792% (2203/2400)\n",
      "Evaluating: Batch 12/19: Loss: 0.2491 | Test Acc: 92.308% (2400/2600)\n",
      "Evaluating: Batch 13/19: Loss: 0.2588 | Test Acc: 92.000% (2576/2800)\n",
      "Evaluating: Batch 14/19: Loss: 0.3110 | Test Acc: 90.867% (2726/3000)\n",
      "Evaluating: Batch 15/19: Loss: 0.3344 | Test Acc: 90.375% (2892/3200)\n",
      "Evaluating: Batch 16/19: Loss: 0.3429 | Test Acc: 89.971% (3059/3400)\n",
      "Evaluating: Batch 17/19: Loss: 0.3541 | Test Acc: 89.889% (3236/3600)\n",
      "Evaluating: Batch 18/19: Loss: 0.3641 | Test Acc: 89.897% (3301/3672)\n",
      "\n",
      "Average test accuracy 93.75 %\n"
     ]
    }
   ],
   "source": [
    "#--- test ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "total = 0\n",
    "test_acc_list=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        outputs=model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        test_correct += (predicted == target).sum()\n",
    "        loss=loss_function(outputs, target)\n",
    "        test_loss+=loss\n",
    "        \n",
    "        test_acc_list.append(100 * float(test_correct) / float(total))\n",
    "        \n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(test_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / total, test_correct, total))\n",
    "\n",
    "print(\"\\nAverage test accuracy\", round(statistics.mean(test_acc_list), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

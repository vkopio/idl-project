{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 1\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 24\n",
    "DATA_DIR = '../data/sign_mnist_%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset initialization ---\n",
    "\n",
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "\n",
    "# Grayscale + toTensor + Normalize\n",
    "train_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "train_set = datasets.ImageFolder(DATA_DIR % 'train', transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DATA_DIR % 'dev',   transform=test_transform)\n",
    "test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        # Sequential 1: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=100, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(num_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # Sequential 2: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=100, out_channels=80, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(num_features=80),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4, padding=0))\n",
    "        \n",
    "        # Sequential 3: Linear + ReLU + Linear\n",
    "        self.seq3 = nn.Sequential(\n",
    "            nn.Linear(in_features=4*80, out_features=250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=250, out_features=NUM_CLASSES))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # WRITE CODE HERE\n",
    "  \n",
    "        # Sequential 1\n",
    "        x = self.seq1(x)\n",
    "        \n",
    "        # Sequential 2\n",
    "        x = self.seq2(x)\n",
    "        \n",
    "        # Reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Sequential 3\n",
    "        x = self.seq3(x)\n",
    "        \n",
    "        # log_softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # Return x\n",
    "        return x\n",
    "\n",
    "# Print model summary\n",
    "#print(CNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: False \n",
      "\n",
      "Training: Epoch 0 - Batch 0/275: Loss: 3.2102 | Train Acc: 6.000% (6/100)\n",
      "Training: Epoch 0 - Batch 1/275: Loss: 3.4393 | Train Acc: 6.000% (12/200)\n",
      "Training: Epoch 0 - Batch 2/275: Loss: 3.3835 | Train Acc: 6.000% (18/300)\n",
      "Training: Epoch 0 - Batch 3/275: Loss: 3.3244 | Train Acc: 6.750% (27/400)\n",
      "Training: Epoch 0 - Batch 4/275: Loss: 3.2747 | Train Acc: 7.400% (37/500)\n",
      "Training: Epoch 0 - Batch 5/275: Loss: 3.2451 | Train Acc: 7.667% (46/600)\n",
      "Training: Epoch 0 - Batch 6/275: Loss: 3.1987 | Train Acc: 8.571% (60/700)\n",
      "Training: Epoch 0 - Batch 7/275: Loss: 3.1650 | Train Acc: 10.000% (80/800)\n",
      "Training: Epoch 0 - Batch 8/275: Loss: 3.1282 | Train Acc: 11.111% (100/900)\n",
      "Training: Epoch 0 - Batch 9/275: Loss: 3.0773 | Train Acc: 12.900% (129/1000)\n",
      "Training: Epoch 0 - Batch 10/275: Loss: 3.0397 | Train Acc: 13.909% (153/1100)\n",
      "Training: Epoch 0 - Batch 11/275: Loss: 2.9978 | Train Acc: 15.083% (181/1200)\n",
      "Training: Epoch 0 - Batch 12/275: Loss: 2.9588 | Train Acc: 15.923% (207/1300)\n",
      "Training: Epoch 0 - Batch 13/275: Loss: 2.9186 | Train Acc: 16.643% (233/1400)\n",
      "Training: Epoch 0 - Batch 14/275: Loss: 2.8792 | Train Acc: 17.600% (264/1500)\n",
      "Training: Epoch 0 - Batch 15/275: Loss: 2.8331 | Train Acc: 19.000% (304/1600)\n",
      "Training: Epoch 0 - Batch 16/275: Loss: 2.7835 | Train Acc: 20.059% (341/1700)\n",
      "Training: Epoch 0 - Batch 17/275: Loss: 2.7442 | Train Acc: 21.000% (378/1800)\n",
      "Training: Epoch 0 - Batch 18/275: Loss: 2.6967 | Train Acc: 22.316% (424/1900)\n",
      "Training: Epoch 0 - Batch 19/275: Loss: 2.6504 | Train Acc: 23.600% (472/2000)\n",
      "Training: Epoch 0 - Batch 20/275: Loss: 2.6075 | Train Acc: 24.857% (522/2100)\n",
      "Training: Epoch 0 - Batch 21/275: Loss: 2.5616 | Train Acc: 25.955% (571/2200)\n",
      "Training: Epoch 0 - Batch 22/275: Loss: 2.5224 | Train Acc: 26.696% (614/2300)\n",
      "Training: Epoch 0 - Batch 23/275: Loss: 2.4817 | Train Acc: 27.875% (669/2400)\n",
      "Training: Epoch 0 - Batch 24/275: Loss: 2.4396 | Train Acc: 28.920% (723/2500)\n",
      "Training: Epoch 0 - Batch 25/275: Loss: 2.4018 | Train Acc: 29.962% (779/2600)\n",
      "Training: Epoch 0 - Batch 26/275: Loss: 2.3644 | Train Acc: 31.074% (839/2700)\n",
      "Training: Epoch 0 - Batch 27/275: Loss: 2.3254 | Train Acc: 32.071% (898/2800)\n",
      "Training: Epoch 0 - Batch 28/275: Loss: 2.2897 | Train Acc: 32.759% (950/2900)\n",
      "Training: Epoch 0 - Batch 29/275: Loss: 2.2580 | Train Acc: 33.267% (998/3000)\n",
      "Training: Epoch 0 - Batch 30/275: Loss: 2.2160 | Train Acc: 34.452% (1068/3100)\n",
      "Training: Epoch 0 - Batch 31/275: Loss: 2.1799 | Train Acc: 35.562% (1138/3200)\n",
      "Training: Epoch 0 - Batch 32/275: Loss: 2.1463 | Train Acc: 36.364% (1200/3300)\n",
      "Training: Epoch 0 - Batch 33/275: Loss: 2.1106 | Train Acc: 37.294% (1268/3400)\n",
      "Training: Epoch 0 - Batch 34/275: Loss: 2.0758 | Train Acc: 38.257% (1339/3500)\n",
      "Training: Epoch 0 - Batch 35/275: Loss: 2.0426 | Train Acc: 39.250% (1413/3600)\n",
      "Training: Epoch 0 - Batch 36/275: Loss: 2.0119 | Train Acc: 40.108% (1484/3700)\n",
      "Training: Epoch 0 - Batch 37/275: Loss: 1.9813 | Train Acc: 40.974% (1557/3800)\n",
      "Training: Epoch 0 - Batch 38/275: Loss: 1.9553 | Train Acc: 41.744% (1628/3900)\n",
      "Training: Epoch 0 - Batch 39/275: Loss: 1.9282 | Train Acc: 42.525% (1701/4000)\n",
      "Training: Epoch 0 - Batch 40/275: Loss: 1.8993 | Train Acc: 43.317% (1776/4100)\n",
      "Training: Epoch 0 - Batch 41/275: Loss: 1.8697 | Train Acc: 44.214% (1857/4200)\n",
      "Training: Epoch 0 - Batch 42/275: Loss: 1.8437 | Train Acc: 44.767% (1925/4300)\n",
      "Training: Epoch 0 - Batch 43/275: Loss: 1.8203 | Train Acc: 45.500% (2002/4400)\n",
      "Training: Epoch 0 - Batch 44/275: Loss: 1.7962 | Train Acc: 46.222% (2080/4500)\n",
      "Training: Epoch 0 - Batch 45/275: Loss: 1.7707 | Train Acc: 46.891% (2157/4600)\n",
      "Training: Epoch 0 - Batch 46/275: Loss: 1.7431 | Train Acc: 47.638% (2239/4700)\n",
      "Training: Epoch 0 - Batch 47/275: Loss: 1.7162 | Train Acc: 48.458% (2326/4800)\n",
      "Training: Epoch 0 - Batch 48/275: Loss: 1.6956 | Train Acc: 49.082% (2405/4900)\n",
      "Training: Epoch 0 - Batch 49/275: Loss: 1.6741 | Train Acc: 49.700% (2485/5000)\n",
      "Training: Epoch 0 - Batch 50/275: Loss: 1.6494 | Train Acc: 50.490% (2575/5100)\n",
      "Training: Epoch 0 - Batch 51/275: Loss: 1.6281 | Train Acc: 51.115% (2658/5200)\n",
      "Training: Epoch 0 - Batch 52/275: Loss: 1.6059 | Train Acc: 51.792% (2745/5300)\n",
      "Training: Epoch 0 - Batch 53/275: Loss: 1.5848 | Train Acc: 52.444% (2832/5400)\n",
      "Training: Epoch 0 - Batch 54/275: Loss: 1.5637 | Train Acc: 53.055% (2918/5500)\n",
      "Training: Epoch 0 - Batch 55/275: Loss: 1.5425 | Train Acc: 53.750% (3010/5600)\n",
      "Training: Epoch 0 - Batch 56/275: Loss: 1.5232 | Train Acc: 54.333% (3097/5700)\n",
      "Training: Epoch 0 - Batch 57/275: Loss: 1.5034 | Train Acc: 54.914% (3185/5800)\n",
      "Training: Epoch 0 - Batch 58/275: Loss: 1.4886 | Train Acc: 55.390% (3268/5900)\n",
      "Training: Epoch 0 - Batch 59/275: Loss: 1.4710 | Train Acc: 55.950% (3357/6000)\n",
      "Training: Epoch 0 - Batch 60/275: Loss: 1.4540 | Train Acc: 56.443% (3443/6100)\n",
      "Training: Epoch 0 - Batch 61/275: Loss: 1.4349 | Train Acc: 57.016% (3535/6200)\n",
      "Training: Epoch 0 - Batch 62/275: Loss: 1.4159 | Train Acc: 57.619% (3630/6300)\n",
      "Training: Epoch 0 - Batch 63/275: Loss: 1.3994 | Train Acc: 58.078% (3717/6400)\n",
      "Training: Epoch 0 - Batch 64/275: Loss: 1.3833 | Train Acc: 58.538% (3805/6500)\n",
      "Training: Epoch 0 - Batch 65/275: Loss: 1.3657 | Train Acc: 59.061% (3898/6600)\n",
      "Training: Epoch 0 - Batch 66/275: Loss: 1.3485 | Train Acc: 59.582% (3992/6700)\n",
      "Training: Epoch 0 - Batch 67/275: Loss: 1.3324 | Train Acc: 60.074% (4085/6800)\n",
      "Training: Epoch 0 - Batch 68/275: Loss: 1.3165 | Train Acc: 60.565% (4179/6900)\n",
      "Training: Epoch 0 - Batch 69/275: Loss: 1.3005 | Train Acc: 61.057% (4274/7000)\n",
      "Training: Epoch 0 - Batch 70/275: Loss: 1.2853 | Train Acc: 61.549% (4370/7100)\n",
      "Training: Epoch 0 - Batch 71/275: Loss: 1.2699 | Train Acc: 62.042% (4467/7200)\n",
      "Training: Epoch 0 - Batch 72/275: Loss: 1.2558 | Train Acc: 62.479% (4561/7300)\n",
      "Training: Epoch 0 - Batch 73/275: Loss: 1.2422 | Train Acc: 62.892% (4654/7400)\n",
      "Training: Epoch 0 - Batch 74/275: Loss: 1.2288 | Train Acc: 63.267% (4745/7500)\n",
      "Training: Epoch 0 - Batch 75/275: Loss: 1.2150 | Train Acc: 63.684% (4840/7600)\n",
      "Training: Epoch 0 - Batch 76/275: Loss: 1.2016 | Train Acc: 64.065% (4933/7700)\n",
      "Training: Epoch 0 - Batch 77/275: Loss: 1.1889 | Train Acc: 64.397% (5023/7800)\n",
      "Training: Epoch 0 - Batch 78/275: Loss: 1.1761 | Train Acc: 64.785% (5118/7900)\n",
      "Training: Epoch 0 - Batch 79/275: Loss: 1.1635 | Train Acc: 65.175% (5214/8000)\n",
      "Training: Epoch 0 - Batch 80/275: Loss: 1.1509 | Train Acc: 65.568% (5311/8100)\n",
      "Training: Epoch 0 - Batch 81/275: Loss: 1.1392 | Train Acc: 65.939% (5407/8200)\n",
      "Training: Epoch 0 - Batch 82/275: Loss: 1.1275 | Train Acc: 66.277% (5501/8300)\n",
      "Training: Epoch 0 - Batch 83/275: Loss: 1.1162 | Train Acc: 66.607% (5595/8400)\n",
      "Training: Epoch 0 - Batch 84/275: Loss: 1.1047 | Train Acc: 66.941% (5690/8500)\n",
      "Training: Epoch 0 - Batch 85/275: Loss: 1.0930 | Train Acc: 67.314% (5789/8600)\n",
      "Training: Epoch 0 - Batch 86/275: Loss: 1.0820 | Train Acc: 67.655% (5886/8700)\n",
      "Training: Epoch 0 - Batch 87/275: Loss: 1.0715 | Train Acc: 67.977% (5982/8800)\n",
      "Training: Epoch 0 - Batch 88/275: Loss: 1.0607 | Train Acc: 68.315% (6080/8900)\n",
      "Training: Epoch 0 - Batch 89/275: Loss: 1.0509 | Train Acc: 68.611% (6175/9000)\n",
      "Training: Epoch 0 - Batch 90/275: Loss: 1.0402 | Train Acc: 68.945% (6274/9100)\n",
      "Training: Epoch 0 - Batch 91/275: Loss: 1.0299 | Train Acc: 69.272% (6373/9200)\n",
      "Training: Epoch 0 - Batch 92/275: Loss: 1.0199 | Train Acc: 69.581% (6471/9300)\n",
      "Training: Epoch 0 - Batch 93/275: Loss: 1.0101 | Train Acc: 69.894% (6570/9400)\n",
      "Training: Epoch 0 - Batch 94/275: Loss: 1.0007 | Train Acc: 70.179% (6667/9500)\n",
      "Training: Epoch 0 - Batch 95/275: Loss: 0.9917 | Train Acc: 70.448% (6763/9600)\n",
      "Training: Epoch 0 - Batch 96/275: Loss: 0.9823 | Train Acc: 70.742% (6862/9700)\n",
      "Training: Epoch 0 - Batch 97/275: Loss: 0.9729 | Train Acc: 71.041% (6962/9800)\n",
      "Training: Epoch 0 - Batch 98/275: Loss: 0.9640 | Train Acc: 71.323% (7061/9900)\n",
      "Training: Epoch 0 - Batch 99/275: Loss: 0.9552 | Train Acc: 71.600% (7160/10000)\n",
      "Training: Epoch 0 - Batch 100/275: Loss: 0.9471 | Train Acc: 71.851% (7257/10100)\n",
      "Training: Epoch 0 - Batch 101/275: Loss: 0.9394 | Train Acc: 72.098% (7354/10200)\n",
      "Training: Epoch 0 - Batch 102/275: Loss: 0.9314 | Train Acc: 72.340% (7451/10300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 - Batch 103/275: Loss: 0.9230 | Train Acc: 72.606% (7551/10400)\n",
      "Training: Epoch 0 - Batch 104/275: Loss: 0.9156 | Train Acc: 72.819% (7646/10500)\n",
      "Training: Epoch 0 - Batch 105/275: Loss: 0.9076 | Train Acc: 73.066% (7745/10600)\n",
      "Training: Epoch 0 - Batch 106/275: Loss: 0.9002 | Train Acc: 73.271% (7840/10700)\n",
      "Training: Epoch 0 - Batch 107/275: Loss: 0.8928 | Train Acc: 73.500% (7938/10800)\n",
      "Training: Epoch 0 - Batch 108/275: Loss: 0.8851 | Train Acc: 73.743% (8038/10900)\n",
      "Training: Epoch 0 - Batch 109/275: Loss: 0.8778 | Train Acc: 73.964% (8136/11000)\n",
      "Training: Epoch 0 - Batch 110/275: Loss: 0.8706 | Train Acc: 74.171% (8233/11100)\n",
      "Training: Epoch 0 - Batch 111/275: Loss: 0.8635 | Train Acc: 74.384% (8331/11200)\n",
      "Training: Epoch 0 - Batch 112/275: Loss: 0.8563 | Train Acc: 74.611% (8431/11300)\n",
      "Training: Epoch 0 - Batch 113/275: Loss: 0.8493 | Train Acc: 74.825% (8530/11400)\n",
      "Training: Epoch 0 - Batch 114/275: Loss: 0.8426 | Train Acc: 75.026% (8628/11500)\n",
      "Training: Epoch 0 - Batch 115/275: Loss: 0.8360 | Train Acc: 75.233% (8727/11600)\n",
      "Training: Epoch 0 - Batch 116/275: Loss: 0.8292 | Train Acc: 75.444% (8827/11700)\n",
      "Training: Epoch 0 - Batch 117/275: Loss: 0.8227 | Train Acc: 75.644% (8926/11800)\n",
      "Training: Epoch 0 - Batch 118/275: Loss: 0.8165 | Train Acc: 75.840% (9025/11900)\n",
      "Training: Epoch 0 - Batch 119/275: Loss: 0.8103 | Train Acc: 76.025% (9123/12000)\n",
      "Training: Epoch 0 - Batch 120/275: Loss: 0.8039 | Train Acc: 76.215% (9222/12100)\n",
      "Training: Epoch 0 - Batch 121/275: Loss: 0.7978 | Train Acc: 76.393% (9320/12200)\n",
      "Training: Epoch 0 - Batch 122/275: Loss: 0.7917 | Train Acc: 76.577% (9419/12300)\n",
      "Training: Epoch 0 - Batch 123/275: Loss: 0.7857 | Train Acc: 76.766% (9519/12400)\n",
      "Training: Epoch 0 - Batch 124/275: Loss: 0.7799 | Train Acc: 76.944% (9618/12500)\n",
      "Training: Epoch 0 - Batch 125/275: Loss: 0.7740 | Train Acc: 77.127% (9718/12600)\n",
      "Training: Epoch 0 - Batch 126/275: Loss: 0.7682 | Train Acc: 77.307% (9818/12700)\n",
      "Training: Epoch 0 - Batch 127/275: Loss: 0.7625 | Train Acc: 77.477% (9917/12800)\n",
      "Training: Epoch 0 - Batch 128/275: Loss: 0.7569 | Train Acc: 77.651% (10017/12900)\n",
      "Training: Epoch 0 - Batch 129/275: Loss: 0.7513 | Train Acc: 77.823% (10117/13000)\n",
      "Training: Epoch 0 - Batch 130/275: Loss: 0.7457 | Train Acc: 77.992% (10217/13100)\n",
      "Training: Epoch 0 - Batch 131/275: Loss: 0.7403 | Train Acc: 78.159% (10317/13200)\n",
      "Training: Epoch 0 - Batch 132/275: Loss: 0.7349 | Train Acc: 78.323% (10417/13300)\n",
      "Training: Epoch 0 - Batch 133/275: Loss: 0.7295 | Train Acc: 78.485% (10517/13400)\n",
      "Training: Epoch 0 - Batch 134/275: Loss: 0.7244 | Train Acc: 78.637% (10616/13500)\n",
      "Training: Epoch 0 - Batch 135/275: Loss: 0.7193 | Train Acc: 78.787% (10715/13600)\n",
      "Training: Epoch 0 - Batch 136/275: Loss: 0.7142 | Train Acc: 78.942% (10815/13700)\n",
      "Training: Epoch 0 - Batch 137/275: Loss: 0.7092 | Train Acc: 79.094% (10915/13800)\n",
      "Training: Epoch 0 - Batch 138/275: Loss: 0.7043 | Train Acc: 79.237% (11014/13900)\n",
      "Training: Epoch 0 - Batch 139/275: Loss: 0.6995 | Train Acc: 79.386% (11114/14000)\n",
      "Training: Epoch 0 - Batch 140/275: Loss: 0.6946 | Train Acc: 79.525% (11213/14100)\n",
      "Training: Epoch 0 - Batch 141/275: Loss: 0.6899 | Train Acc: 79.669% (11313/14200)\n",
      "Training: Epoch 0 - Batch 142/275: Loss: 0.6851 | Train Acc: 79.811% (11413/14300)\n",
      "Training: Epoch 0 - Batch 143/275: Loss: 0.6805 | Train Acc: 79.951% (11513/14400)\n",
      "Training: Epoch 0 - Batch 144/275: Loss: 0.6760 | Train Acc: 80.090% (11613/14500)\n",
      "Training: Epoch 0 - Batch 145/275: Loss: 0.6715 | Train Acc: 80.219% (11712/14600)\n",
      "Training: Epoch 0 - Batch 146/275: Loss: 0.6672 | Train Acc: 80.347% (11811/14700)\n",
      "Training: Epoch 0 - Batch 147/275: Loss: 0.6627 | Train Acc: 80.480% (11911/14800)\n",
      "Training: Epoch 0 - Batch 148/275: Loss: 0.6584 | Train Acc: 80.611% (12011/14900)\n",
      "Training: Epoch 0 - Batch 149/275: Loss: 0.6541 | Train Acc: 80.740% (12111/15000)\n",
      "Training: Epoch 0 - Batch 150/275: Loss: 0.6499 | Train Acc: 80.868% (12211/15100)\n",
      "Training: Epoch 0 - Batch 151/275: Loss: 0.6457 | Train Acc: 80.993% (12311/15200)\n",
      "Training: Epoch 0 - Batch 152/275: Loss: 0.6416 | Train Acc: 81.118% (12411/15300)\n",
      "Training: Epoch 0 - Batch 153/275: Loss: 0.6376 | Train Acc: 81.240% (12511/15400)\n",
      "Training: Epoch 0 - Batch 154/275: Loss: 0.6335 | Train Acc: 81.361% (12611/15500)\n",
      "Training: Epoch 0 - Batch 155/275: Loss: 0.6295 | Train Acc: 81.481% (12711/15600)\n",
      "Training: Epoch 0 - Batch 156/275: Loss: 0.6256 | Train Acc: 81.599% (12811/15700)\n",
      "Training: Epoch 0 - Batch 157/275: Loss: 0.6218 | Train Acc: 81.709% (12910/15800)\n",
      "Training: Epoch 0 - Batch 158/275: Loss: 0.6180 | Train Acc: 81.824% (13010/15900)\n",
      "Training: Epoch 0 - Batch 159/275: Loss: 0.6142 | Train Acc: 81.938% (13110/16000)\n",
      "Training: Epoch 0 - Batch 160/275: Loss: 0.6105 | Train Acc: 82.050% (13210/16100)\n",
      "Training: Epoch 0 - Batch 161/275: Loss: 0.6068 | Train Acc: 82.160% (13310/16200)\n",
      "Training: Epoch 0 - Batch 162/275: Loss: 0.6031 | Train Acc: 82.270% (13410/16300)\n",
      "Training: Epoch 0 - Batch 163/275: Loss: 0.5995 | Train Acc: 82.378% (13510/16400)\n",
      "Training: Epoch 0 - Batch 164/275: Loss: 0.5959 | Train Acc: 82.485% (13610/16500)\n",
      "Training: Epoch 0 - Batch 165/275: Loss: 0.5924 | Train Acc: 82.590% (13710/16600)\n",
      "Training: Epoch 0 - Batch 166/275: Loss: 0.5889 | Train Acc: 82.695% (13810/16700)\n",
      "Training: Epoch 0 - Batch 167/275: Loss: 0.5855 | Train Acc: 82.798% (13910/16800)\n",
      "Training: Epoch 0 - Batch 168/275: Loss: 0.5821 | Train Acc: 82.899% (14010/16900)\n",
      "Training: Epoch 0 - Batch 169/275: Loss: 0.5787 | Train Acc: 83.000% (14110/17000)\n",
      "Training: Epoch 0 - Batch 170/275: Loss: 0.5754 | Train Acc: 83.099% (14210/17100)\n",
      "Training: Epoch 0 - Batch 171/275: Loss: 0.5721 | Train Acc: 83.198% (14310/17200)\n",
      "Training: Epoch 0 - Batch 172/275: Loss: 0.5689 | Train Acc: 83.295% (14410/17300)\n",
      "Training: Epoch 0 - Batch 173/275: Loss: 0.5657 | Train Acc: 83.385% (14509/17400)\n",
      "Training: Epoch 0 - Batch 174/275: Loss: 0.5625 | Train Acc: 83.480% (14609/17500)\n",
      "Training: Epoch 0 - Batch 175/275: Loss: 0.5593 | Train Acc: 83.574% (14709/17600)\n",
      "Training: Epoch 0 - Batch 176/275: Loss: 0.5562 | Train Acc: 83.667% (14809/17700)\n",
      "Training: Epoch 0 - Batch 177/275: Loss: 0.5532 | Train Acc: 83.758% (14909/17800)\n",
      "Training: Epoch 0 - Batch 178/275: Loss: 0.5501 | Train Acc: 83.849% (15009/17900)\n",
      "Training: Epoch 0 - Batch 179/275: Loss: 0.5471 | Train Acc: 83.939% (15109/18000)\n",
      "Training: Epoch 0 - Batch 180/275: Loss: 0.5441 | Train Acc: 84.028% (15209/18100)\n",
      "Training: Epoch 0 - Batch 181/275: Loss: 0.5412 | Train Acc: 84.115% (15309/18200)\n",
      "Training: Epoch 0 - Batch 182/275: Loss: 0.5382 | Train Acc: 84.202% (15409/18300)\n",
      "Training: Epoch 0 - Batch 183/275: Loss: 0.5353 | Train Acc: 84.288% (15509/18400)\n",
      "Training: Epoch 0 - Batch 184/275: Loss: 0.5325 | Train Acc: 84.373% (15609/18500)\n",
      "Training: Epoch 0 - Batch 185/275: Loss: 0.5296 | Train Acc: 84.457% (15709/18600)\n",
      "Training: Epoch 0 - Batch 186/275: Loss: 0.5268 | Train Acc: 84.540% (15809/18700)\n",
      "Training: Epoch 0 - Batch 187/275: Loss: 0.5241 | Train Acc: 84.622% (15909/18800)\n",
      "Training: Epoch 0 - Batch 188/275: Loss: 0.5213 | Train Acc: 84.704% (16009/18900)\n",
      "Training: Epoch 0 - Batch 189/275: Loss: 0.5186 | Train Acc: 84.784% (16109/19000)\n",
      "Training: Epoch 0 - Batch 190/275: Loss: 0.5159 | Train Acc: 84.864% (16209/19100)\n",
      "Training: Epoch 0 - Batch 191/275: Loss: 0.5133 | Train Acc: 84.943% (16309/19200)\n",
      "Training: Epoch 0 - Batch 192/275: Loss: 0.5106 | Train Acc: 85.021% (16409/19300)\n",
      "Training: Epoch 0 - Batch 193/275: Loss: 0.5080 | Train Acc: 85.098% (16509/19400)\n",
      "Training: Epoch 0 - Batch 194/275: Loss: 0.5054 | Train Acc: 85.174% (16609/19500)\n",
      "Training: Epoch 0 - Batch 195/275: Loss: 0.5029 | Train Acc: 85.250% (16709/19600)\n",
      "Training: Epoch 0 - Batch 196/275: Loss: 0.5004 | Train Acc: 85.325% (16809/19700)\n",
      "Training: Epoch 0 - Batch 197/275: Loss: 0.4979 | Train Acc: 85.399% (16909/19800)\n",
      "Training: Epoch 0 - Batch 198/275: Loss: 0.4954 | Train Acc: 85.472% (17009/19900)\n",
      "Training: Epoch 0 - Batch 199/275: Loss: 0.4930 | Train Acc: 85.545% (17109/20000)\n",
      "Training: Epoch 0 - Batch 200/275: Loss: 0.4905 | Train Acc: 85.617% (17209/20100)\n",
      "Training: Epoch 0 - Batch 201/275: Loss: 0.4881 | Train Acc: 85.688% (17309/20200)\n",
      "Training: Epoch 0 - Batch 202/275: Loss: 0.4858 | Train Acc: 85.759% (17409/20300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 - Batch 203/275: Loss: 0.4834 | Train Acc: 85.828% (17509/20400)\n",
      "Training: Epoch 0 - Batch 204/275: Loss: 0.4811 | Train Acc: 85.898% (17609/20500)\n",
      "Training: Epoch 0 - Batch 205/275: Loss: 0.4787 | Train Acc: 85.966% (17709/20600)\n",
      "Training: Epoch 0 - Batch 206/275: Loss: 0.4765 | Train Acc: 86.034% (17809/20700)\n",
      "Training: Epoch 0 - Batch 207/275: Loss: 0.4742 | Train Acc: 86.101% (17909/20800)\n",
      "Training: Epoch 0 - Batch 208/275: Loss: 0.4719 | Train Acc: 86.167% (18009/20900)\n",
      "Training: Epoch 0 - Batch 209/275: Loss: 0.4697 | Train Acc: 86.233% (18109/21000)\n",
      "Training: Epoch 0 - Batch 210/275: Loss: 0.4675 | Train Acc: 86.299% (18209/21100)\n",
      "Training: Epoch 0 - Batch 211/275: Loss: 0.4653 | Train Acc: 86.363% (18309/21200)\n",
      "Training: Epoch 0 - Batch 212/275: Loss: 0.4631 | Train Acc: 86.427% (18409/21300)\n",
      "Training: Epoch 0 - Batch 213/275: Loss: 0.4610 | Train Acc: 86.491% (18509/21400)\n",
      "Training: Epoch 0 - Batch 214/275: Loss: 0.4588 | Train Acc: 86.553% (18609/21500)\n",
      "Training: Epoch 0 - Batch 215/275: Loss: 0.4568 | Train Acc: 86.616% (18709/21600)\n",
      "Training: Epoch 0 - Batch 216/275: Loss: 0.4547 | Train Acc: 86.677% (18809/21700)\n",
      "Training: Epoch 0 - Batch 217/275: Loss: 0.4526 | Train Acc: 86.739% (18909/21800)\n",
      "Training: Epoch 0 - Batch 218/275: Loss: 0.4505 | Train Acc: 86.799% (19009/21900)\n",
      "Training: Epoch 0 - Batch 219/275: Loss: 0.4485 | Train Acc: 86.859% (19109/22000)\n",
      "Training: Epoch 0 - Batch 220/275: Loss: 0.4465 | Train Acc: 86.919% (19209/22100)\n",
      "Training: Epoch 0 - Batch 221/275: Loss: 0.4445 | Train Acc: 86.977% (19309/22200)\n",
      "Training: Epoch 0 - Batch 222/275: Loss: 0.4425 | Train Acc: 87.036% (19409/22300)\n",
      "Training: Epoch 0 - Batch 223/275: Loss: 0.4406 | Train Acc: 87.094% (19509/22400)\n",
      "Training: Epoch 0 - Batch 224/275: Loss: 0.4386 | Train Acc: 87.151% (19609/22500)\n",
      "Training: Epoch 0 - Batch 225/275: Loss: 0.4367 | Train Acc: 87.208% (19709/22600)\n",
      "Training: Epoch 0 - Batch 226/275: Loss: 0.4348 | Train Acc: 87.264% (19809/22700)\n",
      "Training: Epoch 0 - Batch 227/275: Loss: 0.4329 | Train Acc: 87.320% (19909/22800)\n",
      "Training: Epoch 0 - Batch 228/275: Loss: 0.4310 | Train Acc: 87.376% (20009/22900)\n",
      "Training: Epoch 0 - Batch 229/275: Loss: 0.4291 | Train Acc: 87.430% (20109/23000)\n",
      "Training: Epoch 0 - Batch 230/275: Loss: 0.4273 | Train Acc: 87.485% (20209/23100)\n",
      "Training: Epoch 0 - Batch 231/275: Loss: 0.4255 | Train Acc: 87.539% (20309/23200)\n",
      "Training: Epoch 0 - Batch 232/275: Loss: 0.4237 | Train Acc: 87.592% (20409/23300)\n",
      "Training: Epoch 0 - Batch 233/275: Loss: 0.4219 | Train Acc: 87.645% (20509/23400)\n",
      "Training: Epoch 0 - Batch 234/275: Loss: 0.4201 | Train Acc: 87.698% (20609/23500)\n",
      "Training: Epoch 0 - Batch 235/275: Loss: 0.4183 | Train Acc: 87.750% (20709/23600)\n",
      "Training: Epoch 0 - Batch 236/275: Loss: 0.4166 | Train Acc: 87.802% (20809/23700)\n",
      "Training: Epoch 0 - Batch 237/275: Loss: 0.4148 | Train Acc: 87.853% (20909/23800)\n",
      "Training: Epoch 0 - Batch 238/275: Loss: 0.4131 | Train Acc: 87.904% (21009/23900)\n",
      "Training: Epoch 0 - Batch 239/275: Loss: 0.4114 | Train Acc: 87.954% (21109/24000)\n",
      "Training: Epoch 0 - Batch 240/275: Loss: 0.4097 | Train Acc: 88.004% (21209/24100)\n",
      "Training: Epoch 0 - Batch 241/275: Loss: 0.4080 | Train Acc: 88.054% (21309/24200)\n",
      "Training: Epoch 0 - Batch 242/275: Loss: 0.4064 | Train Acc: 88.103% (21409/24300)\n",
      "Training: Epoch 0 - Batch 243/275: Loss: 0.4047 | Train Acc: 88.152% (21509/24400)\n",
      "Training: Epoch 0 - Batch 244/275: Loss: 0.4030 | Train Acc: 88.200% (21609/24500)\n",
      "Training: Epoch 0 - Batch 245/275: Loss: 0.4014 | Train Acc: 88.248% (21709/24600)\n",
      "Training: Epoch 0 - Batch 246/275: Loss: 0.3998 | Train Acc: 88.296% (21809/24700)\n",
      "Training: Epoch 0 - Batch 247/275: Loss: 0.3982 | Train Acc: 88.343% (21909/24800)\n",
      "Training: Epoch 0 - Batch 248/275: Loss: 0.3966 | Train Acc: 88.390% (22009/24900)\n",
      "Training: Epoch 0 - Batch 249/275: Loss: 0.3950 | Train Acc: 88.436% (22109/25000)\n",
      "Training: Epoch 0 - Batch 250/275: Loss: 0.3935 | Train Acc: 88.482% (22209/25100)\n",
      "Training: Epoch 0 - Batch 251/275: Loss: 0.3919 | Train Acc: 88.528% (22309/25200)\n",
      "Training: Epoch 0 - Batch 252/275: Loss: 0.3904 | Train Acc: 88.573% (22409/25300)\n",
      "Training: Epoch 0 - Batch 253/275: Loss: 0.3889 | Train Acc: 88.618% (22509/25400)\n",
      "Training: Epoch 0 - Batch 254/275: Loss: 0.3873 | Train Acc: 88.663% (22609/25500)\n",
      "Training: Epoch 0 - Batch 255/275: Loss: 0.3859 | Train Acc: 88.707% (22709/25600)\n",
      "Training: Epoch 0 - Batch 256/275: Loss: 0.3844 | Train Acc: 88.751% (22809/25700)\n",
      "Training: Epoch 0 - Batch 257/275: Loss: 0.3829 | Train Acc: 88.795% (22909/25800)\n",
      "Training: Epoch 0 - Batch 258/275: Loss: 0.3814 | Train Acc: 88.838% (23009/25900)\n",
      "Training: Epoch 0 - Batch 259/275: Loss: 0.3800 | Train Acc: 88.881% (23109/26000)\n",
      "Training: Epoch 0 - Batch 260/275: Loss: 0.3785 | Train Acc: 88.923% (23209/26100)\n",
      "Training: Epoch 0 - Batch 261/275: Loss: 0.3771 | Train Acc: 88.966% (23309/26200)\n",
      "Training: Epoch 0 - Batch 262/275: Loss: 0.3757 | Train Acc: 89.008% (23409/26300)\n",
      "Training: Epoch 0 - Batch 263/275: Loss: 0.3742 | Train Acc: 89.049% (23509/26400)\n",
      "Training: Epoch 0 - Batch 264/275: Loss: 0.3728 | Train Acc: 89.091% (23609/26500)\n",
      "Training: Epoch 0 - Batch 265/275: Loss: 0.3714 | Train Acc: 89.132% (23709/26600)\n",
      "Training: Epoch 0 - Batch 266/275: Loss: 0.3701 | Train Acc: 89.172% (23809/26700)\n",
      "Training: Epoch 0 - Batch 267/275: Loss: 0.3687 | Train Acc: 89.213% (23909/26800)\n",
      "Training: Epoch 0 - Batch 268/275: Loss: 0.3673 | Train Acc: 89.253% (24009/26900)\n",
      "Training: Epoch 0 - Batch 269/275: Loss: 0.3660 | Train Acc: 89.293% (24109/27000)\n",
      "Training: Epoch 0 - Batch 270/275: Loss: 0.3646 | Train Acc: 89.332% (24209/27100)\n",
      "Training: Epoch 0 - Batch 271/275: Loss: 0.3633 | Train Acc: 89.371% (24309/27200)\n",
      "Training: Epoch 0 - Batch 272/275: Loss: 0.3620 | Train Acc: 89.410% (24409/27300)\n",
      "Training: Epoch 0 - Batch 273/275: Loss: 0.3607 | Train Acc: 89.449% (24509/27400)\n",
      "Training: Epoch 0 - Batch 274/275: Loss: 0.3594 | Train Acc: 89.470% (24564/27455)\n"
     ]
    }
   ],
   "source": [
    "#--- set up ---\n",
    "\n",
    "# Print Cuda info\n",
    "print(\"Cuda is available: {} \\n\".format(torch.cuda.is_available()))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Oprimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Loss functions\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Sum of sizes\n",
    "        total += target.size(0)\n",
    "        \n",
    "        # Get prediction values\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Get prediction labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Sum of number of correct predictions\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss=loss_function(outputs, target)\n",
    "        \n",
    "        # Backward + optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss+=loss\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "    \n",
    "    # WRITE CODE HERE\n",
    "    # Please implement early stopping here.\n",
    "    # You can try different versions, simplest way is to calculate the dev error and\n",
    "    # compare this with the previous dev error, stopping if the error has grown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 0/37: Loss: 1.0364 | Test Acc: 66.000% (66/100)\n",
      "Evaluating: Batch 1/37: Loss: 0.5997 | Test Acc: 80.000% (160/200)\n",
      "Evaluating: Batch 2/37: Loss: 1.0186 | Test Acc: 72.667% (218/300)\n",
      "Evaluating: Batch 3/37: Loss: 1.2728 | Test Acc: 70.750% (283/400)\n",
      "Evaluating: Batch 4/37: Loss: 1.1441 | Test Acc: 73.200% (366/500)\n",
      "Evaluating: Batch 5/37: Loss: 0.9666 | Test Acc: 77.500% (465/600)\n",
      "Evaluating: Batch 6/37: Loss: 0.8684 | Test Acc: 79.143% (554/700)\n",
      "Evaluating: Batch 7/37: Loss: 0.7684 | Test Acc: 81.625% (653/800)\n",
      "Evaluating: Batch 8/37: Loss: 0.6935 | Test Acc: 83.333% (750/900)\n",
      "Evaluating: Batch 9/37: Loss: 0.6251 | Test Acc: 85.000% (850/1000)\n",
      "Evaluating: Batch 10/37: Loss: 0.5781 | Test Acc: 86.182% (948/1100)\n",
      "Evaluating: Batch 11/37: Loss: 0.6959 | Test Acc: 84.083% (1009/1200)\n",
      "Evaluating: Batch 12/37: Loss: 0.7952 | Test Acc: 81.846% (1064/1300)\n",
      "Evaluating: Batch 13/37: Loss: 0.9387 | Test Acc: 77.714% (1088/1400)\n",
      "Evaluating: Batch 14/37: Loss: 0.9080 | Test Acc: 78.533% (1178/1500)\n",
      "Evaluating: Batch 15/37: Loss: 0.8565 | Test Acc: 79.625% (1274/1600)\n",
      "Evaluating: Batch 16/37: Loss: 0.8186 | Test Acc: 80.471% (1368/1700)\n",
      "Evaluating: Batch 17/37: Loss: 0.8010 | Test Acc: 81.000% (1458/1800)\n",
      "Evaluating: Batch 18/37: Loss: 0.7605 | Test Acc: 81.947% (1557/1900)\n",
      "Evaluating: Batch 19/37: Loss: 0.7363 | Test Acc: 82.200% (1644/2000)\n",
      "Evaluating: Batch 20/37: Loss: 0.7201 | Test Acc: 82.048% (1723/2100)\n",
      "Evaluating: Batch 21/37: Loss: 0.7174 | Test Acc: 81.773% (1799/2200)\n",
      "Evaluating: Batch 22/37: Loss: 0.7027 | Test Acc: 81.870% (1883/2300)\n",
      "Evaluating: Batch 23/37: Loss: 0.6975 | Test Acc: 81.542% (1957/2400)\n",
      "Evaluating: Batch 24/37: Loss: 0.6833 | Test Acc: 81.880% (2047/2500)\n",
      "Evaluating: Batch 25/37: Loss: 0.6577 | Test Acc: 82.577% (2147/2600)\n",
      "Evaluating: Batch 26/37: Loss: 0.6423 | Test Acc: 83.000% (2241/2700)\n",
      "Evaluating: Batch 27/37: Loss: 0.6250 | Test Acc: 83.500% (2338/2800)\n",
      "Evaluating: Batch 28/37: Loss: 0.6170 | Test Acc: 83.586% (2424/2900)\n",
      "Evaluating: Batch 29/37: Loss: 0.6434 | Test Acc: 83.233% (2497/3000)\n",
      "Evaluating: Batch 30/37: Loss: 0.6427 | Test Acc: 83.387% (2585/3100)\n",
      "Evaluating: Batch 31/37: Loss: 0.6534 | Test Acc: 82.688% (2646/3200)\n",
      "Evaluating: Batch 32/37: Loss: 0.6517 | Test Acc: 82.424% (2720/3300)\n",
      "Evaluating: Batch 33/37: Loss: 0.6336 | Test Acc: 82.882% (2818/3400)\n",
      "Evaluating: Batch 34/37: Loss: 0.6231 | Test Acc: 83.057% (2907/3500)\n",
      "Evaluating: Batch 35/37: Loss: 0.6235 | Test Acc: 82.833% (2982/3600)\n",
      "Evaluating: Batch 36/37: Loss: 0.6198 | Test Acc: 82.816% (3041/3672)\n"
     ]
    }
   ],
   "source": [
    "#--- test ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        outputs=model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        test_correct += (predicted == target).sum()\n",
    "        loss=loss_function(outputs, target)\n",
    "        \n",
    "        test_loss+=loss\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(test_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / total, test_correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 2\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 24\n",
    "DATA_DIR = '../data/sign_mnist_%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset initialization ---\n",
    "\n",
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "\n",
    "# Grayscale + toTensor + Normalize\n",
    "train_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=0.5, std=0.5, inplace=True)])\n",
    "\n",
    "train_set = datasets.ImageFolder(DATA_DIR % 'train', transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DATA_DIR % 'dev',   transform=test_transform)\n",
    "test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        # Sequential 1: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=100, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(num_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # Sequential 2: Convolution + batch normalization + ReLU + maxpooling\n",
    "        self.seq2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=100, out_channels=80, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(num_features=80),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # Sequential 3: Linear + ReLU + Linear\n",
    "        self.seq3 = nn.Sequential(\n",
    "            nn.Linear(in_features=4*4*80, out_features=250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=250, out_features=NUM_CLASSES))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # WRITE CODE HERE\n",
    "  \n",
    "        # Sequential 1\n",
    "        x = self.seq1(x)\n",
    "        \n",
    "        # Sequential 2\n",
    "        x = self.seq2(x)\n",
    "        \n",
    "        # Reshape\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Sequential 3\n",
    "        x = self.seq3(x)\n",
    "        \n",
    "        # log_softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # Return x\n",
    "        return x\n",
    "\n",
    "# Print model summary\n",
    "#print(CNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: False \n",
      "\n",
      "Training: Epoch 0 - Batch 0/275: Loss: 3.2670 | Train Acc: 6.000% (6/100)\n",
      "Training: Epoch 0 - Batch 1/275: Loss: 4.2831 | Train Acc: 5.000% (10/200)\n",
      "Training: Epoch 0 - Batch 2/275: Loss: 4.1869 | Train Acc: 6.000% (18/300)\n",
      "Training: Epoch 0 - Batch 3/275: Loss: 4.0324 | Train Acc: 6.000% (24/400)\n",
      "Training: Epoch 0 - Batch 4/275: Loss: 3.8802 | Train Acc: 6.600% (33/500)\n",
      "Training: Epoch 0 - Batch 5/275: Loss: 3.8172 | Train Acc: 7.667% (46/600)\n",
      "Training: Epoch 0 - Batch 6/275: Loss: 3.7395 | Train Acc: 8.000% (56/700)\n",
      "Training: Epoch 0 - Batch 7/275: Loss: 3.6712 | Train Acc: 8.750% (70/800)\n",
      "Training: Epoch 0 - Batch 8/275: Loss: 3.6285 | Train Acc: 8.889% (80/900)\n",
      "Training: Epoch 0 - Batch 9/275: Loss: 3.5635 | Train Acc: 10.200% (102/1000)\n",
      "Training: Epoch 0 - Batch 10/275: Loss: 3.5108 | Train Acc: 10.636% (117/1100)\n",
      "Training: Epoch 0 - Batch 11/275: Loss: 3.4369 | Train Acc: 11.500% (138/1200)\n",
      "Training: Epoch 0 - Batch 12/275: Loss: 3.3897 | Train Acc: 11.538% (150/1300)\n",
      "Training: Epoch 0 - Batch 13/275: Loss: 3.3338 | Train Acc: 12.500% (175/1400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3627ea15047e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--- set up ---\n",
    "\n",
    "# Print Cuda info\n",
    "print(\"Cuda is available: {} \\n\".format(torch.cuda.is_available()))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Oprimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Loss functions\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data)\n",
    "        total += target.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        loss=loss_function(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss+=loss\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "    \n",
    "    # WRITE CODE HERE\n",
    "    # Please implement early stopping here.\n",
    "    # You can try different versions, simplest way is to calculate the dev error and\n",
    "    # compare this with the previous dev error, stopping if the error has grown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- test ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        outputs=model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        test_correct += (predicted == target).sum()\n",
    "        loss=loss_function(outputs, target)\n",
    "        \n",
    "        test_loss+=loss\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(test_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / total, test_correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
